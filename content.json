{"meta":{"title":"无题","subtitle":"天南地北问乾坤","description":null,"author":"yutinglin","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"创业一段时间的总结和感悟：","slug":"创业一段时间的总结和感悟","date":"2017-08-17T04:19:54.000Z","updated":"2017-08-25T10:45:47.000Z","comments":true,"path":"2017/08/17/创业一段时间的总结和感悟/","link":"","permalink":"http://yoursite.com/2017/08/17/创业一段时间的总结和感悟/","excerpt":"创业一段时间的总结和感悟","text":"创业一段时间的总结和感悟 1.创业者定位：但凡把自己定位成产品经理或者是专注于技术的创业者都要么失败，要么走在失败的路上 2.论对上管理与对下管理：当对下管理与对上管理出现问题，不能生气，生气解决不了客观上现实存在的问题与主观上个人思维的局限。首先，平心静气，心情平和了然后才可以谈下一个步骤，就是带着超然的态度理性的分析问题，从各个方面入手思考产生矛盾的节点。思考矛盾的形成原因和解决方案。有一些和工作和业务直接相关的矛盾可能是暂时突破不了的，另外一些因为工作风格或者工作细节产生的矛盾可能是永远也解决不了的。这个时候需要做自己的工作。有的时候，要做出突破，突破眼前人和事的局限，或许只有从自身改变，改天换地，换一个环境。从头塑造。 论及不能生气的原因，不利于问题的解决和思想的顺畅是两个方面，还有一个理由很现实也很有道理：就是我们没有必要因为别人的错误和处事的幼稚以及思想的狭隘而生气。 曾经我有一个领导，和他工作过程中很不愉快，觉得有些事情应该做而有些事情不应该做。后来脱离局中冷静思考，发觉根本原因在于他的能力有问题。而我身在局中之时，想当然地认为领导的能力应该比我强，这事情应该做到什么什么程度才算好。其实每个人都有短板，并且会尽一切可能掩藏起来。我错在太过真诚，更错在认为和我一起共事的人也一样真诚。 很多时候，我都深恨自己悟得太晚。永远叫不醒一个装睡的人。 3.创业与个人发展的关系：薪资和待遇差个千八百块钱的是属于局部的细节问题。全局上宏观的问题是个人从整体上有没有把握住时间，对当下情况和个人发展相结合进行有效分析，有效规划，有效实施，最终有效的完善自己，发展自己，给自己一个光明坦途。举例来说，是否沉溺于舒适区，得过且过，放松学习 和视野的开阔，这样的话是必然在走下坡路并在某一个时刻报偿到当事人身上，甚至很大程度上会有无力回天的可能。 4.选择创业我的建议是不要创业。创业是对一个人能力、格局、视野、心胸全方位的要求，根据我的观察，大多数人不适合创业。我个人则是没有办法，我没有赶上房地产 和股票，只好在这个阶段选择创业。除非是拥有顶尖的资源支持或者是技术，否则普通人要是想学马云在这个创业大潮中突围，是想都不要想了。马云自己说：政府要干什么事，我掉头就跑。马云开始做事业的时候，是全中国人大多数都不知道it是什么的时候，马云的事业开始腾飞的时候，是国人大多数认为网购是骗钱的时候。往往于死地而有生，辉煌时或许正是他没落的时候。这个道理大众是不懂的，所以普通人响应号召去创业，只能是当炮灰。 5.面对突发情况我们的人生好像一幅画卷，可是下一秒钟会发生什么我们不知道，如果说下一秒钟发生的事情超出了我们的预期，我们或许就会生气。其实这是无必要的。应该调整好自己的心态，接受自己的画卷，因为发生的一切都是许多偶然的因素造成的一个必然的结果。 人世间没有一帆风顺的事业。综观世界历史，任何一个国家、一个民族的发展，都会跌宕起伏甚至充满曲折。“艰难困苦，玉汝于成。”“多难兴邦，殷忧启圣。”“失败为成功之母。”毛泽东同志也常说，前途是光明的，道路是曲折的。这是一切正义事业发展的历史逻辑。我们的事业之所以伟大，就在于经历世所罕见的艰难而不断取得成功。","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"线程安全与锁优化——深入理解JVM阅读笔记","slug":"线程安全与锁优化","date":"2017-08-14T06:44:54.000Z","updated":"2017-08-15T07:56:21.000Z","comments":true,"path":"2017/08/14/线程安全与锁优化/","link":"","permalink":"http://yoursite.com/2017/08/14/线程安全与锁优化/","excerpt":"我根据我的理解把一些关键的要点整理了出来，并对其中一些内容作了删改。恳请原作者见谅。这是原文的地址：http://www.cnblogs.com/pacoson/p/5351355.html请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"我根据我的理解把一些关键的要点整理了出来，并对其中一些内容作了删改。恳请原作者见谅。这是原文的地址：http://www.cnblogs.com/pacoson/p/5351355.html请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 要点1.线程安全的实现：1）互斥同步（阻塞同步） 悲观的并发策略：sysncronised reentrantlock重入锁2）非阻塞同步 基于冲突检测的乐观并发策略 实现了cas机制的原子类atomicInteger 正文【0】README 0.1）本文部分文字转自“深入理解jvm”， 旨在学习 线程安全与锁优化 的基础知识； 0.2）本文知识对于理解 java并发编程非常有用，个人觉得，所以我总结的很详细； 【1】概述 ###【2】线程安全 线程安全定义：当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获得正确的结果，那这个对象是线程安全的；（干货——线程安全定义） ####【2.1】java 语言中的线程安全（干货——java中各种操作共享的数据分为以下5类）0）java中各种操作共享的数据分为以下5类：不可变， 绝对线程安全， 相对线程安全，线程兼容，线程对立； 1）不可变对象：该对象一定是线程安全的，无论是对象的方法实现还是方法的调用者，都不需要采取任何的线程安全保障措施； 1.1）如果共享数据是一个基本数据类型，那么只要在定义时使用 final 关键字修饰它就可以保证它是不可变的； 1.2）不妨想想java.lang.String类的对象：它是一个典型的不可变对象，调用它的substring(), replace(), concat() 这些方法都不会影响它原来的值，只会返回一个新构造的字符串对象； 1.3）看个荔枝：如java.lang.Integer 构造函数所示的，将value定义为final 来保障状态不变； 2）绝对线程安全2.1）在java API中标注自己是线程安全的类，大多数都不是绝对的线程安全 2.2）java.util.Vector 是一个线程安全的容器，因为它的add()方法，get()方法，size() 方法 这些方法都是被 synchronized修饰的，尽管效率低下，但确实是安全的；对Vector的测试如下： 复制代码// 对线程安全的容器 Vector的测试 public class VectorTest { private static Vector&lt;Integer&gt; vector = new Vector&lt;&gt;(); public static void main(String[] args) { while(true) { for (int i = 0; i &lt; 100; i++) { vector.add(i); } Thread removeThread = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &lt; vector.size(); i++) { vector.remove(i); } } }); Thread printThread = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &lt; vector.size(); i++) { System.out.println(vector.get(i)); } } }); removeThread.start(); printThread.start(); // 不要同时产生过多的线程，否则会导致os 假死 while(Thread.activeCount() &gt; 20); } } } 对以上代码的分析（Analysis）： A1）运行结果： 作者说会抛出异常（但我的运行结果却没有抛出异常），按理说应该是会抛出异常的； A2）抛出异常的原因：因为如果另一个线程恰好在错误的时间里删除了一个元素，导致序号i 已经不再可用的话，再用i 访问数组就会抛出一个 ArrayIndexOutOfBoundsException。 A3）如果要保证这段代码能够正确执行下去，修改后的代码为： // 对线程安全的容器 Vector的测试(修改后的代码) public class ModifiedVectorTest { private static Vector&lt;Integer&gt; vector = new Vector&lt;&gt;(); public static void main(String[] args) { while(true) { for (int i = 0; i &lt; 100; i++) { vector.add(i); } Thread removeThread = new Thread(new Runnable() { @Override public void run() { synchronized (vector) { // 添加同步块，this line for (int i = 0; i &lt; vector.size(); i++) { vector.remove(i); } } } }); Thread printThread = new Thread(new Runnable() { @Override public void run() { synchronized (vector) { // 添加同步块，this line for (int i = 0; i &lt; vector.size(); i++) { System.out.println(vector.get(i)); } } } }); removeThread.start(); printThread.start(); // 不要同时产生过多的线程，否则会导致os 假死 while(Thread.activeCount() &gt; 20); } } } 3）相对线程安全3.1）上述 VectorTest.java 和 ModifiedVectorTest.java 就是相对线程安全的案例； 4）线程兼容4.1）线程兼容定义： 线程兼容是指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用； 5）线程对立5.1）线程对立定义：指无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码； 5.2）由于java语言天生就具备多线程特性，线程对立这种排斥多线程的代码是很少出现的，而且通常是有害的，应当尽量避免； 5.3）线程对立的荔枝：Thread类的suspend() 和 resume() 方法；如果有两个线程同时持有一个线程对象，一个尝试去中断线程，另一个尝试去恢复线程，如果并发进行的话，无论调用时是否进行了同步，目标线程都是存在死锁风险的。正由于这个原因，suspend和result方法已经被JDK废弃了了（@Deprecated） ###【2.2】线程安全的实现方法1）互斥同步 1.1）互斥同步：是常见的并发正确性保障手段； 1.2）同步：是指在多个线程并发访问共享数据时，保证共享数据在同一个时刻被一个线程使用。 1.3）互斥：互斥是实现同步的一种手段；临界区，互斥量和信号量都是主要的互斥实现方式。因此，在这4个字里面，互斥是因，同步是果；互斥是方法，同步是目的； 1.4）最基本的互斥同步手段就是 synchronized关键字：synchronized关键字经过 编译之后，会在同步块的前后分别形成 monitorenter 和 monitorexit 这个两个字节码指令，这两个字节码都需要一个 reference类型的参数来指明要锁定和解锁的对象；如果java程序中的synchronized明确指定了对象参数，那就是这个对象的reference；如果没有明确指定，那就根据 synchronized修饰的实例方法还是类方法，去取对应的对象实例或Class 对象来作为锁对象；（干货——最基本的互斥同步手段就是 synchronized关键字） 1.5）根据虚拟机规范的要求：在执行monitorenter指令时，如果这个对象没有锁定或当前线程已经拥有了那个对象的锁，锁的计数器加1，相应的，在执行 monitorexit 指令时会将锁计数器减1；当计数器为0时，锁就被释放了；（干货——执行monitorenter和monitorexit 指令） Attention）对于monitorenter 和 monitorexit 的行为描述中，有两点需要注意：A1）synchronized同步块对同一条线程来说是可重入的， 不会出现自己把自己锁死的问题；A2）同步块在已进入的线程执行完之前，会阻塞后面其他线程 的进入； 1.6）除了synchronized之外，还可以使用 java.util.concurrent 包中的重入锁（ReentrantLock）来实现同步；（干货——引入重入锁进行同步） 1.6.1）synchronized 和 ReentrantLock 的区别： 一个表现为 API 层面的互斥锁（lock() 和 unlock() 方法配合 try/finally 语句块来完成），另一个表现为 原生语法层面的互斥锁； 1.6.2）ReentrantLock增加了一些高级功能：主要有3项：等待可中断，可实现公平锁， 以及锁可以绑定多个条件；（干货——ReentrantLock 增加了3项高级功能） case1）等待可中断：指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情，可中断特性对处理执行时间非常长的同步块很有帮助； case2）公平锁：指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁； case3）锁绑定多个条件：指一个 ReentrantLock对象可以同时绑定多个 Condition对象，而在 synchronized中，锁对象的wait() 和 notify() 或 notifyAll() 方法可以实现一个隐含的条件，如果要和多于一个的条件关联的时候，就不得不额外地添加一个锁，而ReentrantLock 则无需这样做，只需要多次调用 newCondition() 方法即可；（干货——可重入锁ReentrantLock 和 synchronized 绑定多个条件的实现方式的区别） 1.6.3）关于synchronized 和 ReentrantLock 性能的分析： 对上图的分析（Analysis）： A1）多线程环境下 synchronized的吞吐量下降得非常严重，而 ReentrantLock 则能基本保持在同一个比较稳定的水平上；与其说ReentrantLock性能好，还不如说 synchronized还有非常大的优化余地； A2）虚拟机在未来的性能改进中肯定也会更加偏向于原生的 synchronized，所以还是提倡在 synchronized能实现需求的情况下，优先考虑使用 synchronized 来进行同步；（干货——同步方式推荐使用synchronized） 2）非阻塞同步2.1）阻塞同步（互斥同步）的问题：就是进行线程阻塞和唤醒所带来的性能问题，互斥同步属于一种悲观的并发策略，无论共享数据是否真的会出现竞争，它都要进行加锁，用户态核心态转换，维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作；（干货——阻塞同步（互斥同步）的问题） 2.2）非阻塞同步定义：基于冲突检测的乐观并发策略，通俗的说，就是先进行操作，如果没有其他线程争用共享数据，那操作就成功了；如果共享数据有争用，产生了冲突，那就再采用其他的补偿措施，这种乐观的并发策略的许多实现都不需要把线程挂起，因此这种同步操作称为 非阻塞同步；（干货——非阻塞同步定义） 2.3）为什么作者要说使用乐观并发策略需要“硬件指令集的发展”才能进行呢？因为 我们需要操作和冲突检测这两个步骤具备原子性，靠什么来保证呢？2.3.1）硬件：保证一个从语义上看起来需要多次操作的行为只通过一次处理器指令就能完成，这类指令常用的有：（instructions）i1）测试并设置（Test-and-Set）；i2）获取并增加（Fetch-and-Increment）；i3）交换（Swap）；i4）比较并交换（Compare-and-Swap，下文简称 CAS）；i5）加载链接/ 条件存储（Load-Linked/Store-Conditional，下文简称 LL/SC）；2.4）如何使用CAS 操作来避免阻塞同步，看个荔枝：（测试incrementAndGet 方法的原子性） // Atomic 变量自增运算测试(incrementAndGet 方法的原子性) public class AtomicTest { public static AtomicInteger race = new AtomicInteger(0); public static void increase() { // 输出正确结果，一切都要归功于 incrementAndGet 方法的原子性 race.incrementAndGet(); } public static final int THREADS_COUNT = 20; public static void main(String[] args) throws Exception { Thread[] threads = new Thread[THREADS_COUNT]; for (int i = 0; i &lt; threads.length; i++) { threads[i] = new Thread(new Runnable() { @Override public void run() { for (int j = 0; j &lt; 10000; j++) { increase(); } } }); threads[i].start(); } while(Thread.activeCount() &gt; 1) { Thread.yield(); } System.out.println(race); } /** * incrementAndGet() 方法的JDK 源码 * Atomically increment by one the current value. * @return the updated value */ public final int incrementAndGet() { for(;;) { int current = get(); int next = current + 1; if(compareAndSet(current,next)) { return next; } } } } 2.5）CAS操作（比较并交换操作）的ABA问题：如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就说它的值没有被其他线程改变过了吗？ 如果在这段期间它的值曾经被改为了B，之后又改回了A，那CAS操作就会误认为它从来没有被改变过，这个漏洞称为 CAS操作的 ABA问题； 2.6）解决方法：J.U.C 包为了解决这个问题，提供了一个带有标记的原子引用类“AtomicStampedReference”，它可以通过控制变量值的version 来保证CAS的正确性。不过目前来说这个类比较鸡肋， 大部分cases 下 ABA问题 不会影响程序并发的正确性，如果需要解决ABA问题，改用传统的互斥同步可能会比原子类更高效；（干货——CAS操作（比较并交换操作）的ABA问题及其解决方法） 3）无同步方案3.0）intro： 如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性，因此会有一些代码天生就是线程安全的；下面介绍两类线程安全代码： 3.1）第一类线程安全代码——可重入代码：也叫作纯代码，可以在代码执行的任何时刻中断它，转而去执行另外一段代码，而在控制权返回后，原来的程序不会出现任何错误； （干货——可重入代码定义）3.1.1）所有的可重入代码都是线程安全的； 3.1.2）如何判断代码是否具备可重入性：如果一个方法，它的返回结果是可以预测的，只要输入了相同的数据，就都能返回相同的结果，那它就满足可重入性的要求，当然也就是线程安全的； 3.2）第二类线程安全代码——线程本地存储：如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能够保证在同一线程中执行？ 如果能保证，我们就可以把共享数据的可见范围限制在同一个线程内，这样，无需同步也可以保证线程间不出现数据争用问题； 【3】锁优化【3.1】 自旋锁与自适应自旋（干货——引入自旋锁与自适应自旋）1）problem：前文中我们提到，互斥同步对性能最大的影响是阻塞的实现，挂起线程和恢复线程的操作都需要转入内核态中完成，共享数据的锁定状态只会持续很短的一段时间，为了这段时间去挂起和恢复线程很不值得；（干货——产生自旋锁与自适应自旋的背景） 2）自旋锁定义：为了让线程等待，我们只需让线程执行一个忙循环（自旋），这项技术就是所谓的自旋锁；（solution） 2.1）jdk1.6中 自旋锁是默认开启的，可以使用 -XX:+UseSpinning 参数来开启； 2.2）自旋等待的时间必须要有一定的限度： 如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程了。自旋次数的默认值是10，用户可以用参数 -XX:PreBlockSpin 来更改； 2.3）自适应自旋锁：jdk1.6 中引入了自适应的自旋锁。自适应意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定； case1）如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间，比如100个cycle； case2）如果对于某个锁，自旋很少成功获得过， 那在以后要获取这个锁时将可能省略掉自旋过程，以避免浪费处理器资源； 【3.2】锁消除1）定义：锁消除是指虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检查到不可能存在共享数据竞争的锁进行消除；（干货——引入锁消除的概念） 2）锁消除的主要判定依据：来源于逃逸分析的数据支持；如果判定在一段代码中，堆上的所有数据都不会逃逸出去从而被其他线程访问到，那就可以把它们当做栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行了； 3）problem+solution 3.1）problem：程序员自己应该很清楚，怎么会在明知道不存在数据争用的case下还要求同步呢？ 3.2）solution：许多同步措施并不是程序员自己加入的，同步的代码在java程序中的普遍程度早就超过了大部分人的想象；（干货——许多同步措施并不是程序员自己加入的） 3.3）看个荔枝：这段code 仅仅是输出3个字符串相加的结果，无论是源码字面上还是程序语义上都没有同步；（干货——锁消除的荔枝） public class LockEliminateTest { // raw code public String concatString(String s1, String s2, String s3) { return s1 + s2 + s3; } // javac 转化后的字符串连接操作 public String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } } 对以上代码的分析（Analysis）： A1）对于 javac 转化后的字符串连接操作代码： 使用了同步，因为StringBuffer.append() 方法中都有一个同步块，锁就是sb对象。虚拟机观察变量sb，很快就会发现他的动态作用域被限制在 concatString() 方法内部；也就是所 sb 的所有引用都不会逃逸到方法之外； A2）所以，虽然这里有锁，但是可以被安全地消除掉，在即时编译之后，这段代码就会忽略掉所有的同步而直接执行了； 【3.3】锁粗化1）problem：如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗； 2）锁粗化的定义：如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部； 3）看个荔枝：以下面的代码为例，就是扩展到第一个 append() 操作前直到最后一个 append()操作之后，这样只需要加锁一次就可以了； // javac 转化后的字符串连接操作 public String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } 【3.4】轻量级锁1）重量级锁定义：使用操作系统互斥量来实现的传统锁；2）轻量级锁的目的：是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗；（干货——轻量级锁的作用）3）HotSpot虚拟机的对象头分为两部分信息：（干货——HotSpot虚拟机的对象头分为两部分信息）3.1）第一部分：用于存储对象自身的运行时数据，如哈希码，GC分代年龄等；这部分数据的长度在32位和64位的虚拟机中分别为 32bit 和 64bit，官方称它为 Mark Word，它是实现轻量级锁和偏向锁的关键；（干货——Mark Word 是实现轻量级锁和偏向锁的关键）3.2）第二部分：用于存储指向方法区对象类型数据的指针，如果是数组对象的话，还会有一个额外的部分用于存储数组长度；3.3）对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word 被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会工具对象的状态复用自己的存储空间；3.4）HotSpot 虚拟机对象头Mark Word 如下图所示： 4）在代码进入同步块的时候：4.1）轻量级锁的加锁过程：（干货——轻量级锁的加锁过程） step1）如果此同步对象没有被锁定（锁标志位为01状态）：虚拟机首先将在当前线程的栈帧中建立一个名为 锁记录的空间，用于存储对象目前的Mark Word 的拷贝； step2）然后，虚拟机将使用CAS 操作尝试将对象的 Mark Word 更新为指向 Lock Record的指针； step3）如果这个更新工作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位将转变为 00，即表示 此对象处于轻量级锁定状态； step4）如果这个更新失败了，虚拟机首先会检查对象的Mark Word 是否指向当前线程的栈帧，如果只说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明这个锁对象以及被其他线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁，锁标志的状态值变为 10，Mark Word中存储的就是指向重量级（互斥量）的指针，后面等待锁的线程也要进入阻塞状态； 4.2）轻量级锁的解锁过程：（干货——轻量级锁的解锁过程，其解锁过程也是通过CAS 操作来进行的） step1）如果对象的Mark Word仍然指向着线程的锁记录，那就用CAS 操作把对象当前的Mark Word 和 线程中复制的 Dispatched Mard Word替换回来； step2）如果替换成功，整个同步过程就over了； step3）如果替换失败，说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程；Conclusion） C1）轻量级锁能提升程序同步性能的依据是： 对于绝大部分的锁，在整个同步周期内都是不存在竞争的； C2）如果没有竞争，轻量级锁使用CAS 操作避免了使用互斥量的开销；但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS 操作，因此在有竞争的case下， 轻量级锁会比传统的重量级锁更慢； 【3.5】偏向锁 1）偏向锁的目的：消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能； 2）如果说轻量级锁是在无竞争的情况使用CAS 操作去消除同步使用的互斥量：那偏向锁就是在无竞争的情况下把整个同步都消除掉，连CAS 操作都不做了；（干货——偏向锁的定义） 3）偏向锁的偏： 它的意思是这个锁会偏向于 第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步； 4）偏向锁的原理：若当前虚拟机启用了偏向锁，那么，当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为01， 即偏向模式；同时使用CAS 操作把获取到这个锁的线程的ID 记录在对象的 Mark Word之中，如果 CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作；（干货——偏向锁的原理） 5）当有另一个线程去尝试获取这个锁时，偏向模式就结束了：根据锁对象目前是否处于被锁定的状态， 撤销偏向后恢复到未锁定（标志位为01）或轻量级锁定（标志位为00）的状态，后续的同步操作就如上面介绍的轻量级锁那样执行； Conclusion）C1）偏向锁可以提高带有同步但无竞争的程序性能；C2）如果程序中大多数的锁总是被多个不同的线程访问：那偏向模式是多余的；","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"Java并发编程：volatile关键字解析","slug":"Java并发编程：volatile关键字解析","date":"2017-08-13T02:56:31.000Z","updated":"2017-08-15T07:39:12.000Z","comments":true,"path":"2017/08/13/Java并发编程：volatile关键字解析/","link":"","permalink":"http://yoursite.com/2017/08/13/Java并发编程：volatile关键字解析/","excerpt":"最近阅读技术书籍有一个体会，就是接触到一些新的领域的时候可能需要先大致读一遍有一个雏形，然后再读抓住关键字，一定要体会到每一个关键的名字的意义才可能理解整句话的意思，最终我认为还是要就该领域的内容到网上看看广大同行们的看法和观点。这也是很重要的。曾经有一位阿里客户体验部的面试官问我一个问题，“volatile能否解决i++的并发问题”，问得我一脸茫然。最近对volatile关键字以及Java并发做了一次细致的了解，主要还是利用的周智明先生的《深入了解JVM》。这篇文章是我转载过来的，对该部分内容整理的比较细致，最重要的是通俗易懂，在读完周先生的书后在阅读一边此文真是感觉一切都通了。周先生写书还是有些天马行空的，哈哈。我根据我的理解把一些关键的要点整理了出来，并对其中一些内容作了删改。恳请原作者见谅。这是原文的地址：http://www.cnblogs.com/dolphin0520/p/3920373.html请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"最近阅读技术书籍有一个体会，就是接触到一些新的领域的时候可能需要先大致读一遍有一个雏形，然后再读抓住关键字，一定要体会到每一个关键的名字的意义才可能理解整句话的意思，最终我认为还是要就该领域的内容到网上看看广大同行们的看法和观点。这也是很重要的。曾经有一位阿里客户体验部的面试官问我一个问题，“volatile能否解决i++的并发问题”，问得我一脸茫然。最近对volatile关键字以及Java并发做了一次细致的了解，主要还是利用的周智明先生的《深入了解JVM》。这篇文章是我转载过来的，对该部分内容整理的比较细致，最重要的是通俗易懂，在读完周先生的书后在阅读一边此文真是感觉一切都通了。周先生写书还是有些天马行空的，哈哈。我根据我的理解把一些关键的要点整理了出来，并对其中一些内容作了删改。恳请原作者见谅。这是原文的地址：http://www.cnblogs.com/dolphin0520/p/3920373.html请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 volatile关键字要点与总结因为本文较长，所以我将一些最关键的要点和结论先做一下总结。如果要做细致的理解，通读全文还是很有必要的。1.并发编程中三个原则：原子性，可见性，有序性2.先行发生原则：保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。3.volatile关键字两层语义：##### 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。（满足可见性要求） ##### 2）禁止进行指令重排序。（一定程度上满足有序性要求） 4.volatile关键字局限：在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件：1）对变量的写操作不依赖于当前值2）该变量没有包含在具有其他变量的不变式中我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行，因为volatile关键字自身满足不了原子性的要求。5.针对volatile关键字局限的解决方案：1）利用synchronized2）利用Lock3）利用并发包下的atomic正文 volatile这个关键字可能很多朋友都听说过，或许也都用过。在Java 5之前，它是一个备受争议的关键字，因为在程序中使用它往往会导致出人意料的结果。在Java 5之后，volatile关键字才得以重获生机。 volatile关键字虽然从字面上理解起来比较简单，但是要用好不是一件容易的事情。由于volatile关键字是与Java的内存模型有关的，因此在讲述volatile关键之前，我们先来了解一下与内存模型相关的概念和知识，然后分析了volatile关键字的实现原理，最后给出了几个使用volatile关键字的场景。 以下是本文的目录大纲： 一.内存模型的相关概念 二.并发编程中的三个概念 三.Java内存模型 四..深入剖析volatile关键字 五.使用volatile关键字的场景 若有不正之处请多多谅解，并欢迎批评指正。 一.内存模型的相关概念 大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中，势必涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。因此在CPU里面就有了高速缓存。 也就是，当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。举个简单的例子，比如下面的这段代码： i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 这个代码在单线程中运行是没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。 最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 为了解决缓存不一致性问题，通常来说有以下2种解决方法： 1）通过在总线加LOCK#锁的方式 2）通过缓存一致性协议 这2种方式都是硬件层面上提供的方式。 在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。 所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 二.并发编程中的三个概念 在并发编程中，我们通常会遇到以下三个问题：原子性问题，可见性问题，有序性问题。我们先看具体看一下这三个概念： ####1.原子性 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 一个很经典的例子就是银行账户转账问题： 比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。 试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。 所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。 同样地反映到并发编程中会出现什么结果呢？ 举个最简单的例子，大家想一下假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？ i = 9; 假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。 那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。 2.可见性 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 举个简单的例子，看下面这段代码： //线程1执行的代码 int i = 0; i = 10; //线程2执行的代码 j = i; 假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中，然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。 此时线程2执行 j = i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10. 这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。 3.有序性 有序性：即程序执行的顺序按照代码的先后顺序执行。举个简单的例子，看下面这段代码： int i = 0; boolean flag = false; i = 1; //语句1 flag = true; //语句2 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。 但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子： int a = 10; //语句1 int r = 2; //语句2 a = a + 3; //语句3 r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是： 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3 不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子： //线程1: context = loadContext(); //语句1 inited = true; //语句2 //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。 从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。 也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 三.Java内存模型 在前面谈到了一些关于内存模型以及并发编程中可能会出现的一些问题。下面我们来看一下Java内存模型，研究一下Java内存模型为我们提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。 Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 举个简单的例子：在java中，执行下面这个语句： i = 10; 执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。 那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？ 1.原子性 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。 上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i： 请分析以下哪些操作是原子性操作： x = 10; //语句1 y = x; //语句2 x++; //语句3 x = x + 1; //语句4 咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。 语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 所以上面4个语句只有语句1的操作具备原子性。 也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。 不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 2.可见性 对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 3.有序性 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 下面就来具体介绍下happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 这8条原则摘自《深入理解Java虚拟机》。 这8条规则中，前4条规则是比较重要的，后4条规则都是显而易见的。 下面我们来解释一下前4条规则： 对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。 第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。 第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。 第四条规则实际上就是体现happens-before原则具备传递性。 四.深入剖析volatile关键字 在前面讲述了很多东西，其实都是为讲述volatile关键字作铺垫，那么接下来我们就进入主题。 1.volatile关键字的两层语义 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2）禁止进行指令重排序。 先看一段代码，假如线程1先执行，线程2后执行： //线程1 boolean stop = false; while(!stop){ doSomething(); } //线程2 stop = true; 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了： 第一：使用volatile关键字会强制将修改的值立即写入主存； 第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 那么线程1读取到的就是最新的正确的值。 2.volatile保证原子性吗？ 从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？ 下面看一个例子： public class Test { public volatile int inc = 0; public void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 假如某个时刻变量inc的值为10， 线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 那么两个线程分别进行了一次自增操作后，inc只增加了1。 解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 把上面的代码改成以下任何一种都可以达到效果： 采用synchronized： public class Test { public int inc = 0; public synchronized void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 采用Lock： public class Test { public int inc = 0; Lock lock = new ReentrantLock(); public void increase() { lock.lock(); try { inc++; } finally{ lock.unlock(); } } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 采用AtomicInteger： public class Test { public AtomicInteger inc = new AtomicInteger(); public void increase() { inc.getAndIncrement(); } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i&lt;10;i++){ new Thread(){ public void run() { for(int j=0;j&lt;1000;j++) test.increase(); }; }.start(); } while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。 3.volatile能保证有序性吗？ 在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 可能上面说的比较绕，举个简单的例子： //x、y为非volatile变量 //flag为volatile变量 x = 2; //语句1 y = 0; //语句2 flag = true; //语句3 x = 4; //语句4 y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。 那么我们回到前面举的一个例子： //线程1: context = loadContext(); //语句1 inited = true; //语句2 //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么久可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。 这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。 4.volatile的原理和实现机制 前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。 下面这段话摘自《深入理解Java虚拟机》： “观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令” lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 2）它会强制将对缓存的修改操作立即写入主存； 3）如果是写操作，它会导致其他CPU中对应的缓存行无效。 五.使用volatile关键字的场景 synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件： 1）对变量的写操作不依赖于当前值 2）该变量没有包含在具有其他变量的不变式中 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。 事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。 下面列举几个Java中使用volatile的几个场景。 1.状态标记量 volatile boolean flag = false; while(!flag){ doSomething(); } public void setFlag() { flag = true; } volatile boolean inited = false; //线程1: context = loadContext(); inited = true; //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 2.double check class Singleton{ private volatile static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if(instance==null) { synchronized (Singleton.class) { if(instance==null) instance = new Singleton(); } } return instance; } }","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"我为什么离开学校——十六岁那一年","slug":"我为什么离开学校——十六岁那一年","date":"2017-08-12T03:45:54.000Z","updated":"2017-08-25T10:46:07.000Z","comments":true,"path":"2017/08/12/我为什么离开学校——十六岁那一年/","link":"","permalink":"http://yoursite.com/2017/08/12/我为什么离开学校——十六岁那一年/","excerpt":"最近有人问我的经历，问为什么，我才想起好像好多人都问我，我却都没有好好回答。我不愿意谈论我自己，因为许多普通的群众并不值得听我说话，他们听不懂。懒得引用道德经的原话了。以前一无所有的时候谈论 自己显得 很可笑，现在多少有点进步了，想给多少听得懂的人们一个大致的答案，不至于让他们太过疑惑。 我的故事说来话长，但是我尽量长话短说。","text":"最近有人问我的经历，问为什么，我才想起好像好多人都问我，我却都没有好好回答。我不愿意谈论我自己，因为许多普通的群众并不值得听我说话，他们听不懂。懒得引用道德经的原话了。以前一无所有的时候谈论 自己显得 很可笑，现在多少有点进步了，想给多少听得懂的人们一个大致的答案，不至于让他们太过疑惑。 我的故事说来话长，但是我尽量长话短说。 毛泽东选集里面有这么一段话： 如果要直接地认识某种或某些事物，便只有亲身参加于变革现实、变革某种或某些事物的实践的斗争中，才能触到那种或那些事物的现象，也只有在亲身参加变革现实的实践的斗争中，才能暴露那种或那些事物的本质而理解他们。 我是十六岁那一年离开学校的，当时是高一，刚上高中。说到具体的感受就是两点，一个是失望，再一个就是没意思。原本我对高中生活有很多期待，以为会认识很多新的人，但是发现他们都一样，和以前的同学没有什么区别。我当时是感觉学也学够了，玩也玩够了，所以觉得没意思，就想出去看看外面的世界，见识见识外面的社会。 为什么说学也学够了玩也玩够了呢？可能以往很多不了解我的人以为我当时离开学校是因为学习太差在学校混不下去了，其实还真不是这样。我初中的时候学习上就一直挺拔尖的，我记得我当时中考是有六百多分。当然这些都不值一提，太微不足道了。基本上在我整个青春期的阶段我是学习和玩两不耽误，学习上也拼搏过了，也出过风头了，觉得成绩靠前被人认同就是那么回事儿。学生时代那些小打小闹的玩也完全提不起我的兴趣。因为成绩的原因当时我决定退学也是在学校的圈子里造成很大的震撼，我个人也是拿出了相当大的勇气，毕竟是物质基础塑造了一个人，而对于十六岁的我来说要完全脱离一直塑造我的学校环境和家庭环境，是需要真正独自承担一些东西的。而且在之后的几年里，才是独自承担的真正开始。就这样我先是离开了学校，就在同时离开了家。 当时具体的计划是去西藏看一看，我买了好几本骑行西藏的书，计划在社会上打工攒下一些钱。后来慢慢做市场工作全国到处跑，虽然没有去到过西藏，但是也非常有意义。举个例子来说，曾经有一个月我连续出差，当时是十一月份，从大连出发的时候天上已经开始飘雪，我先是去了南昌，当时南昌在下雨；然后坐高铁去南宁，南宁当时还特别热，满城绿色；然后再往南去了海口，海口更热，植被也更多。最后我去了武汉，在天河机场一下飞机我就蒙圈了，特别冷，干冷干冷。我当时身上就是单衣单裤。在武汉那些天也去了武汉的周边城市，襄阳等。我主要的工作是帮助省级代理商开大型招商会并以厂家身份帮助代理商与其客户进行沟通。 再到后来，我下定决心脱离舒适区，加入互联网行业，想抓住潮流的尾巴，做一回弄潮儿。于是走到今天。先后加入几家互联网公司，到今年创业。我从事互联网这段经历包括之前的那段市场经历包括再往前的社会经历有时间再说吧。 ps:在我刚离开学校的时候经常有人说：“你将来一定会后悔的”。事实上呢，我前期在社会上经常有人说：“当时在学校要是好好学习就好了”，这是别人后悔的例子。我却从未后悔过，从来没有。十六岁到二十岁其实是一个男人比较脆弱的时期，有想法却囿于身边无所不在的禁锢。这段时期的经历将塑造一个个体一生的的画面。我选择在中国社会这个熔炉之中，在煎熬和挣扎中淬炼。大多数时候，我都很煎熬 ，思想和灵魂的煎熬。在除了煎熬以外的时候，我都很自豪。 相反，我发现我在十六岁的时候对我的父母以及我的老师以及我的同学所做的评价在现在看来是比较中肯的。当时人们质疑我，觉得我是青春期叛逆，你的父母哪里对不起你？你的老师怎么会有问题？你的同学这么的优秀，前途一片光明。现在看来，看似的光明的，不一定会光明，选择了在黑暗中沉潜的，不一定不会有光明。小资产阶级的软弱、动摇、自私自利（毛主席语）在他们的身上体现的淋漓尽致，这也决定了他们的视野和眼光，固然也决定了他们的前途。一切都早已被预料到了。当然，人即使是欺骗自己，也是确信自己是时刻处在光明之中的，自己走的是一条光明坦途。","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"事务的四大特性以及事务的四种隔离级别","slug":"事务的四大特性以及事务的四种隔离级别","date":"2017-08-11T06:54:54.000Z","updated":"2017-08-15T07:39:24.000Z","comments":true,"path":"2017/08/11/事务的四大特性以及事务的四种隔离级别/","link":"","permalink":"http://yoursite.com/2017/08/11/事务的四大特性以及事务的四种隔离级别/","excerpt":"这是原文的地址：http://www.cnblogs.com/fjdingsd/p/5273008.html请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"这是原文的地址：http://www.cnblogs.com/fjdingsd/p/5273008.html请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 本篇讲诉数据库中事务的四大特性（ACID），并且将会详细地说明事务的隔离级别。 事务的四大特性如果一个数据库声称支持事务的操作，那么该数据库必须要具备以下四个特性： ⑴ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 ⑵ 一致性（Consistency） 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 ⑶ 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始之前就已经结束，要么在T1结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 关于事务的隔离性数据库提供了多种隔离级别，稍后会介绍到。 ⑷ 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们程序执行完成直到看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但是数据库因为故障而没有执行事务的重大错误。 以上介绍完事务的四大特性(简称ACID)，现在重点来说明下事务的隔离性，当多个线程都开启事务操作数据库中的数据时，数据库系统要能进行隔离操作，以保证各个线程获取数据的准确性，在介绍数据库提供的各种隔离级别之前，我们先看看如果不考虑事务的隔离性，会发生的几种问题： 1，脏读 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。 当一个事务正在多次修改某个数据，而在这个事务中这多次的修改都还未提交，这时一个并发的事务来访问该数据，就会造成两个事务得到的数据不一致。例如：用户A向用户B转账100元，对应SQL命令如下 update account set money=money+100 where name=’B’; (此时A通知B) update account set money=money - 100 where name=’A’; 当只执行第一条SQL时，A通知B查看账户，B发现确实钱已到账（此时即发生了脏读），而之后无论第二条SQL是否执行，只要该事务不提交，则所有操作都将回滚，那么当B以后再次查看账户时就会发现钱其实并没有转。 2，不可重复读 不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 例如事务T1在读取某一数据，而事务T2立马修改了这个数据并且提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发送了不可重复读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 在某些情况下，不可重复读并不是问题，比如我们多次查询某个数据当然以最后查询得到的结果为主。但在另一些情况下就有可能发生问题，例如对于同一个数据A和B依次查询就可能不同，A和B就可能打起来了…… 3，虚读(幻读) 幻读是事务非独立执行时发生的一种现象。例如事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 事务的四种隔离级别 现在来看看MySQL数据库为我们提供的四种隔离级别： ① Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 ② Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 ③ Read committed (读已提交)：可避免脏读的发生。 ④ Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 Read uncommitted读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。 事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。 分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读。 那怎么解决脏读呢？Read committed！读提交，能解决脏读问题。 Read committed读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的… 分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。 那怎么解决可能的不可重复读问题？Repeatable read ！ Repeatable read重复读，就是在开始读取数据（事务开启）时，不再允许修改操作 事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。 分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。 什么时候会出现幻读？事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。 那怎么解决幻读问题？Serializable！ Serializable 序列化Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 值得一提的是：大多数数据库默认的事务隔离级别是Read committed，比如Sql Server , Oracle。MySQL的默认隔离级别是Repeatable read。 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 在MySQL数据库中，支持上面四种隔离级别，默认的为Repeatable read (可重复读)；而在Oracle数据库中，只支持Serializable (串行化)级别和Read committed (读已提交)这两种级别，其中默认的为Read committed级别。 在MySQL数据库中查看当前事务的隔离级别： select @@tx_isolation; 在MySQL数据库中设置事务的隔离 级别： set [glogal | session] transaction isolation level 隔离级别名称; set tx_isolation=’隔离级别名称;’ 例1：查看当前事务的隔离级别： 例2：将事务的隔离级别设置为Read uncommitted级别： 或： 记住：设置数据库的隔离级别一定要是在开启事务之前！ 如果是使用JDBC对数据库的事务设置隔离级别的话，也应该是在调用Connection对象的setAutoCommit(false)方法之前。调用Connection对象的setTransactionIsolation(level)即可设置当前链接的隔离级别，至于参数level，可以使用Connection对象的字段： 在JDBC中设置隔离级别的部分代码： 后记：隔离级别的设置只对当前链接有效。对于使用MySQL命令窗口而言，一个窗口就相当于一个链接，当前窗口设置的隔离级别只对当前窗口中的事务有效；对于JDBC操作数据库来说，一个Connection对象相当于一个链接，而对于Connection对象设置的隔离级别只对该Connection对象有效，与其他链接Connection对象无关。 参考博客： http://www.zhihu.com/question/23989904 http://dev.mysql.com/doc/refman/5.6/en/set-transaction.html http://www.cnblogs.com/xdp-gacl/p/3984001.html","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/tags/Java基础/"}]},{"title":"悲观锁与乐观锁与事务","slug":"悲观锁与乐观锁与事务","date":"2017-08-10T02:56:31.000Z","updated":"2017-08-15T07:39:42.000Z","comments":true,"path":"2017/08/10/悲观锁与乐观锁与事务/","link":"","permalink":"http://yoursite.com/2017/08/10/悲观锁与乐观锁与事务/","excerpt":"参考地址：http://blog.csdn.net/hongchangfirst/article/details/26004335http://blog.csdn.net/zhangwj0101/article/details/50946054 请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"参考地址：http://blog.csdn.net/hongchangfirst/article/details/26004335http://blog.csdn.net/zhangwj0101/article/details/50946054 请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 悲观锁悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 乐观锁乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 对比两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。 锁与事务的关系数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 无论是悲观锁还是乐观锁，都是人们定义出来的概念，可以认为是一种思想。其实不仅仅是关系型数据库系统中有乐观锁和悲观锁的概念，像memcache、hibernate、tair等都有类似的概念。 针对于不同的业务场景，应该选用不同的并发控制方式。所以，不要把乐观并发控制和悲观并发控制狭义的理解为DBMS中的概念，更不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在DBMS中，悲观锁正是利用数据库本身提供的锁机制来实现的。","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"JAVA中Object类中的方法以及finalize函数作用","slug":" JAVA中Object类中的方法以及finalize函数作用","date":"2017-08-08T06:54:54.000Z","updated":"2017-08-21T04:02:06.000Z","comments":true,"path":"2017/08/08/ JAVA中Object类中的方法以及finalize函数作用/","link":"","permalink":"http://yoursite.com/2017/08/08/ JAVA中Object类中的方法以及finalize函数作用/","excerpt":"这篇文章对Object中所有的函数进行总结和梳理。Object是所有类的父类，任何类都默认继承Object。","text":"这篇文章对Object中所有的函数进行总结和梳理。Object是所有类的父类，任何类都默认继承Object。Object是所有类的父类，任何类都默认继承Object。 一、Object类中的方法1．clone方法保护方法，实现对象的浅复制，只有实现了Cloneable接口才可以调用该方法，否则抛出CloneNotSupportedException异常。 主要是Java里除了8种基本类型传参数是值传递，其他的类对象传参数都是引用传递，我们有时候不希望在方法里讲参数改变，这是就需要在类中复写clone方法。 2．getClass方法final方法，获得运行时类型。 3．toString方法该方法用得比较多，一般子类都有覆盖。 4．finalize方法该方法用于释放资源。因为无法确定该方法什么时候被调用，很少使用。 5．equals方法该方法是非常重要的一个方法。一般equals和==是不一样的，但是在Object中两者是一样的。子类一般都要重写这个方法。 6．hashCode方法该方法用于哈希查找，可以减少在查找中使用equals的次数，重写了equals方法一般都要重写hashCode方法。这个方法在一些具有哈希功能的Collection中用到。 一般必须满足obj1.equals(obj2)==true。可以推出obj1.hash- Code()==obj2.hashCode()，但是hashCode相等不一定就满足equals。不过为了提高效率，应该尽量使上面两个条件接近等价。 如果不重写hashcode(),在HashSet中添加两个equals的对象，会将两个对象都加入进去。 详细解释：1.hashcode是用来查找的，如果你学过数据结构就应该知道，在查找和排序这一章有例如内存中有这样的位置0 1 2 3 4 5 6 7而我有个类，这个类有个字段叫ID,我要把这个类存放在以上8个位置之一，如果不用hashcode而任意存放，那么当查找时就需要到这八个位置里挨个去找，或者用二分法一类的算法。但如果用hashcode那就会使效率提高很多。我们这个类中有个字段叫ID,那么我们就定义我们的hashcode为ID％8，然后把我们的类存放在取得得余数那个位置。比如我们的ID为9，9除8的余数为1，那么我们就把该类存在1这个位置，如果ID是13，求得的余数是5，那么我们就把该类放在5这个位置。这样，以后在查找该类时就可以通过ID除 8求余数直接找到存放的位置了。 2.但是如果两个类有相同的hashcode怎么办那（我们假设上面的类的ID不是唯一的），例如9除以8和17除以8的余数都是1，那么这是不是合法的，回答是：可以这样。那么如何判断呢？在这个时候就需要定义 equals了。也就是说，我们先通过 hashcode来判断两个类是否存放某个桶里，但这个桶里可能有很多类，那么我们就需要再通过 equals 来在这个桶里找到我们要的类。那么。重写了equals()，为什么还要重写hashCode()呢？想想，你要在一个桶里找东西，你必须先要找到这个桶啊，你不通过重写hashcode()来找到桶，光重写equals()有什么用啊 7．wait方法wait方法就是使当前线程等待该对象的锁，当前线程必须是该对象的拥有者，也就是具有该对象的锁。wait()方法一直等待，直到获得锁或者被中断。wait(long timeout)设定一个超时间隔，如果在规定时间内没有获得锁就返回。 调用该方法后当前线程进入睡眠状态，直到以下事件发生。 （1）其他线程调用了该对象的notify方法。 （2）其他线程调用了该对象的notifyAll方法。 （3）其他线程调用了interrupt中断该线程。 （4）时间间隔到了。 此时该线程就可以被调度了，如果是被中断的话就抛出一个InterruptedException异常。 8．notify方法该方法唤醒在该对象上等待的某个线程。 9．notifyAll方法该方法唤醒在该对象上等待的所有线程。 二、finalize（）的作用Java允许在类中定义一个名为finalize()的方法。它的工作原理是：一旦垃圾回收器准备好释放对象占用的存储空间，将首先调用其finalize()方法。并且在下一次垃圾回收动作发生时，才会真正回收对象占用的内存。 关于垃圾回收，有三点需要记住： 1、对象可能不被垃圾回收。只要程序没有濒临存储空间用完的那一刻，对象占用的空间就总也得不到释放。 2、垃圾回收并不等于“析构”。 3、垃圾回收只与内存有关。使用垃圾回收的唯一原因是为了回收程序不再使用的内存。 finalize()的用途： 无论对象是如何创建的，垃圾回收器都会负责释放对象占据的所有内存。这就将对finalize()的需求限制到一种特殊情况，即通过某种创建对象方式以外的方式为对象分配了存储空间。不过这种情况一般发生在使用“本地方法”的情况下，本地方法是一种在Java中调用非Java代码的方式。 为什么不能显示直接调用finalize方法？ 如前文所述，finalize方法在垃圾回收时一定会被执行，而如果在此之前显示执行的话，也就是说finalize会被执行两次以上，而在第一次资源已经被释放，那么在第二次释放资源时系统一定会报错，因此一般finalize方法的访问权限和父类保持一致，为protected。","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/tags/Java基础/"}]},{"title":"各RDB与Nosql性能与特点总结","slug":"各RDB与Nosql性能与特点总结","date":"2017-08-05T09:37:31.000Z","updated":"2017-08-18T06:46:08.000Z","comments":true,"path":"2017/08/05/各RDB与Nosql性能与特点总结/","link":"","permalink":"http://yoursite.com/2017/08/05/各RDB与Nosql性能与特点总结/","excerpt":"参考地址：http://www.besttest.cn/article/42.htmlhttp://www.cnblogs.com/crazylights/archive/2013/05/08/3066056.htmlhttp://blog.csdn.net/yumengkk/article/details/7902103 最近考虑到数据库包括各种缓存到底面对高并发情况性能到底是怎么样的，所以多方收集整理成此篇，以后也会持续更新。","text":"参考地址：http://www.besttest.cn/article/42.htmlhttp://www.cnblogs.com/crazylights/archive/2013/05/08/3066056.htmlhttp://blog.csdn.net/yumengkk/article/details/7902103 最近考虑到数据库包括各种缓存到底面对高并发情况性能到底是怎么样的，所以多方收集整理成此篇，以后也会持续更新。 mysql：1.性能从10万条规模升到100万条时降低非常明显，从100万到1000万性能降低更明显。 Memcached：1、Memcached是一个cache机制，当内存不足时会采用LRU机制，替换出陈旧数据，因此他不能保证我们的数据像在HashMap中一样不丢失，且没有数据持久化机制； redis：1、redis克服了这一缺点，采取磁盘存储机制实现数据持久化。但是，当数据量达到1千万左右时，由于内存中不能存储如此大量数目的数据，频繁同磁盘进行数据交换，导致数据查询、存储性能的急剧下降，将导致服务不可用。 ###redis作者在stackoverflow上关于redis与memcache对比的一段话： 没有必要过于关注性能，因为二者的性能都已经足够高了。由于Redis只使用单核，而Memcached可以使用多核，所以二者比较起来，平均每一个核上，Redis在存储小数据时比Memcached性能更高。而在100k以上的数据中，Memcached性能要高于Redis。虽然Redis最近也在存储大数据的性能上进行优化，但是比起Memcached，还是稍有逊色。说了这么多，结论是，无论你使用哪一个，每秒处理请求的次数都不会成为瓶颈。 在内存使用效率上，如果使用简单的key-value存储，Memcached的内存利用率更高。而如果Redis采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于Memcached。当然，这和你的应用场景和数据特性有关。 如果你对数据持久化和数据同步有所要求，那么推荐你选择Redis。因为这两个特性Memcached都不具备。即使你只是希望在升级或者重启系统后缓存数据不会丢失，选择Redis也是明智的。 当然，最后还得说到你的具体应用需求。Redis相比Memcached来说，拥有更多的数据结构，并支持更丰富的数据操作。通常在Memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。在Redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果你需要缓存能够支持更复杂的结构和操作，那么Redis会是不错的选择。 mongo：1.不设其他唯一索引的情况下，只用_id 在普通办公电脑上每秒插入几万，在普通x86服务器上每秒插入十几万，，比mysql强出一个数量级。 2.检索是真的慢，和sql数据库不同，越复杂的条件搜索MangoDB越吃亏，CPU和IO的双重压力。面对那些直接把SQL查询改写成MangoDB的用法，别转了，你不会收获任何性能提升。 结论：当前还没有好的产品可以实现key-value保证数据完整性，千万级条数量级的，高效存储和查询支持产品。","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"Java容器各实现类的底层实现原理","slug":"Java容器各实现类的底层实现原理","date":"2017-08-05T05:18:31.000Z","updated":"2017-08-25T10:43:03.000Z","comments":true,"path":"2017/08/05/Java容器各实现类的底层实现原理/","link":"","permalink":"http://yoursite.com/2017/08/05/Java容器各实现类的底层实现原理/","excerpt":"参考地址：http://blog.csdn.net/qq_25868207/article/details/55259978http://blog.csdn.net/zcbyzcb/article/details/70666438 在原作基础做了一些删减和补充，请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"参考地址：http://blog.csdn.net/qq_25868207/article/details/55259978http://blog.csdn.net/zcbyzcb/article/details/70666438 在原作基础做了一些删减和补充，请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 ArrayList实现原理要点概括参考文献：http://zhangshixi.iteye.com/blog/674856l ArrayList是List接口的可变数组非同步实现，并允许包括null在内的所有元素。 底层使用数组实现 该集合是可变长度数组，数组扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量增长大约是其容量的1.5倍，这种操作的代价很高。 采用了Fail-Fast机制，面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险 LinkedList实现原理要点概括参考文献：http://www.cnblogs.com/ITtangtang/p/3948610.htmll LinkedList是List接口的双向链表非同步实现，并允许包括null在内的所有元素。 底层的数据结构是基于双向链表的，该数据结构我们称为节点 双向链表节点对应的类Entry的实例，Entry中包含成员变量：previous，next，element。其中，previous是该节点的上一个节点，next是该节点的下一个节点，element是该节点所包含的值。 HashMap实现原理要点概括参考文献：http://zhangshixi.iteye.com/blog/672697l HashMap是基于哈希表的Map接口的非同步实现，允许使用null值和null键，但不保证映射的顺序。 底层使用数组实现，数组中每一项是个链表，即数组和链表的结合体 HashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。HashMap底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 HashMap进行数组扩容需要重新计算扩容后每个元素在数组中的位置，很耗性能 采用了Fail-Fast机制，通过一个modCount值记录修改次数，对HashMap内容的修改都将增加这个值。迭代器初始化过程中会将这个值赋给迭代器的expectedModCount，在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map，马上抛出异常 Hashtable实现原理要点概括参考文献：http://blog.csdn.net/zheng0518/article/details/42199477 Hashtable是基于哈希表的Map接口的同步实现，不允许使用null值和null键 底层使用数组实现，数组中每一项是个单链表，即数组和链表的结合体 Hashtable在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 synchronized是针对整张Hash表的，即每次锁住整张表让线程独占 ConcurrentHashMap实现原理要点概括参考文献：http://blog.csdn.net/zheng0518/article/details/42199477 ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对hash表的不同段进行的修改，每个段其实就是一个小的hashtable，它们有自己的锁。只要多个并发发生在不同的段上，它们就可以并发进行。 ConcurrentHashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。 Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 与HashMap不同的是，ConcurrentHashMap使用多个子Hash表，也就是段(Segment) ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。如果使用传统的技术，如HashMap中的实现，如果允许可以在hash链的中间添加或删除元素，读操作不加锁将得到不一致的数据。ConcurrentHashMap实现技术是保证HashEntry几乎是不可变的。 HashSet实现原理要点概括参考文献：http://zhangshixi.iteye.com/blog/673143l HashSet由哈希表(实际上是一个HashMap实例)支持，不保证set的迭代顺序，并允许使用null元素。基于HashMap实现，API也是对HashMap的行为进行了封装，可参考HashMap LinkedHashMap实现原理要点概括参考文献：http://zhangshixi.iteye.com/blog/673789l LinkedHashMap继承于HashMap，底层使用哈希表和双向链表来保存所有元素，并且它是非同步，允许使用null值和null键。 基本操作与父类HashMap相似，通过重写HashMap相关方法，重新定义了数组中保存的元素Entry，来实现自己的链接列表特性。该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而构成了双向链接列表。 LinkedHashSet实现原理要点概括参考文献：http://zhangshixi.iteye.com/blog/673319l 对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同。","categories":[],"tags":[{"name":"Java容器相关","slug":"Java容器相关","permalink":"http://yoursite.com/tags/Java容器相关/"}]},{"title":"秒杀系统解决方案","slug":"秒杀系统解决方案","date":"2017-08-01T02:56:31.000Z","updated":"2017-08-16T10:10:54.000Z","comments":true,"path":"2017/08/01/秒杀系统解决方案/","link":"","permalink":"http://yoursite.com/2017/08/01/秒杀系统解决方案/","excerpt":"总结1、前端三板斧【扩容】【限流】【静态化】2、后端两条路【内存】+【排队】 我看了二十篇左右的秒杀系统设计及解决方案的文章，觉得这篇文章思路比较清晰，比较容易懂，所以贴出来。技术上基本都会采用redis，本文下面有其他一些文章的推荐，侧重点不同，都是很有益处的。读多了会发现基本的解决思路是一致的。 原文参考地址：http://blog.csdn.net/WeiJiaXiaoBao/article/details/50173785 请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"总结1、前端三板斧【扩容】【限流】【静态化】2、后端两条路【内存】+【排队】 我看了二十篇左右的秒杀系统设计及解决方案的文章，觉得这篇文章思路比较清晰，比较容易懂，所以贴出来。技术上基本都会采用redis，本文下面有其他一些文章的推荐，侧重点不同，都是很有益处的。读多了会发现基本的解决思路是一致的。 原文参考地址：http://blog.csdn.net/WeiJiaXiaoBao/article/details/50173785 请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 一、秒杀带来了什么？ 秒杀或抢购活动一般会经过【预约】【抢订单】【支付】这3个大环节，而其中【抢订单】这个环节是最考验业务提供方的抗压能力的。 抢订单环节一般会带来2个问题： 1、高并发 比较火热的秒杀在线人数都是10w起的，如此之高的在线人数对于网站架构从前到后都是一种考验。 2、超卖 任何商品都会有数量上限，如何避免成功下订单买到商品的人数不超过商品数量的上限，这是每个抢购活动都要面临的难题。 二、如何解决？ 首先，产品解决方案我们就不予讨论了。我们只讨论技术解决方案 1、前端 面对高并发的抢购活动，前端常用的三板斧是【扩容】【静态化】【限流】 A：扩容 加机器，这是最简单的方法，通过增加前端池的整体承载量来抗峰值。 B：静态化 将活动页面上的所有可以静态的元素全部静态化，并尽量减少动态元素。通过CDN来抗峰值。 C：限流 一般都会采用IP级别的限流，即针对某一个IP，限制单位时间内发起请求数量。 或者活动入口的时候增加游戏或者问题环节进行消峰操作。 D：有损服务 最后一招，在接近前端池承载能力的水位上限的时候，随机拒绝部分请求来保护活动整体的可用性。 2、后端 那么后端的数据库在高并发和超卖下会遇到什么问题呢？主要会有如下3个问题：（主要讨论写的问题，读的问题通过增加cache可以很容易的解决） I： 首先MySQL自身对于高并发的处理性能就会出现问题，一般来说，MySQL的处理性能会随着并发thread上升而上升，但是到了一定的并发度之后会出现明显的拐点，之后一路下降，最终甚至会比单thread的性能还要差。 II： 其次，超卖的根结在于减库存操作是一个事务操作，需要先select，然后insert，最后update -1。最后这个-1操作是不能出现负数的，但是当多用户在有库存的情况下并发操作，出现负数这是无法避免的。 III：最后，当减库存和高并发碰到一起的时候，由于操作的库存数目在同一行，就会出现争抢InnoDB行锁的问题，导致出现互相等待甚至死锁，从而大大降低mysql的处理性能，最终导致前端页面出现超时异常。 针对上述问题，如何解决呢？ 我们先看眼淘宝的高大上解决方案： I： 关闭死锁检测，提高并发处理性能。 II：修改源代码，将排队提到进入引擎层前，降低引擎层面的并发度。 III：组提交，降低server和引擎的交互次数，降低IO消耗。 以上内容可以参考丁奇在DTCC2013上分享的《秒杀场景下MySQL的低效》一文。在文中所有优化都使用后，TPS在高并发下，从原始的150飙升到8.5w，提升近566倍，非常吓人！！！ 不过结合我们的实际，改源码这种高大上的解决方案显然有那么一点不切实际。于是小伙伴们需要讨论出一种适合我们实际情况的解决方案。以下就是我们讨论的解决方案： 首先设定一个前提，为了防止超卖现象，所有减库存操作都需要进行一次减后检查，保证减完不能等于负数。（由于MySQL事务的特性，这种方法只能降低超卖的数量，但是不可能完全避免超卖） update number set x=x-1 where (x -1）&gt;= 0; 解决方案1： 将存库从MySQL前移到Redis中，所有的写操作放到内存中，由于redis中不存在锁故不会出现互相等待，并且由于Redis的写性能和读性能都远高于MySQL，这就解决了高并发下的性能问题。然后通过队列等异步手段，将变化的数据异步写入到DB中。 优点：解决性能问题 缺点：没有解决超卖问题，同时由于异步写入DB，存在某一时刻DB和Redis中数据不一致的风险。 解决方案2： 引入队列，然后将所有写DB操作在单队列中排队，完全串行处理。当达到库存阀值的时候就不在消费队列，并关闭购买功能。这就解决了超卖问题。 优点：解决超卖问题，略微提升性能。 缺点：性能受限于队列处理机处理性能和DB的写入性能中最短的那个，另外多商品同时抢购的时候需要准备多条队列。 解决方案3： 将写操作前移到MC中，同时利用MC的轻量级的锁机制CAS来实现减库存操作。 优点：读写在内存中，操作性能快，引入轻量级锁之后可以保证同一时刻只有一个写入成功，解决减库存问题。 缺点：没有实测，基于CAS的特性不知道高并发下是否会出现大量更新失败？不过加锁之后肯定对并发性能会有影响。 解决方案4： 将提交操作变成两段式，先申请后确认。然后利用Redis的原子自增操作（相比较MySQL的自增来说没有空洞），同时利用Redis的事务特性来发号，保证拿到小于等于库存阀值的号的人都可以成功提交订单。然后数据异步更新到DB中。 优点：解决超卖问题，库存读写都在内存中，故同时解决性能问题。 缺点：由于异步写入DB，可能存在数据不一致。另可能存在少买，也就是如果拿到号的人不真正下订单，可能库存减为0，但是订单数并没有达到库存阀值。 三、总结 1、前端三板斧【扩容】【限流】【静态化】 2、后端两条路【内存】+【排队】 四、非技术感想 1、团队的力量是无穷的，各种各样的解决方案（先不谈可行性）都是在小伙伴们七嘴八舌中讨论出来的。我们需要让所有人都发出自己的声音，不要着急去否定。 2、优化需要从整体层面去思考，不要只纠结于自己负责的部分，如果只盯着一个点思考，最后很可能就走进死胡同中了。 3、有很多东西以为读过了就懂了，其实不然。依然还是需要实践，否则别人的知识永远不可能变成自己的。 4、多思考为什么，会发生什么，不要想当然。只有这样才能深入进去，而不是留在表面。 ps：以上仅仅是我们讨论的一些方案设想，欢迎大家一起讨论各种可行方案。 其他秒杀方案参考地址：徐汉彬先生的电商秒杀与抢购（着重在可能遇到的细节问题的讲解）：http://www.csdn.net/article/2014-11-28/2822858 陶邦仁先生的秒杀系统架构分析与实战（基本思路与本文一致，着重在各个节点解决方案的实现，很良心）：https://my.oschina.net/xianggao/blog/524943 主打redis的一个方案，比较笼统：http://blog.csdn.net/shendl/article/details/51092916","categories":[],"tags":[{"name":"高并发方案","slug":"高并发方案","permalink":"http://yoursite.com/tags/高并发方案/"}]},{"title":"Java类加载机制原理解析","slug":"Java类加载机制原理解析","date":"2017-07-25T04:19:54.000Z","updated":"2017-08-18T06:45:08.000Z","comments":true,"path":"2017/07/25/Java类加载机制原理解析/","link":"","permalink":"http://yoursite.com/2017/07/25/Java类加载机制原理解析/","excerpt":"Java类加载机制原理解析","text":"Java类加载机制原理解析 转载 地址：http://blog.csdn.net/zhoudaxia/article/details/35824249 1 基本信息每个开发人员对Java.lang.ClassNotFoundExcetpion这个异常肯定都不陌生，这背后就涉及到了java技术体系中的类加载。Java的类加载机制是技术体系中比较核心的部分，虽然和大部分开发人员直接打交道不多，但是对其背后的机理有一定理解有助于排查程序中出现的类加载失败等技术问题，对理解java虚拟机的连接模型和java语言的动态性都有很大帮助。 2 Java虚拟机类加载器结构简述2.1 JVM三种预定义类型类加载器 我们首先看一下JVM预定义的三种类型类加载器，当一个 JVM启动的时候，Java缺省开始使用如下三种类型类装入器： 启动（Bootstrap）类加载器： 引导类装入器是用本地代码实现的类装入器，它负责将 /lib下面的核心类库或-Xbootclasspath选项指定的jar包加载到内存中。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作。 扩展（Extension）类加载器：扩展类加载器是由Sun的ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将&lt; Java_Runtime_Home &gt;/lib/ext或者由系统变量-Djava.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器。 系统（System）类加载器：系统类加载器是由 Sun的 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。它负责将系统类路径java -classpath或-Djava.class.path变量所指的目录下的类库加载到内存中。开发者可以直接使用系统类加载器。 除了以上列举的三种类加载器，还有一种比较特殊的类型就是线程上下文类加载器，这个将在后面单独介绍。 2.2 类加载双亲委派机制介绍和分析在这里，需要着重说明的是，JVM在加载类时默认采用的是双亲委派机制。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。关于虚拟机默认的双亲委派机制，我们可以从系统类加载器和扩展类加载器为例作简单分析。 图一 标准扩展类加载器继承层次图 图二系统类加载器继承层次图 通过图一和图二我们可以看出，类加载器均是继承自java.lang.ClassLoader抽象类。我们下面我们就看简要介绍一下java.lang.ClassLoader中几个最重要的方法： [java] view plain copy 1. //加载指定名称（包括包名）的二进制类型，供用户调用的接口 2. public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException{ … } 3. 4. //加载指定名称（包括包名）的二进制类型，同时指定是否解析（但是这里的resolve参数不一定真正能达到解析的效果），供继承用 5. protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException{ … } 6. 7. //findClass方法一般被loadClass方法调用去加载指定名称类，供继承用 8. protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { … } 9. 10. //定义类型，一般在findClass方法中读取到对应字节码后调用，可以看出不可继承 11. //（说明：JVM已经实现了对应的具体功能，解析对应的字节码，产生对应的内部数据结构放置到方法区，所以无需覆写，直接调用就可以了） 12. protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len) throws ClassFormatError{ … } 通过进一步分析标准扩展类加载器（sun.misc.Launcher$ExtClassLoader）和系统类加载器（sun.misc.Launcher$AppClassLoader）的代码以及其公共父类（java.net.URLClassLoader和java.security.SecureClassLoader）的代码可以看出，都没有覆写java.lang.ClassLoader中默认的加载委派规则—loadClass（…）方法。既然这样，我们就可以通过分析java.lang.ClassLoader中的loadClass（String name）方法的代码就可以分析出虚拟机默认采用的双亲委派机制到底是什么模样： 1. public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException { 2. return loadClass(name, false); 3. } 4. 5. protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve) 6. throws ClassNotFoundException { 7. 8. // 首先判断该类型是否已经被加载 9. Class c = findLoadedClass(name); 10. if (c == null) { 11. //如果没有被加载，就委托给父类加载或者委派给启动类加载器加载 12. try { 13. if (parent != null) { 14. //如果存在父类加载器，就委派给父类加载器加载 15. c = parent.loadClass(name, false); 16. } else { 17. //如果不存在父类加载器，就检查是否是由启动类加载器加载的类， 18. //通过调用本地方法native findBootstrapClass0(String name) 19. c = findBootstrapClass0(name); 20. } 21. } catch (ClassNotFoundException e) { 22. // 如果父类加载器和启动类加载器都不能完成加载任务，才调用自身的加载功能 23. c = findClass(name); 24. } 25. } 26. if (resolve) { 27. resolveClass(c); 28. } 29. return c; 30. } 通过上面的代码分析，我们可以对JVM采用的双亲委派类加载机制有了更感性的认识，下面我们就接着分析一下启动类加载器、标准扩展类加载器和系统类加载器三者之间的关系。可能大家已经从各种资料上面看到了如下类似的一幅图片： 图三 类加载器默认委派关系图 上面图片给人的直观印象是系统类加载器的父类加载器是标准扩展类加载器，标准扩展类加载器的父类加载器是启动类加载器，下面我们就用代码具体测试一下： [java] view plain copy 1. public class LoaderTest { 2. 3. public static void main(String[] args) { 4. try { 5. System.out.println(ClassLoader.getSystemClassLoader()); 6. System.out.println(ClassLoader.getSystemClassLoader().getParent()); 7. System.out.println(ClassLoader.getSystemClassLoader().getParent().getParent()); 8. } catch (Exception e) { 9. e.printStackTrace(); 10. } 11. } 12. } 说明：通过java.lang.ClassLoader.getSystemClassLoader()可以直接获取到系统类加载器。 代码输出如下： [plain] view plain copy 1. sun.misc.Launcher$AppClassLoader@6d06d69c 2. sun.misc.Launcher$ExtClassLoader@70dea4e 3. null 通过以上的代码输出，我们可以判定系统类加载器的父加载器是标准扩展类加载器，但是我们试图获取标准扩展类加载器的父类加载器时确得到了null，就是说标准扩展类加载器本身强制设定父类加载器为null。我们还是借助于代码分析一下。 我们首先看一下java.lang.ClassLoader抽象类中默认实现的两个构造函数： [java] view plain copy 1. protected ClassLoader() { 2. SecurityManager security = System.getSecurityManager(); 3. if (security != null) { 4. security.checkCreateClassLoader(); 5. } 6. //默认将父类加载器设置为系统类加载器，getSystemClassLoader()获取系统类加载器 7. this.parent = getSystemClassLoader(); 8. initialized = true; 9. } 10. 11. protected ClassLoader(ClassLoader parent) { 12. SecurityManager security = System.getSecurityManager(); 13. if (security != null) { 14. security.checkCreateClassLoader(); 15. } 16. //强制设置父类加载器 17. this.parent = parent; 18. initialized = true; 19. } 我们再看一下ClassLoader抽象类中parent成员的声明： [java] view plain copy // The parent class loader for delegation private ClassLoader parent; 声明为私有变量的同时并没有对外提供可供派生类访问的public或者protected设置器接口（对应的setter方法），结合前面的测试代码的输出，我们可以推断出： 1． 系统类加载器（AppClassLoader）调用ClassLoader(ClassLoader parent)构造函数将父类加载器设置为标准扩展类加载器(ExtClassLoader)。（因为如果不强制设置，默认会通过调用getSystemClassLoader()方法获取并设置成系统类加载器，这显然和测试输出结果不符。） 2． 扩展类加载器（ExtClassLoader）调用ClassLoader(ClassLoader parent)构造函数将父类加载器设置为null。（因为如果不强制设置，默认会通过调用getSystemClassLoader()方法获取并设置成系统类加载器，这显然和测试输出结果不符。） 现在我们可能会有这样的疑问：扩展类加载器（ExtClassLoader）的父类加载器被强制设置为null了，那么扩展类加载器为什么还能将加载任务委派给启动类加载器呢？ 通过图四和图五可以看出，标准扩展类加载器和系统类加载器及其父类（java.NET.URLClassLoader和java.security.SecureClassLoader）都没有覆写java.lang.ClassLoader中默认的加载委派规则—loadClass（…）方法。有关java.lang.ClassLoader中默认的加载委派规则前面已经分析过，如果父加载器为null，则会调用本地方法进行启动类加载尝试。所以，图三中，启动类加载器、标准扩展类加载器和系统类加载器之间的委派关系事实上是仍就成立的。（在后面的用户自定义类加载器部分，还会做更深入的分析）。 2.3 类加载双亲委派示例 以上已经简要介绍了虚拟机默认使用的启动类加载器、标准扩展类加载器和系统类加载器，并以三者为例结合JDK代码对JVM默认使用的双亲委派类加载机制做了分析。下面我们就来看一个综合的例子。首先在IDE中建立一个简单的java应用工程，然后写一个简单的JavaBean如下： [java] view plain copy 1. package classloader.test.bean; 2. 3. public class TestBean { 4. 5. public TestBean() { } 6. } 在现有当前工程中另外建立一测试类（ClassLoaderTest.java）内容如下： 测试一： [java] view plain copy 1. package classloader.test.bean; 2. 3. public class ClassLoaderTest { 4. 5. public static void main(String[] args) { 6. try { 7. //查看当前系统类路径中包含的路径条目 8. System.out.println(System.getProperty(&quot;java.class.path&quot;)); 9. //调用加载当前类的类加载器（这里即为系统类加载器）加载TestBean 10. Class typeLoaded = Class.forName(&quot;classloader.test.bean.TestBean&quot;); 11. //查看被加载的TestBean类型是被那个类加载器加载的 12. System.out.println(typeLoaded.getClassLoader()); 13. } catch (Exception e) { 14. e.printStackTrace(); 15. } 16. } 17. } 对应的输出如下： [plain] view plain copy 1. C:\\Users\\JackZhou\\Documents\\NetBeansProjects\\ClassLoaderTest\\build\\classes 2. sun.misc.Launcher$AppClassLoader@73d16e93 说明：当前类路径默认的含有的一个条目就是工程的输出目录。 测试二： 将当前工程输出目录下的TestBean.class打包进test.jar剪贴到/lib/ext目录下（现在工程输出目录下和JRE扩展目录下都有待加载类型的class文件）。再运行测试一测试代码，结果如下： [plain] view plain copy C:\\Users\\JackZhou\\Documents\\NetBeansProjects\\ClassLoaderTest\\build\\classes sun.misc.Launcher$ExtClassLoader@15db9742 对比测试一和测试二，我们明显可以验证前面说的双亲委派机制，系统类加载器在接到加载classloader.test.bean.TestBean类型的请求时，首先将请求委派给父类加载器（标准扩展类加载器），标准扩展类加载器抢先完成了加载请求。 测试三： 将test.jar拷贝一份到/lib下，运行测试代码，输出如下： [plain] view plain copy 1. C:\\Users\\JackZhou\\Documents\\NetBeansProjects\\ClassLoaderTest\\build\\classes 2. sun.misc.Launcher$ExtClassLoader@15db9742 测试三和测试二输出结果一致。 那就是说，放置到/lib目录下的TestBean对应的class字节码并没有被加载，这其实和前面讲的双亲委派机制并不矛盾。虚拟机出于安全等因素考虑，不会加载/lib存在的陌生类，开发者通过将要加载的非JDK自身的类放置到此目录下期待启动类加载器加载是不可能的。做个进一步验证，删除/lib/ext目录下和工程输出目录下的TestBean对应的class文件，然后再运行测试代码，则将会有ClassNotFoundException异常抛出。有关这个问题，大家可以在java.lang.ClassLoader中的loadClass(String name, boolean resolve)方法中设置相应断点运行测试三进行调试，会发现findBootstrapClass0()会抛出异常，然后在下面的findClass方法中被加载，当前运行的类加载器正是扩展类加载器（sun.misc.Launcher$ExtClassLoader），这一点可以通过JDT中变量视图查看验证。 3 java程序动态扩展方式Java的连接模型允许用户运行时扩展引用程序，既可以通过当前虚拟机中预定义的加载器加载编译时已知的类或者接口，又允许用户自行定义类装载器，在运行时动态扩展用户的程序。通过用户自定义的类装载器，你的程序可以装载在编译时并不知道或者尚未存在的类或者接口，并动态连接它们并进行有选择的解析。 运行时动态扩展java应用程序有如下两个途径： 3.1 调用java.lang.Class.forName(…)加载类这个方法其实在前面已经讨论过，在后面的问题2解答中说明了该方法调用会触发哪个类加载器开始加载任务。这里需要说明的是多参数版本的forName(…)方法： [java] view plain copy 1. public static Class&lt;?&gt; forName(String name, boolean initialize, ClassLoader loader) throws ClassNotFoundException 这里的initialize参数是很重要的。它表示在加载同时是否完成初始化的工作（说明：单参数版本的forName方法默认是完成初始化的）。有些场景下需要将initialize设置为true来强制加载同时完成初始化。例如典型的就是利用DriverManager进行JDBC驱动程序类注册的问题。因为每一个JDBC驱动程序类的静态初始化方法都用DriverManager注册驱动程序，这样才能被应用程序使用。这就要求驱动程序类必须被初始化，而不单单被加载。Class.forName的一个很常见的用法就是在加载数据库驱动的时候。如 Class.forName(“org.apache.derby.jdbc.EmbeddedDriver”).newInstance()用来加载 Apache Derby 数据库的驱动。 3.2 用户自定义类加载器 通过前面的分析，我们可以看出，除了和本地实现密切相关的启动类加载器之外，包括标准扩展类加载器和系统类加载器在内的所有其他类加载器我们都可以当做自定义类加载器来对待，唯一区别是是否被虚拟机默认使用。前面的内容中已经对java.lang.ClassLoader抽象类中的几个重要的方法做了介绍，这里就简要叙述一下一般用户自定义类加载器的工作流程吧（可以结合后面问题解答一起看）： 1、首先检查请求的类型是否已经被这个类装载器装载到命名空间中了，如果已经装载，直接返回；否则转入步骤2； 2、委派类加载请求给父类加载器（更准确的说应该是双亲类加载器，真实虚拟机中各种类加载器最终会呈现树状结构），如果父类加载器能够完成，则返回父类加载器加载的Class实例；否则转入步骤3； 3、调用本类加载器的findClass（…）方法，试图获取对应的字节码，如果获取的到，则调用defineClass（…）导入类型到方法区；如果获取不到对应的字节码或者其他原因失败，返回异常给loadClass（…）， loadClass（…）转而抛异常，终止加载过程（注意：这里的异常种类不止一种）。 说明：这里说的自定义类加载器是指JDK 1.2以后版本的写法，即不覆写改变java.lang.loadClass(…)已有委派逻辑情况下。 4 常见问题分析4.1 由不同的类加载器加载的指定类还是相同的类型吗？在Java中，一个类用其完全匹配类名(fully qualified class name)作为标识，这里指的完全匹配类名包括包名和类名。但在JVM中一个类用其全名和一个加载类ClassLoader的实例作为唯一标识，不同类加载器加载的类将被置于不同的命名空间。我们可以用两个自定义类加载器去加载某自定义类型（注意不要将自定义类型的字节码放置到系统路径或者扩展路径中，否则会被系统类加载器或扩展类加载器抢先加载），然后用获取到的两个Class实例进行java.lang.Object.equals（…）判断，将会得到不相等的结果。这个大家可以写两个自定义的类加载器去加载相同的自定义类型，然后做个判断；同时，可以测试加载java.*类型，然后再对比测试一下测试结果。 4.2 在代码中直接调用Class.forName(String name)方法，到底会触发那个类加载器进行类加载行为？Class.forName(String name)默认会使用调用类的类加载器来进行类加载。我们直接来分析一下对应的jdk的代码： [java] view plain copy 1. //java.lang.Class.java 2. publicstatic Class&lt;?&gt; forName(String className) throws ClassNotFoundException { 3. return forName0(className, true, ClassLoader.getCallerClassLoader()); 4. } 5. 6. //java.lang.ClassLoader.java 7. // Returns the invoker&apos;s class loader, or null if none. 8. static ClassLoader getCallerClassLoader() { 9. // 获取调用类（caller）的类型 10. Class caller = Reflection.getCallerClass(3); 11. // This can be null if the VM is requesting it 12. if (caller == null) { 13. return null; 14. } 15. // 调用java.lang.Class中本地方法获取加载该调用类（caller）的ClassLoader 16. return caller.getClassLoader0(); 17. } 18. 19. //java.lang.Class.java 20. //虚拟机本地实现，获取当前类的类加载器，前面介绍的Class的getClassLoader()也使用此方法 21. native ClassLoader getClassLoader0(); 4.3 在编写自定义类加载器时，如果没有设定父加载器，那么父加载器是谁？ 前面讲过，在不指定父类加载器的情况下，默认采用系统类加载器。可能有人觉得不明白，现在我们来看一下JDK对应的代码实现。众所周知，我们编写自定义的类加载器直接或者间接继承自java.lang.ClassLoader抽象类，对应的无参默认构造函数实现如下： [java] view plain copy 1. //摘自java.lang.ClassLoader.java 2. protected ClassLoader() { 3. SecurityManager security = System.getSecurityManager(); 4. if (security != null) { 5. security.checkCreateClassLoader(); 6. } 7. this.parent = getSystemClassLoader(); 8. initialized = true; 9. } 我们再来看一下对应的getSystemClassLoader()方法的实现： [java] view plain copy 1. private static synchronized void initSystemClassLoader() { 2. //... 3. sun.misc.Launcher l = sun.misc.Launcher.getLauncher(); 4. scl = l.getClassLoader(); 5. //... 6. } 我们可以写简单的测试代码来测试一下： [java] view plain copy 1. System.out.println(sun.misc.Launcher.getLauncher().getClassLoader()); 本机对应输出如下： [plain] view plain copy 1. sun.misc.Launcher$AppClassLoader@73d16e93 所以，我们现在可以相信当自定义类加载器没有指定父类加载器的情况下，默认的父类加载器即为系统类加载器。同时，我们可以得出如下结论：即使用户自定义类加载器不指定父类加载器，那么，同样可以加载如下三个地方的类： 1. /lib下的类； 2. &lt; Java_Runtime_Home &gt;/lib/ext下或者由系统变量java.ext.dir指定位置中的类； 3. 当前工程类路径下或者由系统变量java.class.path指定位置中的类。 4.4 在编写自定义类加载器时，如果将父类加载器强制设置为null，那么会有什么影响？如果自定义的类加载器不能加载指定类，就肯定会加载失败吗？ JVM规范中规定如果用户自定义的类加载器将父类加载器强制设置为null，那么会自动将启动类加载器设置为当前用户自定义类加载器的父类加载器（这个问题前面已经分析过了）。同时，我们可以得出如下结论： 即使用户自定义类加载器不指定父类加载器，那么，同样可以加载到/lib下的类，但此时就不能够加载/lib/ext目录下的类了。 说明：问题3和问题4的推断结论是基于用户自定义的类加载器本身延续了java.lang.ClassLoader.loadClass（…）默认委派逻辑，如果用户对这一默认委派逻辑进行了改变，以上推断结论就不一定成立了，详见问题5。 4.5 编写自定义类加载器时，一般有哪些注意点？ 1、一般尽量不要覆写已有的loadClass(…)方法中的委派逻辑 一般在JDK 1.2之前的版本才这样做，而且事实证明，这样做极有可能引起系统默认的类加载器不能正常工作。在JVM规范和JDK文档中（1.2或者以后版本中），都没有建议用户覆写loadClass(…)方法，相比而言，明确提示开发者在开发自定义的类加载器时覆写findClass(…)逻辑。举一个例子来验证该问题： [java] view plain copy 1. //用户自定义类加载器WrongClassLoader.Java（覆写loadClass逻辑） 2. public class WrongClassLoader extends ClassLoader { 3. 4. public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException { 5. return this.findClass(name); 6. } 7. 8. protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { 9. // 假设此处只是到工程以外的特定目录D:\\library下去加载类 10. // 具体实现代码省略 11. } 12. } 13. 通过前面的分析我们已经知道，这个自定义类加载器WrongClassLoader的默认类加载器是系统类加载器，但是现在问题4种的结论就不成立了。大家可以简单测试一下，现在/lib、&lt; Java_Runtime_Home &gt;/lib/ext和工程类路径上的类都加载不上了。 [java] view plain copy 1. //问题5测试代码一 2. public class WrongClassLoaderTest { 3. 4. publicstaticvoid main(String[] args) { 5. try { 6. WrongClassLoader loader = new WrongClassLoader(); 7. Class classLoaded = loader.loadClass(&quot;beans.Account&quot;); 8. System.out.println(classLoaded.getName()); 9. System.out.println(classLoaded.getClassLoader()); 10. } catch (Exception e) { 11. e.printStackTrace(); 12. } 13. } 14. } 这里D:”classes”beans”Account.class是物理存在的。输出结果： [plain] view plain copy java.io.FileNotFoundException: D:”classes”java”lang”Object.class (系统找不到指定的路径。) at java.io.FileInputStream.open(Native Method) at java.io.FileInputStream.(FileInputStream.java:106) at WrongClassLoader.findClass(WrongClassLoader.java:40) at WrongClassLoader.loadClass(WrongClassLoader.java:29) at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:620) at java.lang.ClassLoader.defineClass(ClassLoader.java:400) at WrongClassLoader.findClass(WrongClassLoader.java:43) at WrongClassLoader.loadClass(WrongClassLoader.java:29) at WrongClassLoaderTest.main(WrongClassLoaderTest.java:27) Exception in thread “main” java.lang.NoClassDefFoundError: java/lang/Object at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:620) at java.lang.ClassLoader.defineClass(ClassLoader.java:400) at WrongClassLoader.findClass(WrongClassLoader.java:43) at WrongClassLoader.loadClass(WrongClassLoader.java:29) at WrongClassLoaderTest.main(WrongClassLoaderTest.java:27) 这说明，连要加载的类型的超类型java.lang.Object都加载不到了。这里列举的由于覆写loadClass()引起的逻辑错误明显是比较简单的，实际引起的逻辑错误可能复杂的多。 [java] view plain copy 1. //问题5测试二 2. //用户自定义类加载器WrongClassLoader.Java(不覆写loadClass逻辑) 3. public class WrongClassLoader extends ClassLoader { 4. 5. protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { 6. //假设此处只是到工程以外的特定目录D:\\library下去加载类 7. //具体实现代码省略 8. } 9. } 将自定义类加载器代码WrongClassLoader.Java做以上修改后，再运行测试代码，输出结果如下： [plain] view plain copy 1. beans.Account 2. WrongClassLoader@1c78e57 2、正确设置父类加载器 通过上面问题4和问题5的分析我们应该已经理解，个人觉得这是自定义用户类加载器时最重要的一点，但常常被忽略或者轻易带过。有了前面JDK代码的分析作为基础，我想现在大家都可以随便举出例子了。 3、保证findClass(String name)方法的逻辑正确性 事先尽量准确理解待定义的类加载器要完成的加载任务，确保最大程度上能够获取到对应的字节码内容。 4.6 如何在运行时判断系统类加载器能加载哪些路径下的类？ 一是可以直接调用ClassLoader.getSystemClassLoader()或者其他方式获取到系统类加载器（系统类加载器和扩展类加载器本身都派生自URLClassLoader），调用URLClassLoader中的getURLs()方法可以获取到。 二是可以直接通过获取系统属性java.class.path来查看当前类路径上的条目信息 ：System.getProperty(“java.class.path”)。 4.7 如何在运行时判断标准扩展类加载器能加载哪些路径下的类？ 方法之一： [java] view plain copy 1. import java.net.URL; 2. import java.net.URLClassLoader; 3. 4. public class ClassLoaderTest { 5. 6. /** 7. * @param args the command line arguments 8. */ 9. public static void main(String[] args) { 10. try { 11. URL[] extURLs = ((URLClassLoader) ClassLoader.getSystemClassLoader().getParent()).getURLs(); 12. for (int i = 0; i &lt; extURLs.length; i++) { 13. System.out.println(extURLs[i]); 14. } 15. } catch (Exception e) { 16. //… 17. } 18. } 19. } 本机对应输出如下： [plain] view plain copy 1. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/access-bridge-64.jar 2. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/cldrdata.jar 3. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/dnsns.jar 4. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/jaccess.jar 5. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/jfxrt.jar 6. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/localedata.jar 7. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/nashorn.jar 8. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/sunec.jar 9. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/sunjce_provider.jar 10. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/sunmscapi.jar 11. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/sunpkcs11.jar 12. file:/C:/Program%20Files/Java/jdk1.8.0_05/jre/lib/ext/zipfs.jar 以下未深入研究，故没有整理格式5 开发自己的类加载器 在前面介绍类加载器的代理委派模式的时候，提到过类加载器会首先代理给其它类加载器来尝试加载某个类。这就意味着真正完成类的加载工作的类加载器和启动这个加载过程的类加载器，有可能不是同一个。真正完成类的加载工作是通过调用defineClass来实现的；而启动类的加载过程是通过调用loadClass来实现的。前者称为一个类的定义加载器（defining loader），后者称为初始加载器（initiating loader）。在Java虚拟机判断两个类是否相同的时候，使用的是类的定义加载器。也就是说，哪个类加载器启动类的加载过程并不重要，重要的是最终定义这个类的加载器。两种类加载器的关联之处在于：一个类的定义加载器是它引用的其它类的初始加载器。如类 com.example.Outer引用了类 com.example.Inner，则由类 com.example.Outer的定义加载器负责启动类 com.example.Inner的加载过程。 方法 loadClass()抛出的是 java.lang.ClassNotFoundException异常；方法 defineClass()抛出的是 java.lang.NoClassDefFoundError异常。 类加载器在成功加载某个类之后，会把得到的 java.lang.Class类的实例缓存起来。下次再请求加载该类的时候，类加载器会直接使用缓存的类的实例，而不会尝试再次加载。也就是说，对于一个类加载器实例来说，相同全名的类只加载一次，即 loadClass方法不会被重复调用。 在绝大多数情况下，系统默认提供的类加载器实现已经可以满足需求。但是在某些情况下，您还是需要为应用开发出自己的类加载器。比如您的应用通过网络来传输Java类的字节代码，为了保证安全性，这些字节代码经过了加密处理。这个时候您就需要自己的类加载器来从某个网络地址上读取加密后的字节代码，接着进行解密和验证，最后定义出要在Java虚拟机中运行的类来。下面将通过两个具体的实例来说明类加载器的开发。 5.1 文件系统类加载器 第一个类加载器用来加载存储在文件系统上的Java字节代码。完整的实现如下所示。 [java] view plain copy package classloader; import java.io.ByteArrayOutputStream; import java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.io.InputStream; // 文件系统类加载器 public class FileSystemClassLoader extends ClassLoader { private String rootDir; public FileSystemClassLoader(String rootDir) { this.rootDir = rootDir; } // 获取类的字节码 @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { byte[] classData = getClassData(name); // 获取类的字节数组 if (classData == null) { throw new ClassNotFoundException(); } else { return defineClass(name, classData, 0, classData.length); } } private byte[] getClassData(String className) { // 读取类文件的字节 String path = classNameToPath(className); try { InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead = 0; // 读取类文件的字节码 while ((bytesNumRead = ins.read(buffer)) != -1) { baos.write(buffer, 0, bytesNumRead); } return baos.toByteArray(); } catch (IOException e) { e.printStackTrace(); } return null; } private String classNameToPath(String className) { // 得到类文件的完全路径 return rootDir + File.separatorChar className.replace(‘.’, File.separatorChar) + “.class”; } } 如上所示，类 FileSystemClassLoader继承自类java.lang.ClassLoader。在java.lang.ClassLoader类的常用方法中，一般来说，自己开发的类加载器只需要覆写 findClass(String name)方法即可。java.lang.ClassLoader类的方法loadClass()封装了前面提到的代理模式的实现。该方法会首先调用findLoadedClass()方法来检查该类是否已经被加载过；如果没有加载过的话，会调用父类加载器的loadClass()方法来尝试加载该类；如果父类加载器无法加载该类的话，就调用findClass()方法来查找该类。因此，为了保证类加载器都正确实现代理模式，在开发自己的类加载器时，最好不要覆写 loadClass()方法，而是覆写 findClass()方法。 类 FileSystemClassLoader的 findClass()方法首先根据类的全名在硬盘上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过defineClass()方法来把这些字节代码转换成 java.lang.Class类的实例。 加载本地文件系统上的类，示例如下： [java] view plain copy package com.example; public class Sample { private Sample instance; public void setSample(Object instance) { System.out.println(instance.toString()); this.instance = (Sample) instance; } }[java] view plain copy package classloader; import java.lang.reflect.Method; public class ClassIdentity { public static void main(String[] args) { new ClassIdentity().testClassIdentity(); } public void testClassIdentity() { String classDataRootPath = “C:\\Users\\JackZhou\\Documents\\NetBeansProjects\\classloader\\build\\classes”; FileSystemClassLoader fscl1 = new FileSystemClassLoader(classDataRootPath); FileSystemClassLoader fscl2 = new FileSystemClassLoader(classDataRootPath); String className = “com.example.Sample”; try { Class&lt;?&gt; class1 = fscl1.loadClass(className); // 加载Sample类 Object obj1 = class1.newInstance(); // 创建对象 Class&lt;?&gt; class2 = fscl2.loadClass(className); Object obj2 = class2.newInstance(); Method setSampleMethod = class1.getMethod(“setSample”, java.lang.Object.class); setSampleMethod.invoke(obj1, obj2); } catch (Exception e) { e.printStackTrace(); } } } 运行输出：com.example.Sample@7852e922 5.2 网络类加载器 下面将通过一个网络类加载器来说明如何通过类加载器来实现组件的动态更新。即基本的场景是：Java 字节代码（.class）文件存放在服务器上，客户端通过网络的方式获取字节代码并执行。当有版本更新的时候，只需要替换掉服务器上保存的文件即可。通过类加载器可以比较简单的实现这种需求。 类 NetworkClassLoader负责通过网络下载Java类字节代码并定义出Java类。它的实现与FileSystemClassLoader类似。[java] view plain copy package classloader; import java.io.ByteArrayOutputStream; import java.io.InputStream; import java.net.URL; public class NetworkClassLoader extends ClassLoader { private String rootUrl; public NetworkClassLoader(String rootUrl) { // 指定URL this.rootUrl = rootUrl; } // 获取类的字节码 @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { byte[] classData = getClassData(name); if (classData == null) { throw new ClassNotFoundException(); } else { return defineClass(name, classData, 0, classData.length); } } private byte[] getClassData(String className) { // 从网络上读取的类的字节 String path = classNameToPath(className); try { URL url = new URL(path); InputStream ins = url.openStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead = 0; // 读取类文件的字节 while ((bytesNumRead = ins.read(buffer)) != -1) { baos.write(buffer, 0, bytesNumRead); } return baos.toByteArray(); } catch (Exception e) { e.printStackTrace(); } return null; } private String classNameToPath(String className) { // 得到类文件的URL return rootUrl + “/“ className.replace(‘.’, ‘/‘) + “.class”; } } 在通过NetworkClassLoader加载了某个版本的类之后，一般有两种做法来使用它。第一种做法是使用Java反射API。另外一种做法是使用接口。需要注意的是，并不能直接在客户端代码中引用从服务器上下载的类，因为客户端代码的类加载器找不到这些类。使用Java反射API可以直接调用Java类的方法。而使用接口的做法则是把接口的类放在客户端中，从服务器上加载实现此接口的不同版本的类。在客户端通过相同的接口来使用这些实现类。我们使用接口的方式。示例如下： 客户端接口： [java] view plain copy package classloader; public interface Versioned { String getVersion(); }[java] view plain copy package classloader; public interface ICalculator extends Versioned { String calculate(String expression); } 网络上的不同版本的类： [java] view plain copy package com.example; import classloader.ICalculator; public class CalculatorBasic implements ICalculator { @Override public String calculate(String expression) { return expression; } @Override public String getVersion() { return “1.0”; } }[java] view plain copy package com.example; import classloader.ICalculator; public class CalculatorAdvanced implements ICalculator { @Override public String calculate(String expression) { return “Result is “ + expression; } @Override public String getVersion() { return “2.0”; } } 在客户端加载网络上的类的过程： [java] view plain copy package classloader; public class CalculatorTest { public static void main(String[] args) { String url = “http://localhost:8080/ClassloaderTest/classes“; NetworkClassLoader ncl = new NetworkClassLoader(url); String basicClassName = “com.example.CalculatorBasic”; String advancedClassName = “com.example.CalculatorAdvanced”; try { Class&lt;?&gt; clazz = ncl.loadClass(basicClassName); // 加载一个版本的类 ICalculator calculator = (ICalculator) clazz.newInstance(); // 创建对象 System.out.println(calculator.getVersion()); clazz = ncl.loadClass(advancedClassName); // 加载另一个版本的类 calculator = (ICalculator) clazz.newInstance(); System.out.println(calculator.getVersion()); } catch (Exception e) { e.printStackTrace(); } } } 参考文献： http://www.blogjava.net/zhuxing/archive/2008/08/08/220841.html http://www.ibm.com/developerworks/cn/java/j-lo-classloader/ 顶191 线程上下文类加载器 线程上下文类加载器（context class loader）是从 JDK 1.2 开始引入的。类 Java.lang.Thread中的方法 getContextClassLoader()和 setContextClassLoader(ClassLoader cl)用来获取和设置线程的上下文类加载器。如果没有通过 setContextClassLoader(ClassLoader cl)方法进行设置的话，线程将继承其父线程的上下文类加载器。Java 应用运行的初始线程的上下文类加载器是系统类加载器。在线程中运行的代码可以通过此类加载器来加载类和资源。 前面提到的类加载器的代理模式并不能解决 Java 应用开发中会遇到的类加载器的全部问题。Java 提供了很多服务提供者接口（Service Provider Interface，SPI），允许第三方为这些接口提供实现。常见的 SPI 有 JDBC、JCE、JNDI、JAXP 和 JBI 等。这些 SPI 的接口由 Java 核心库来提供，如 JAXP 的 SPI 接口定义包含在 javax.xml.parsers包中。这些 SPI 的实现代码很可能是作为 Java 应用所依赖的 jar 包被包含进来，可以通过类路径（CLASSPATH）来找到，如实现了 JAXP SPI 的 Apache Xerces所包含的 jar 包。SPI 接口中的代码经常需要加载具体的实现类。如 JAXP 中的 javax.xml.parsers.DocumentBuilderFactory类中的 newInstance()方法用来生成一个新的 DocumentBuilderFactory的实例。这里的实例的真正的类是继承自 javax.xml.parsers.DocumentBuilderFactory，由 SPI 的实现所提供的。如在 Apache Xerces 中，实现的类是 org.apache.xerces.jaxp.DocumentBuilderFactoryImpl。而问题在于，SPI 的接口是 Java 核心库的一部分，是由引导类加载器来加载的；SPI 实现的 Java 类一般是由系统类加载器来加载的。引导类加载器是无法找到 SPI 的实现类的，因为它只加载 Java 的核心库。它也不能代理给系统类加载器，因为它是系统类加载器的祖先类加载器。也就是说，类加载器的代理模式无法解决这个问题。 线程上下文类加载器正好解决了这个问题。如果不做任何的设置，Java 应用的线程的上下文类加载器默认就是系统上下文类加载器。在 SPI 接口的代码中使用线程上下文类加载器，就可以成功的加载到 SPI 实现的类。线程上下文类加载器在很多 SPI 的实现中都会用到。 Java默认的线程上下文类加载器是系统类加载器(AppClassLoader)。以下代码摘自sun.misc.Launch的无参构造函数Launch()。 [java] view plain copy // Now create the class loader to use to launch the application try { loader = AppClassLoader.getAppClassLoader(extcl); } catch (IOException e) { throw new InternalError( “Could not create application class loader” ); } // Also set the context class loader for the primordial thread. Thread.currentThread().setContextClassLoader(loader); 使用线程上下文类加载器，可以在执行线程中抛弃双亲委派加载链模式，使用线程上下文里的类加载器加载类。典型的例子有：通过线程上下文来加载第三方库jndi实现，而不依赖于双亲委派。大部分Java application服务器(jboss, tomcat..)也是采用contextClassLoader来处理web服务。还有一些采用hot swap特性的框架，也使用了线程上下文类加载器，比如 seasar (full stack framework in japenese)。 线程上下文从根本解决了一般应用不能违背双亲委派模式的问题。使java类加载体系显得更灵活。随着多核时代的来临，相信多线程开发将会越来越多地进入程序员的实际编码过程中。因此，在编写基础设施时， 通过使用线程上下文来加载类，应该是一个很好的选择。 当然，好东西都有利弊。使用线程上下文加载类，也要注意保证多个需要通信的线程间的类加载器应该是同一个，防止因为不同的类加载器导致类型转换异常(ClassCastException)。 defineClass(String name, byte[] b, int off, int len,ProtectionDomain protectionDomain)是java.lang.Classloader提供给开发人员，用来自定义加载class的接口。使用该接口，可以动态的加载class文件。例如在jdk中，URLClassLoader是配合findClass方法来使用defineClass，可以从网络或硬盘上加载class。而使用类加载接口，并加上自己的实现逻辑，还可以定制出更多的高级特性。 下面是一个简单的hot swap类加载器实现。hot swap即热插拔的意思，这里表示一个类已经被一个加载器加载了以后，在不卸载它的情况下重新再加载它一次。我们知道Java缺省的加载器对相同全名的类只会加载一次，以后直接从缓存中取这个Class object。因此要实现hot swap，必须在加载的那一刻进行拦截，先判断是否已经加载，若是则重新加载一次，否则直接首次加载它。我们从URLClassLoader继承，加载类的过程都代理给系统类加载器URLClassLoader中的相应方法来完成。 [java] view plain copy package classloader; import java.net.URL; import java.net.URLClassLoader; /** 可以重新载入同名类的类加载器实现 放弃了双亲委派的加载链模式，需要外部维护重载后的类的成员变量状态 */ public class HotSwapClassLoader extends URLClassLoader { public HotSwapClassLoader(URL[] urls) { super(urls); } public HotSwapClassLoader(URL[] urls, ClassLoader parent) { super(urls, parent); } // 下面的两个重载load方法实现类的加载，仿照ClassLoader中的两个loadClass() // 具体的加载过程代理给父类中的相应方法来完成 public Class&lt;?&gt; load(String name) throws ClassNotFoundException { return load(name, false); } public Class&lt;?&gt; load(String name, boolean resolve) throws ClassNotFoundException { // 若类已经被加载，则重新再加载一次 if (null != super.findLoadedClass(name)) { return reload(name, resolve); } // 否则用findClass()首次加载它 Class&lt;?&gt; clazz = super.findClass(name); if (resolve) { super.resolveClass(clazz); } return clazz; } public Class&lt;?&gt; reload(String name, boolean resolve) throws ClassNotFoundException { return new HotSwapClassLoader(super.getURLs(), super.getParent()).load( name, resolve); } } 两个重载的load方法参数与ClassLoader类中的两个loadClass()相似。在load的实现中，用findLoadedClass()查找指定的类是否已经被祖先加载器加载了，若已加载则重新再加载一次，从而放弃了双亲委派的方式（这种方式只会加载一次）。若没有加载则用自身的findClass()来首次加载它。 下面是使用示例： [java] view plain copy package classloader; public class A { private B b; public void setB(B b) { this.b = b; } public B getB() { return b; } }[java] view plain copy package classloader; public class B { }[java] view plain copy package classloader; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; import java.net.MalformedURLException; import java.net.URL; public class TestHotSwap { public static void main(String args[]) throws MalformedURLException { A a = new A(); // 加载类A B b = new B(); // 加载类B a.setB(b); // A引用了B，把b对象拷贝到A.b System.out.printf(“A classLoader is %s\\n”, a.getClass().getClassLoader()); System.out.printf(“B classLoader is %s\\n”, b.getClass().getClassLoader()); System.out.printf(“A.b classLoader is %s\\n”, a.getB().getClass().getClassLoader()); try { URL[] urls = new URL[]{ new URL(“file:///C:/Users/JackZhou/Documents/NetBeansProjects/classloader/build/classes/“) }; HotSwapClassLoader c1 = new HotSwapClassLoader(urls, a.getClass().getClassLoader()); Class clazz = c1.load(“classloader.A”); // 用hot swap重新加载类A Object aInstance = clazz.newInstance(); // 创建A类对象 Method method1 = clazz.getMethod(“setB”, B.class); // 获取setB(B b)方法 method1.invoke(aInstance, b); // 调用setB(b)方法，重新把b对象拷贝到A.b Method method2 = clazz.getMethod(“getB”); // 获取getB()方法 Object bInstance = method2.invoke(aInstance); // 调用getB()方法 System.out.printf(“Reloaded A.b classLoader is %s\\n”, bInstance.getClass().getClassLoader()); } catch (MalformedURLException | ClassNotFoundException | InstantiationException | IllegalAccessException | NoSuchMethodException | SecurityException | IllegalArgumentException | InvocationTargetException e) { e.printStackTrace(); } } } 运行输出：[java] view plain copy A classLoader is sun.misc.Launcher$AppClassLoader@73d16e93 B classLoader is sun.misc.Launcher$AppClassLoader@73d16e93 A.b classLoader is sun.misc.Launcher$AppClassLoader@73d16e93 Reloaded A.b classLoader is sun.misc.Launcher$AppClassLoader@73d16e93 HotSwapClassLoader加载器的作用是重新加载同名的类。为了实现hot swap，一个类在加载过后，若重新再加载一次，则新的Class object的状态会改变，老的状态数据需要通过其他方式拷贝到重新加载过的类生成的全新Class object实例中来。上面A类引用了B类，加载A时也会加载B（如果B已经加载，则直接从缓存中取出）。在重新加载A后，其Class object中的成员b会重置，因此要重新调用setB(b)拷贝一次。你可以注释掉这行代码，再运行会抛出java.lang.NullPointerException，指示A.b为null。 注意新的A Class object实例所依赖的B类Class object，如果它与老的B Class object实例不是同一个类加载器加载的， 将会抛出类型转换异常(ClassCastException)，表示两种不同的类。因此在重新加载A后，要特别注意给它的B类成员b传入外部值时，它们是否由同一个类加载器加载。为了解决这种问题， HotSwapClassLoader自定义的l/oad方法中，当前类（类A）是由自身classLoader加载的， 而内部依赖的类（类B）还是老对象的classLoader加载的。 2 何时使用Thread.getContextClassLoader()? 这是一个很常见的问题，但答案却很难回答。这个问题通常在需要动态加载类和资源的系统编程时会遇到。总的说来动态加载资源时，往往需要从三种类加载器里选择：系统或程序的类加载器、当前类加载器、以及当前线程的上下文类加载器。在程序中应该使用何种类加载器呢？ 系统类加载器通常不会使用。此类加载器处理启动应用程序时classpath指定的类，可以通过ClassLoader.getSystemClassLoader()来获得。所有的ClassLoader.getSystemXXX()接口也是通过这个类加载器加载的。一般不要显式调用这些方法，应该让其他类加载器代理到系统类加载器上。由于系统类加载器是JVM最后创建的类加载器，这样代码只会适应于简单命令行启动的程序。一旦代码移植到EJB、Web应用或者Java Web Start应用程序中，程序肯定不能正确执行。 因此一般只有两种选择，当前类加载器和线程上下文类加载器。当前类加载器是指当前方法所在类的加载器。这个类加载器是运行时类解析使用的加载器，Class.forName(String)和Class.getResource(String)也使用该类加载器。代码中X.class的写法使用的类加载器也是这个类加载器。 线程上下文类加载器在java 2(J2SE)时引入。每个线程都有一个关联的上下文类加载器。如果你使用new Thread()方式生成新的线程，新线程将继承其父线程的上下文类加载器。如果程序对线程上下文类加载器没有任何改动的话，程序中所有的线程将都使用系统类加载器作为上下文类加载器。Web应用和Java企业级应用中，应用服务器经常要使用复杂的类加载器结构来实现JNDI（Java命名和目录接口)、线程池、组件热部署等功能，因此理解这一点尤其重要。 为什么要引入线程的上下文类加载器？将它引入J2SE并不是纯粹的噱头，由于Sun没有提供充分的文档解释说明这一点，这使许多开发者很糊涂。实际上，上下文类加载器为同样在J2SE中引入的类加载代理机制提供了后门。通常JVM中的类加载器是按照层次结构组织的，目的是每个类加载器（除了启动整个JVM的原初类加载器）都有一个父类加载器。当类加载请求到来时，类加载器通常首先将请求代理给父类加载器。只有当父类加载器失败后，它才试图按照自己的算法查找并定义当前类。 有时这种模式并不能总是奏效。这通常发生在JVM核心代码必须动态加载由应用程序动态提供的资源时。拿JNDI为例，它的核心是由JRE核心类(rt.jar)实现的。但这些核心JNDI类必须能加载由第三方厂商提供的JNDI实现。这种情况下调用父类加载器（原初类加载器）来加载只有其子类加载器可见的类，这种代理机制就会失效。解决办法就是让核心JNDI类使用线程上下文类加载器，从而有效的打通类加载器层次结构，逆着代理机制的方向使用类加载器。 顺便提一下，XML解析API(JAXP)也是使用此种机制。当JAXP还是J2SE扩展时，XML解析器使用当前类加载器方法来加载解析器实现。但当JAXP成为J2SE核心代码后，类加载机制就换成了使用线程上下文加载器，这和JNDI的原因相似。 好了，现在我们明白了问题的关键：这两种选择不可能适应所有情况。一些人认为线程上下文类加载器应成为新的标准。但这在不同JVM线程共享数据来沟通时，就会使类加载器的结构乱七八糟。除非所有线程都使用同一个上下文类加载器。而且，使用当前类加载器已成为缺省规则，它们广泛应用在类声明、Class.forName等情景中。即使你想尽可能只使用上下文类加载器，总是有这样那样的代码不是你所能控制的。这些代码都使用代理到当前类加载器的模式。混杂使用代理模式是很危险的。 更为糟糕的是，某些应用服务器将当前类加载器和上下文类加器分别设置成不同的ClassLoader实例。虽然它们拥有相同的类路径，但是它们之间并不存在父子代理关系。想想这为什么可怕：记住加载并定义某个类的类加载器是虚拟机内部标识该类的组成部分，如果当前类加载器加载类X并接着执行它，如JNDI查找类型为Y的数据，上下文类加载器能够加载并定义Y，这个Y的定义和当前类加载器加载的相同名称的类就不是同一个，使用隐式类型转换就会造成异常。 这种混乱的状况还将在Java中存在很长时间。在J2SE中还包括以下的功能使用不同的类加载器： （1）JNDI使用线程上下文类加载器。 （2）Class.getResource()和Class.forName()使用当前类加载器。 （3）JAXP使用上下文类加载器。 （4）java.util.ResourceBundle使用调用者的当前类加载器。 （5）URL协议处理器使用java.protocol.handler.pkgs系统属性并只使用系统类加载器。 （6）Java序列化API缺省使用调用者当前的类加载器。 这些类加载器非常混乱，没有在J2SE文档中给以清晰明确的说明。 该如何选择类加载器？ 如若代码是限于某些特定框架，这些框架有着特定加载规则，则不要做任何改动，让框架开发者来保证其工作（比如应用服务器提供商，尽管他们并不能总是做对）。如在Web应用和EJB中，要使用Class.gerResource来加载资源。 在其他情况下，我们可以自己来选择最合适的类加载器。可以使用策略模式来设计选择机制。其思想是将“总是使用上下文类加载器”或者“总是使用当前类加载器”的决策同具体实现逻辑分离开。往往设计之初是很难预测何种类加载策略是合适的，该设计能够让你可以后来修改类加载策略。 考虑使用下面的代码，这是作者本人在工作中发现的经验。这儿有一个缺省实现，应该可以适应大部分工作场景： [java] view plain copy package classloader.context; /** 类加载上下文，持有要加载的类 */ public class ClassLoadContext { private final Class m_caller; public final Class getCallerClass() { return m_caller; } ClassLoadContext(final Class caller) { m_caller = caller; } } [java] view plain copy package classloader.context; /** 类加载策略接口 */ public interface IClassLoadStrategy { ClassLoader getClassLoader(ClassLoadContext ctx); }[java] view plain copy /** 缺省的类加载策略，可以适应大部分工作场景 */ public class DefaultClassLoadStrategy implements IClassLoadStrategy { /** 为ctx返回最合适的类加载器，从系统类加载器、当前类加载器 和当前线程上下文类加载中选择一个最底层的加载器 @param ctx @return */ @Override public ClassLoader getClassLoader(final ClassLoadContext ctx) { final ClassLoader callerLoader = ctx.getCallerClass().getClassLoader(); final ClassLoader contextLoader = Thread.currentThread().getContextClassLoader(); ClassLoader result; // If ‘callerLoader’ and ‘contextLoader’ are in a parent-child // relationship, always choose the child: if (isChild(contextLoader, callerLoader)) { result = callerLoader; } else if (isChild(callerLoader, contextLoader)) { result = contextLoader; } else { // This else branch could be merged into the previous one, // but I show it here to emphasize the ambiguous case: result = contextLoader; } final ClassLoader systemLoader = ClassLoader.getSystemClassLoader(); // Precaution for when deployed as a bootstrap or extension class: if (isChild(result, systemLoader)) { result = systemLoader; } return result; } // 判断anotherLoader是否是oneLoader的child private boolean isChild(ClassLoader oneLoader, ClassLoader anotherLoader){ //… } // … more methods } 决定应该使用何种类加载器的接口是IClassLoaderStrategy，为了帮助IClassLoadStrategy做决定，给它传递了个ClassLoadContext对象作为参数。ClassLoadContext持有要加载的类。 上面代码的逻辑很简单：如调用类的当前类加载器和上下文类加载器是父子关系，则总是选择子类加载器。对子类加载器可见的资源通常是对父类可见资源的超集，因此如果每个开发者都遵循J2SE的代理规则，这样做大多数情况下是合适的。 当前类加载器和上下文类加载器是兄弟关系时，决定使用哪一个是比较困难的。理想情况下，Java运行时不应产生这种模糊。但一旦发生，上面代码选择上下文类加载器。这是作者本人的实际经验，绝大多数情况下应该能正常工作。你可以修改这部分代码来适应具体需要。一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程。 最后需要检查一下，以便保证所选类加载器不是系统类加载器的父亲，在开发标准扩展类库时这通常是个好习惯。 注意作者故意没有检查要加载资源或类的名称。Java XML API成为J2SE核心的历程应该能让我们清楚过滤类名并不是好想法。作者也没有试图检查哪个类加载器加载首先成功，而是检查类加载器的父子关系，这是更好更有保证的方法。 下面是类加载器的选择器：[java] view plain copy package classloader.context; /** 类加载解析器，获取最合适的类加载器 */ public abstract class ClassLoaderResolver { private static IClassLoadStrategy s_strategy; // initialized in private static final int CALL_CONTEXT_OFFSET = 3; // may need to change if this class is redesigned private static final CallerResolver CALLER_RESOLVER; // set in static { try { // This can fail if the current SecurityManager does not allow // RuntimePermission (“createSecurityManager”): CALLER_RESOLVER = new CallerResolver(); } catch (SecurityException se) { throw new RuntimeException(“ClassLoaderResolver: could not create CallerResolver: “ + se); } s_strategy = new DefaultClassLoadStrategy(); //默认使用缺省加载策略 } /** This method selects the best classloader instance to be used for class/resource loading by whoever calls this method. The decision typically involves choosing between the caller’s current, thread context, system, and other classloaders in the JVM and is made by the {@link IClassLoadStrategy} instance established by the last call to {@link #setStrategy}. @return classloader to be used by the caller [‘null’ indicates the primordial loader] */ public static synchronized ClassLoader getClassLoader() { final Class caller = getCallerClass(0); // 获取执行当前方法的类 final ClassLoadContext ctx = new ClassLoadContext(caller); // 创建类加载上下文 return s_strategy.getClassLoader(ctx); // 获取最合适的类加载器 } public static synchronized IClassLoadStrategy getStrategy() { return s_strategy; } public static synchronized IClassLoadStrategy setStrategy(final IClassLoadStrategy strategy) { final IClassLoadStrategy old = s_strategy; // 设置类加载策略 s_strategy = strategy; return old; } /** A helper class to get the call context. It subclasses SecurityManager to make getClassContext() accessible. An instance of CallerResolver only needs to be created, not installed as an actual security manager. */ private static final class CallerResolver extends SecurityManager { @Override protected Class[] getClassContext() { return super.getClassContext(); // 获取当执行栈的所有类，native方法 } } /* Indexes into the current method call context with a given offset. */ private static Class getCallerClass(final int callerOffset) { return CALLER_RESOLVER.getClassContext()[CALL_CONTEXT_OFFSET callerOffset]; // 获取执行栈上某个方法所属的类 } } 可通过调用ClassLoaderResolver.getClassLoader()方法来获取类加载器对象，并使用其ClassLoader的接口如loadClass()等来加载类和资源。此外还可使用下面的ResourceLoader接口来取代ClassLoader接口： [java] view plain copy package classloader.context; import java.net.URL; public class ResourceLoader { /** 加载一个类 @param name @return @throws java.lang.ClassNotFoundException @see java.lang.ClassLoader#loadClass(java.lang.String) */ public static Class&lt;?&gt; loadClass(final String name) throws ClassNotFoundException { //获取最合适的类加载器 final ClassLoader loader = ClassLoaderResolver.getClassLoader(); //用指定加载器加载类 return Class.forName(name, false, loader); } /** 加载一个资源 @param name @return @see java.lang.ClassLoader#getResource(java.lang.String) */ public static URL getResource(final String name) { //获取最合适的类加载器 final ClassLoader loader = ClassLoaderResolver.getClassLoader(); //查找指定的资源 if (loader != null) { return loader.getResource(name); } else { return ClassLoader.getSystemResource(name); } } // … more methods … } ClassLoadContext.getCallerClass()返回的类在ClassLoaderResolver或ResourceLoader使用，这样做的目的是让其能找到调用类的类加载器（上下文加载器总是能通过Thread.currentThread().getContextClassLoader()来获得）。注意调用类是静态获得的，因此这个接口不需现有业务方法增加额外的Class参数，而且也适合于静态方法和类初始化代码。具体使用时，可以往这个上下文对象中添加具体部署环境中所需的其他属性。 3 类加载器与Web容器 对于运行在 Java EE容器中的 Web 应用来说，类加载器的实现方式与一般的 Java 应用有所不同。不同的 Web 容器的实现方式也会有所不同。以 Apache Tomcat 来说，每个 Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得 Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 绝大多数情况下，Web 应用的开发人员不需要考虑与类加载器相关的细节。下面给出几条简单的原则： （1）每个 Web 应用自己的 Java 类文件和使用的库的 jar 包，分别放在 WEB-INF/classes和 WEB-INF/lib目录下面。 （2）多个应用共享的 Java 类文件和 jar 包，分别放在 Web 容器指定的由所有 Web 应用共享的目录下面。 （3）当出现找不到类的错误时，检查当前类的类加载器和当前线程的上下文类加载器是否正确。 4 类加载器与OSGi OSGi是 Java 上的动态模块系统。它为开发人员提供了面向服务和基于组件的运行环境，并提供标准的方式用来管理软件的生命周期。OSGi 已经被实现和部署在很多产品上，在开源社区也得到了广泛的支持。Eclipse就是基于OSGi 技术来构建的。 OSGi 中的每个模块（bundle）都包含 Java 包和类。模块可以声明它所依赖的需要导入（import）的其它模块的 Java 包和类（通过 Import-Package），也可以声明导出（export）自己的包和类，供其它模块使用（通过 Export-Package）。也就是说需要能够隐藏和共享一个模块中的某些 Java 包和类。这是通过 OSGi 特有的类加载器机制来实现的。OSGi 中的每个模块都有对应的一个类加载器。它负责加载模块自己包含的 Java 包和类。当它需要加载 Java 核心库的类时（以 java开头的包和类），它会代理给父类加载器（通常是启动类加载器）来完成。当它需要加载所导入的 Java 类时，它会代理给导出此 Java 类的模块来完成加载。模块也可以显式的声明某些 Java 包和类，必须由父类加载器来加载。只需要设置系统属性 org.osgi.framework.bootdelegation的值即可。 假设有两个模块 bundleA 和 bundleB，它们都有自己对应的类加载器 classLoaderA 和 classLoaderB。在 bundleA 中包含类 com.bundleA.Sample，并且该类被声明为导出的，也就是说可以被其它模块所使用的。bundleB 声明了导入 bundleA 提供的类 com.bundleA.Sample，并包含一个类 com.bundleB.NewSample继承自 com.bundleA.Sample。在 bundleB 启动的时候，其类加载器 classLoaderB 需要加载类 com.bundleB.NewSample，进而需要加载类 com.bundleA.Sample。由于 bundleB 声明了类 com.bundleA.Sample是导入的，classLoaderB 把加载类 com.bundleA.Sample的工作代理给导出该类的 bundleA 的类加载器 classLoaderA。classLoaderA 在其模块内部查找类 com.bundleA.Sample并定义它，所得到的类 com.bundleA.Sample实例就可以被所有声明导入了此类的模块使用。对于以 java开头的类，都是由父类加载器来加载的。如果声明了系统属性 org.osgi.framework.bootdelegation=com.example.core.*，那么对于包 com.example.core中的类，都是由父类加载器来完成的。 OSGi 模块的这种类加载器结构，使得一个类的不同版本可以共存在 Java 虚拟机中，带来了很大的灵活性。不过它的这种不同，也会给开发人员带来一些麻烦，尤其当模块需要使用第三方提供的库的时候。下面提供几条比较好的建议： （1）如果一个类库只有一个模块使用，把该类库的 jar 包放在模块中，在 Bundle-ClassPath中指明即可。 （2）如果一个类库被多个模块共用，可以为这个类库单独的创建一个模块，把其它模块需要用到的 Java 包声明为导出的。其它模块声明导入这些类。 （3）如果类库提供了 SPI 接口，并且利用线程上下文类加载器来加载 SPI 实现的 Java 类，有可能会找不到 Java 类。如果出现了 NoClassDefFoundError异常，首先检查当前线程的上下文类加载器是否正确。通过 Thread.currentThread().getContextClassLoader()就可以得到该类加载器。该类加载器应该是该模块对应的类加载器。如果不是的话，可以首先通过 class.getClassLoader()来得到模块对应的类加载器，再通过 Thread.currentThread().setContextClassLoader()来设置当前线程的上下文类加载器。 总结 类加载器是 Java 语言的一个创新。它使得动态安装和更新软件组件成为可能。本文详细介绍了类加载器的相关话题，包括基本概念、代理模式、线程上下文类加载器、与 Web 容器和 OSGi 的关系等。开发人员在遇到 ClassNotFoundException和 NoClassDefFoundError等异常的时候，应该检查抛出异常的类的类加载器和当前线程的上下文类加载器，从中可以发现问题的所在。在开发自己的类加载器的时候，需要注意与已有的类加载器组织结构的协调。 参考文献： https://www.ibm.com/developerworks/cn/java/j-lo-classloader/ http://www.blogjava.net/lihao336/archive/2009/09/17/295489.html http://kenwublog.com/structure-of-java-class-loader","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"Java类加载机制常见问题","slug":"Java类加载机制常见问题","date":"2017-07-20T06:43:54.000Z","updated":"2017-08-18T06:44:58.000Z","comments":true,"path":"2017/07/20/Java类加载机制常见问题/","link":"","permalink":"http://yoursite.com/2017/07/20/Java类加载机制常见问题/","excerpt":"Java类加载机制常见问题梳理","text":"Java类加载机制常见问题梳理 什么是类加载：概念：虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验–转换解析–初始化，最终形成能被java虚拟机直接使用的java类型，就是jvm的类加载机制。类加载包含了以下过程：加载–验证–准备–解析–初始化–使用–卸载 Java的类加载器都有哪些？每个类加载器都加载哪些类？启动（Bootstrap）类加载器：引导类装入器是用本地代码实现的类装入器，它负责将 /lib下面的核心类库或-Xbootclasspath选项指定的jar包加载到内存中。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作。 扩展（Extension）类加载器：扩展类加载器是由Sun的ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将&lt; Java_Runtime_Home &gt;/lib/ext或者由系统变量-Djava.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器。 系统（System）类加载器：系统类加载器是由 Sun的 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。它负责将系统类路径java -classpath或-Djava.class.path变量所指的目录下的类库加载到内存中。开发者可以直接使用系统类加载器。 这些类加载之间的父子关系是怎样的？他们的委派关系是系统类加载器委派给扩展类加载器，扩展类加载器委派给启动类加载器继承关系是系统类继承自扩展类加载器，扩展类加载器继承自url加载器和secure加载器以及classloader抽象类 什么是双亲委派模型？就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。 为什么Java的类加载器要使用双亲委派模型？java类以及它的类加载器一起具备了一种带有优先级的层次关系。如 java.lang.Object(它在rt.jar之中)。 首先我们要知道一点，同一个类 被不同的 类加载器 加载成两个类之后，这两个类是不同的。如果java.lang.Object类被不同的类加载器加载很多次，jvm中存在了多个不同的Object类，那么java类型体系中最基础的行为也无从保证，应用程序会一片混乱(we know,all the class extends from java.lang.Object,如果Object存在多个品种，那绝对是灾难)。 相反，使用了双亲委派模型，无论哪个类加载器去加载，都会委派到BoorStrap ClassLoader进行加载，保证了Object在各个类加载器环境中，都是同一个类。 再提一点双亲委派模型的工作机制，以类 A 为例，A如果以Application ClassLoader方式来加载，Application ClassLoader委派给Extension ClassLoader,再到BootStrap ClassLoader.然而，BootStrap ClassLoader管理范围内(/lib目录中 ，或-Xbootclasspath参数指定路径中的jar包)没查到类A，那么就让最初请求的类加载器，Application ClassLoader来加载 at last，提一点，双亲委派模型 在jvm成长史上被重大破坏过三次，但是 双亲委派模型确实不错，还是像小强一样的活了下来 如何自定义自己的类加载器，自己的类加载器和Java自带的类加载器关系如何处理？除了和本地实现密切相关的启动类加载器之外，包括标准扩展类加载器和系统类加载器在内的所有其他类加载器我们都可以当做自定义类加载器来对待，唯一区别是是否被虚拟机默认使用。前面的内容中已经对java.lang.ClassLoader抽象类中的几个重要的方法做了介绍，这里就简要叙述一下一般用户自定义类加载器的工作流程吧（可以结合后面问题解答一起看） 1、首先检查请求的类型是否已经被这个类装载器装载到命名空间中了，如果已经装载，直接返回；否则转入步骤 2、委派类加载请求给父类加载器（更准确的说应该是双亲类加载器，真实虚拟机中各种类加载器最终会呈现树状结构），如果父类加载器能够完成，则返回父类加载器加载的Class实例；否则转入步骤 3、调用本类加载器的findClass（…）方法，试图获取对应的字节码，如果获取的到，则调用defineClass（…）导入类型到方法区；如果获取不到对应的字节码或者其他原因失败，返回异常给loadClass（…）， loadClass（…）转而抛异常，终止加载过程（注意：这里的异常种类不止一种）。 说明：这里说的自定义类加载器是指JDK 1.2以后版本的写法，即不覆写改变java.lang.loadClass(…)已有委派逻辑情况下。 由不同的类加载器加载的指定类还是相同的类型吗？在Java中，一个类用其完全匹配类名(fully qualified class name)作为标识，这里指的完全匹配类名包括包名和类名。但在JVM中一个类用其全名和一个加载类ClassLoader的实例作为唯一标识，不同类加载器加载的类将被置于不同的命名空间。 我们可以用两个自定义类加载器去加载某自定义类型（注意不要将自定义类型的字节码放置到系统路径或者扩展路径中，否则会被系统类加载器或扩展类加载器抢先加载），然后用获取到的两个Class实例进行java.lang.Object.equals（…）判断，将会得到不相等的结果。 这个大家可以写两个自定义的类加载器去加载相同的自定义类型，然后做个判断；同时，可以测试加载java.*类型，然后再对比测试一下测试结果。 在编写自定义类加载器时，如果没有设定父加载器，那么父加载器是谁？前面讲过，在不指定父类加载器的情况下，默认采用系统类加载器。可能有人觉得不明白，现在我们来看一下JDK对应的代码实现。 在编写自定义类加载器时，如果没有设定父加载器，那么父加载器是谁？前面讲过，在不指定父类加载器的情况下，默认采用系统类加载器。可能有人觉得不明白，现在我们来看一下JDK对应的代码实现。众所周知，我们编写自定义的类加载器直接或者间接继承自java.lang.ClassLoader抽象类，","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"mongoDB常见问题整理","slug":"mongoDB常见问题整理","date":"2017-07-15T09:37:31.000Z","updated":"2017-08-18T06:48:43.000Z","comments":true,"path":"2017/07/15/mongoDB常见问题整理/","link":"","permalink":"http://yoursite.com/2017/07/15/mongoDB常见问题整理/","excerpt":"多方收集整理成此篇，以后也会持续更新。","text":"多方收集整理成此篇，以后也会持续更新。 为什么要使用Nosql关系型数据库采用的结构化的数据，NoSQL采用的是键值对的方式存储数据。 mongo使用场景：在处理非结构化/半结构化的大数据时；在水平方向上进行扩展时；随时应对动态增加的数据项时可以优先考虑使用NoSQL数据库。高并发解决方案时； 以下特点使得MongoDB成为最好的NoSQL数据库：面向文件的高性能高可用性易扩展性丰富的查询语言完全索引 mongoDB的基本结构MongoDB的最基本的数据单元，叫document，类似于关系式数据库中的行 row。一系列documents，组成了一个collection，相当于关系式数据库中的table。当一个 collection 数据量太大时，可以把该collection按documents切分，分成多个数据块，每个数据块叫做一个chunk，多个chunks聚集在一起，组成了一个shard。 Sharding 的意义，不仅保障了数据库的扩容（scalability），同时也保障了系统的负载均衡（load balance）。 MongoDB支持存储过程吗？如果支持的话，怎么用？MongoDB支持存储过程，它是JavaScript写的，保存在db.system.js表中。 如何执行事务/加锁?MongoDB没有使用传统的锁或者复杂的带回滚的事务，因为它设计的宗旨是轻量，快速以及可预计的高性能。可以把它类比成MySQL MylSAM的自动提交模式。通过精简对事务的支持，性能得到了提升，特别是在一个可能会穿过多个服务器的系统里。 什么是“片键”片键是拆分集合的依据，管理员设置的“片键”将数据分摊到自己管理的mongod集群，数据 和片的对应关系以及相应的配置信息保存在”config服务器”上。 什么是master或primary?它是当前备份集群(replica set)中负责处理所有写入操作的主要节点/成员。在一个备份集群中，当失效备援(failover)事件发生时，一个另外的成员会变成primary。 主从复制模式：master slave副本集 ：primary（活跃） secondary（备份） 什么是secondary或slave?Seconday从当前的primary上复制相应的操作。它是通过跟踪复制oplog(local.oplog.rs)做到的。 数据库的整体结构键值对–》文档–》集合–》数据库 mongodb的结构介绍数据库中存储的对象设计bson，一种类似json的二进制文件，由键值对组成 为什么mongodb的数据文件那么庞大mongodb会积极的预分配预留空间，防止文件系统碎片 名字空间（namespace）是什么？在collection中，数据库名+集合名叫做名字空间。也就是一个集合的完整名 数据在什么时候才会扩展到多个分片（shard）里？mongodb分片是基于区域的，所以一个集合的所有对象都放置在同一个块中，只有当存在多余一个块的时候，才会有多个分片获取数据的选项 当我试图更新一个正在被迁移的块（chunk）上的文档时会发生什么？会立即更新旧的分片，然后更改才会在所有权转移前复制到新的分片上 能否使用日志特征进行安全备份？是的。 更新操作立刻fsync到磁盘？一般磁盘的写操作都是延迟执行的 如果用户移除对象的属性，该属性是否从存储层中删除？是的，用户移除属性然后对象会重新保存（re-save()）。 分析器在MongoDB中的作用是什么?分析器就是explain 显示每次操作性能特点的数据库分析器。通过分析器可能查找比预期慢的操作","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"mongoDB底层运作原理","slug":"mongoDB底层运作原理","date":"2017-07-05T09:37:31.000Z","updated":"2017-08-18T06:57:58.000Z","comments":true,"path":"2017/07/05/mongoDB底层运作原理/","link":"","permalink":"http://yoursite.com/2017/07/05/mongoDB底层运作原理/","excerpt":"觉得这一篇整理比较好，主要在于通俗易懂，逻辑清晰。但是网上转载的人太多了，我实在找不到原作者了。","text":"觉得这一篇整理比较好，主要在于通俗易懂，逻辑清晰。但是网上转载的人太多了，我实在找不到原作者了。 关于MongoDB，我们能看到的资料，基本都是在指导大家如何使用MongoDB，但是，MongoDB内部是如何运作的，资料不是很多。 阅读使用手册，会有很多疑惑之处。例如，有人说，MongoDB 等同于分布式的 MySQL。它把一个Table，按 row，分割成多个Shards，分别存放在不同的 Servers 上。这种说法是否正确？ 不深入了解 MongoDB 的内部结构，就无法透彻地回答类似问题。这个系列文章，就来和大家探讨MongoDB的内部的工作方式。 图1-1 MongoDB架构图 MongoDB通常运行在一个服务器集群上，而不是一个单机。图1-1，描述了一个MongoDB集群的基本组成部分，包括若干shards，至少一个config server，至少一个routing servers（又称 mongos）。 ShardsMongoDB的最基本的数据单元，叫document，类似于关系式数据库中的行 row。一系列documents，组成了一个collection，相当于关系式数据库中的table。当一个 collection 数据量太大时，可以把该collection按documents切分，分成多个数据块，每个数据块叫做一个chunk，多个chunks聚集在一起，组成了一个shard。 Sharding 的意义，不仅保障了数据库的扩容（scalability），同时也保障了系统的负载均衡（load balance）。 Shard keys为了把collection切分成不同的chunks，从而存放到不同的shards中，我们需要制定一个切分的方式。 如前所述，在 MongoDB 数据库中，一个表collection由多个行 documents 组成，而每个 document，有多个属性 fields。同一个 collection 中的不同的 documents，可能会有不同的 fields。例如，有个 collection叫 Media，包含两条 documents， { &quot;ISBN&quot;: &quot;987-30-3652-5130-82&quot;, &quot;Type&quot;: &quot;CD&quot;, &quot;Author&quot;: &quot;Nirvana&quot;, &quot;Title&quot;: &quot;Nevermind&quot;, &quot;Genre&quot;: &quot;Grunge&quot;, &quot;Releasedate&quot;: &quot;1991.09.24&quot;, &quot;Tracklist&quot;: [ { &quot;Track&quot; : &quot;1&quot;, &quot;Title&quot; : &quot;Smells like teen spirit&quot;, &quot;Length&quot; : &quot;5:02&quot; }, { &quot;Track&quot; : &quot;2&quot;, &quot;Title&quot; : &quot;In Bloom&quot;, &quot;Length&quot; : &quot;4:15&quot; } ] } { &quot;ISBN&quot;: &quot;987-1-4302-3051-9&quot;, &quot;Type&quot;: &quot;Book&quot;, &quot;Title&quot;: &quot;Definite Guide to MongoDB: The NoSQL Database&quot;, &quot;Publisher&quot;: &quot;Apress&quot;, &quot;Author&quot;: &quot; Eelco Plugge&quot;, &quot;Releasedate&quot;: &quot;2011.06.09&quot; } 假如，在同一个 collection 中的所有 document，都包含某个共同的 field，例如前例中的“ISBN”，那么我们就可以按照这个 field 的值，来分割 collection。这个 field 的值，又称为 shard key。 在选择 shard key 的时候，一定要确保这个 key 能够把 collection 均匀地切分成很多 chunks。 例如，如果我们选择“author”作为 shard key，如果有大量的作者是重名的，那么就会有大量的数据聚集在同一个 chunk 中。当然，假设很少有作者同名同姓，那么“author”也可以作为一个shard key。换句话说，shard key 的选择，与使用场景密切相关。 很多情况下，无论选择哪一个单一的 field 作为 shard key，都无法均匀分割 collection。在这种情况下，我们可以考虑，用多个 fields，构成一个复合的 shard key。 延续前例，假如有很多作者同名同姓，他们都叫“王二”。用 author 作为 shard key，显然无法均匀切割 collection。这时我们可以加上 release-date，组成 name-date 的复合的 shard key，例如“王二 2011”。 ChunksMongoDB 按 shard key，把 collection切割成若干chunks。每个 chunk 的数据结构，是一个三元组，{collection，minKey，maxKey}，如图1-2 所示。 图1-2 chunk的三元组 其中，collection 是数据库中某一个表的名称，而 minKey 和 maxKey 是 shard key的范围。每一个 document 的shard key 的值，决定了这条document应该存放在哪个chunk中。 如果两条 documents 的 shard keys 的值很接近，这两条 documents 很可能被存放在同一个 chunk 中。 Shard key 的值的顺序，决定了 document 存放的 chunk。在 MongoDB 的文献中，这种切割 collection的方式，称为 order-preserving。 一个 chunk 最多能够存储64MB的数据。 当某个 chunk 存储的 documents 包含的数据量，接近这个阈值时，一个 chunk 会被切分成两个新的 chunks。 当一个 shard 存储了过多的 chunks，这个shard中的某些 chunks 会被迁移到其它 shard 中。 这里有个问题，假如某一条 document 包含的数据量很大，超过 64MB，一个 chunk 存放不下，怎么办？在后续章节介绍 GridFS 时，我们会详细讨论。 Replica set在生产环境中，为了保证数据不丢失，为了提高系统的可用性（availability），每一个shard被存储多份，每个备份所在的 servers，组成了一个 replica set。 这个 replica set 包括一个 primary DB 和多个secondary DBs。为了数据的一致性，所有的修改 (insert / update / deletes) 请求都交给 primary 处理。处理结束之后，再异步地备份到其他 secondary 中。 Primary DB 由 replica set中的所有 servers，共同选举产生。当这个 primaryDB server 出错的时候，可以从 replica set 中重新选举一个新的 primaryDB，从而避免了单点故障。 Replica set 的选举策略和数据同步机制，确保了系统的数据的一致性。后文详述。 Config ServerConfig servers 用于存储 MongoDB 集群的元数据 metadata，这些元数据包括如下两个部分，每一个 shard server 包括哪些 chunks，每个 chunk 存储了哪些 collections 的哪些 documents。 每一个 config server 都包括了 MongoDB 中所有 chunk 的信息。 Config server 也需要 replication。但是有趣的是，config server 采用了自己独特的 replication 模式，而没有沿用 replica set。 如果任何一台 config server 挂了，整个 config server 集群中，其它 config server 变成只读状态。这样做的原因，是避免在系统不稳定的情况下，冒然对元数据做任何改动，导致在不同的 config servers 中，出现元数据不一致的情况。 MongoDB 的官方文档建议，配置 3 个 config servers 比较合适，既提供了足够的安全性，又避免了更多的 config servers 实例之间的数据同步，引起的元数据不一致的麻烦。 Mongos用户使用MongoDB 时，用户的操作请求，全部由 mongos 来转发。 当 mongos 接收到用户请求时，它先查询 config server，找到存放相应数据的 shard servers。然后把用户请求，转发到这些 shard servers。当这些 shard servers完成操作后，它们把结果分别返回给 mongos。而当 mongos 汇总了所有的结果后，它把结果返回给用户。 Mongos 每次启动的时候，都要到 config servers 中读取元数据，并缓存在本地。每当 config server中的元数据有改动，它都会通知所有的 mongos。 Mongos 之间，不存在彼此协同工作的问题。因此，MongoDB 所需要配置的 mongos server的数量，没有限制。 通过以上的介绍，我们对每个组成部分都有了基本的了解，但是涉及到工作的细节，我们尚有诸多疑问，例如，一个chunk的数据太大，如何切分？一个shard数据太多，如何迁移？在 replica set 中，如何选择primary？server挂了，怎么进行故障恢复？接下来的章节，我们逐个回答这些问题。 Reference， [0] Architectural Overview http://www.mongodb.org/display/DOCS/Sharding+Introduction","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"Shiro权限框架整合总结与开源分享","slug":"shiro权限框架整合开源分享","date":"2017-06-12T09:52:54.000Z","updated":"2017-08-14T05:05:26.000Z","comments":true,"path":"2017/06/12/shiro权限框架整合开源分享/","link":"","permalink":"http://yoursite.com/2017/06/12/shiro权限框架整合开源分享/","excerpt":"2017年年初在供应链项目正式应用shiro权限框架，当时对一些问题做了一些记录。此次做全面总结。","text":"2017年年初在供应链项目正式应用shiro权限框架，当时对一些问题做了一些记录。此次做全面总结。shiro框架已经整合成单独一个服务，github地址：https://github.com/yuaman/shiro-service 添加favicon重定向问题（转）文件放在 static/common/images 路径下，页面 head 里加上 浏览器可以成功显示 favicon.ico，但是在第一次登陆成功后会自动重定向到 /favicon.ico 文件的路径。 网上的解决办法：基本大多数浏览器都会请求 favicon.ico 这个图标文件用来展示在浏览器的URL地址前面，而这个文件被shiro保护了。解决方法： 在 filterChainDefinitions 下配置 /favicon.ico 以匿名访问 /favicon.ico = anon可是我配置好 /static/common/images/favicon.ico = anon 后，还是会重定向。 多次测试之后发现anon配置的顺序会有影响。 Shiro验证URL时，URL匹配成功便不再继续匹配查找，所以要注意配置文件中的URL顺序，尤其在使用通配符时。故filterChainDefinitions的配置顺序为自上而下，以最上面的为准。 1. &lt;property name=&quot;filterChainDefinitions&quot;&gt; 2. &lt;value&gt; 3. /static/common/images/favicon.ico = anon 4. /resources/**=anon 5. /systemManage/resources/**=anon 6. /unauthorized = authc 7. /login = authc 8. /logout = logout 9. &lt;!--/authenticated = authc--&gt; 10. /** = user,sysUser 11. &lt;/value&gt; 12. &lt;/property&gt; 只是调整了 /static/common/images/favicon.ico = anon 的顺序。 过滤器 anon 表示可匿名使用，可以理解为匿名用户或游客，无需认证便可以访问的的文件。","categories":[],"tags":[{"name":"个人开源项目","slug":"个人开源项目","permalink":"http://yoursite.com/tags/个人开源项目/"}]},{"title":"mac下IntelliJ使用记录","slug":"mac下IntelliJ使用记录","date":"2017-06-07T03:17:54.000Z","updated":"2017-08-14T05:06:44.000Z","comments":true,"path":"2017/06/07/mac下IntelliJ使用记录/","link":"","permalink":"http://yoursite.com/2017/06/07/mac下IntelliJ使用记录/","excerpt":"遇坑总结","text":"遇坑总结不要用汉化包用了汉化包preferences就不好用了，字号完全无法更改，切换页面风格也会产生一半黑一半白的问题，jrebel也无法重启。无法，只能重装。 项目结构颜色问题重装问题也无法解决一半黑一半白的问题，最终发现是设置-颜色（file-color）中以前某个时候定义的，删掉就好了。 svn集成问题idea 自动集成svn，可是后来出了点问题。在提交与更新时总是弹出需要提交svn的认证，stackoverflow上有几个回答，mac下的情况更复杂，实在解决不了。改用cornerstone （mac下目前唯一能够被破解的SVN client）进行进行svn管理。cornerstone的自动识别被修改的文件功能很强大。附上该工具具体的版本管理使用说明：（http://www.cnblogs.com/fyongbetter/p/5404697.html） 快捷键设置最好先把keymap设置成“eclipse mac”的模式，这样的话我们的ctrl就变成command，举例来说ctrl+d删除一行就变成command+d，在此基础上设置其他快捷键。","categories":[],"tags":[{"name":"操作记录","slug":"操作记录","permalink":"http://yoursite.com/tags/操作记录/"}]},{"title":"mongoDB生产环境三种模式","slug":"mongoDB生产环境三种模式","date":"2017-05-05T09:37:31.000Z","updated":"2017-08-18T07:01:11.000Z","comments":true,"path":"2017/05/05/mongoDB生产环境三种模式/","link":"","permalink":"http://yoursite.com/2017/05/05/mongoDB生产环境三种模式/","excerpt":"MongoDb在用于生产环境的三种模式，master/slaves（主从模式）;replcation副本集;auto shard 分片模式 找不到原作了，并且我在文末对分片模式进行了一下补充","text":"MongoDb在用于生产环境的三种模式，master/slaves（主从模式）;replcation副本集;auto shard 分片模式 找不到原作了，并且我在文末对分片模式进行了一下补充 1.主从复制 在早期的系统设计中，主从模式是比较流行的，将读写分离，在不同的DB上操作，可以有效降低数据库的压力，而且还能实现数据的备份，但是在master节点故障的时候，不能及时的自动的切换到slaves节点，需要手动干预，这个是硬伤 2.副本集 目前在Mongodb的官方说法中已经不推荐使用master/slave/模式，推荐使用副本集模式，应为该模式不但实现了主从模式的读写分离，而且有自己的一套选举机制，能通过自己的算法，选举出当前最优的节点作为活跃节点，一旦活跃节点宕机，选举出来的新的节点将成为活跃节点对外提供服务，其他节点则继续作为复制节点，当原先的活跃节点恢复，会自动作为非活跃节点（备份节点）存在。 这种模式的最大优点在于Mongodb的自动选举活跃节点的机制，不需要手动干预便可以实现活跃与非活跃的切换，但是它由于数据没有shard，每个节点都是一个完成的备份，则不能使用MongoDb的分布式计算功能，当然，也可以通过程序自己来实现（成本很高），所以就有了Auto shard模式 3.分片 利用Mongo的分片，可以将数据自动的分解成多个块，存储在不同的节点上，每个被差分的块都有三个副本集，这样是为了数据备份和恢复，而且数据分片以后，可以利用多台廉价的存储和CPU的计算构建一个水平可扩展的计算架构，这就是我们的分布式计算 目前在单台Mongodb上做MapReduce，速度还是比较慢的，但是如果数据分散在多台机器上，利用多太机器建立一个计算集群，计算速度估计会线性增长。 分片（sharding）是指将数据库拆分，将其分散在不同的机器上的过程。将数据分散到不同的机器上，不需要功能强大的服务器就可以存储更多的数据和处理更大的负载。基本思想就是将集合切成小块，这些块分散到若干片里，每个片只负责总数据的一部分，最后通过一个均衡器来对各个分片进行均衡（数据迁移）。通过一个名为mongos的路由进程进行操作，mongos知道数据和片的对应关系（通过配置服务器）。大部分使用场景都是解决磁盘空间的问题，对于写入有可能会变差（+++里面的说明+++），查询则尽量避免跨分片查询。 使用分片的时机：1，机器的磁盘不够用了。使用分片解决磁盘空间的问题。2，单个mongod已经不能满足写数据的性能要求。通过分片让写压力分散到各个分片上面，使用分片服务器自身的资源。3，想把大量数据放到内存里提高性能。和上面一样，通过分片使用分片服务器自身的资源。 分片中各个角色的作用：① 配置服务器。是一个独立的mongod进程，保存集群和分片的元数据，即各分片包含了哪些数据的信息。最先开始建立，启用日志功能。像启动普通的mongod一样启动配置服务器，指定configsvr选项。不需要太多的空间和资源，配置服务器的1KB空间相当于真是数据的200MB。保存的只是数据的分布表。当服务不可用，则变成只读，无法分块、迁移数据。② 路由服务器。即mongos，起到一个路由的功能，供程序连接。本身不保存数据，在启动时从配置服务器加载集群信息，开启mongos进程需要知道配置服务器的地址，指定configdb选项。③ 分片服务器。是一个独立普通的mongod进程，保存数据信息。可以是一个副本集也可以是单独的一台服务器。 mongos路由会在后台对各片进行负载均衡,直至各片的chunks块数量相等! 对于负载均衡的Sharding Cluster(各片的chunks块数量相等),对于随机键的操作会非常有效,基本整个过程是很均匀的，而此时递增键的操作还是会出现严重的负载不均衡的情况!","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"MongoDB 进阶——大文件存储(GridFS)","slug":"MongoDB 进阶——大文件存储(GridFS)","date":"2017-04-05T09:37:31.000Z","updated":"2017-08-18T07:02:54.000Z","comments":true,"path":"2017/04/05/MongoDB 进阶——大文件存储(GridFS)/","link":"","permalink":"http://yoursite.com/2017/04/05/MongoDB 进阶——大文件存储(GridFS)/","excerpt":"MongoDB 进阶——大文件存储(GridFS)","text":"MongoDB 进阶——大文件存储(GridFS) GridFS是一种在MongoDB中存储大二进制文件的机制。使用GridFS存文件有如下几个原因：● GridFS可以简化需求。如果已经用了mongodb,GridFS就可以不需要独立的文件存储架构。 ● GridFS利用已经建立的复制和分片机制，所以对于文件存储来说故障恢复和扩展都很容易。 ● GridFS可以避免用于存储用户上传内容的文件系统出现的某些问题。例如：GridFS在同一目录下放置大量文件是没有任何问题的。 ● GridFS不产生磁片，因为MongoDB分配的数据文件空间以2G为一块。 使用GridFS:mongofilesmongofiles是GridFS的实用工具，用于管理GridFS文件 Gridfs内部原理Gridfs的基本思想就是可以将大文件分成很多块，每块作为一个单独的文档存储，这样就能存大文件了。它一个建立在普通MongoDB文档基础上轻量级文件规范。 由于MongoDB支持在文档存储二进制数据，可以最大限度减少块的存储开销。另外，除了存储文件本身的块，还有一个单独的文档用来存储分块的信息和文件的元数据。","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"深入浅出数据库索引原理（平衡二叉树）","slug":"深入浅出数据库索引原理（平衡二叉树）","date":"2017-03-13T09:37:31.000Z","updated":"2017-08-18T07:05:56.000Z","comments":true,"path":"2017/03/13/深入浅出数据库索引原理（平衡二叉树）/","link":"","permalink":"http://yoursite.com/2017/03/13/深入浅出数据库索引原理（平衡二叉树）/","excerpt":"转载地址：http://www.cnblogs.com/aspwebchh/p/6652855.html 由于探究mongo索引的原理所以引申到索引的实现平衡二叉树的探究","text":"转载地址：http://www.cnblogs.com/aspwebchh/p/6652855.html 由于探究mongo索引的原理所以引申到索引的实现平衡二叉树的探究 前段时间，公司一个新上线的网站出现页面响应速度缓慢的问题， 一位负责这个项目的但并不是搞技术的妹子找到我，让我想办法提升网站的访问速度 ，因为已经有很多用户来投诉了。我第一反应觉的是数据库上的问题，假装思索了一下，摆着一副深沉炫酷的模样说：“是不是数据库查询上出问题了， 给表加上索引吧”，然后妹子来了一句：“现在我们网站访问量太大，加索引有可能导致写入数据时性能下降，影响用户使用的”。当时我就楞了一下， 有种强行装逼被拆穿的感觉，在自己的专业领域居然被非专业的同学教育， 面子上真有点挂不住。 其实， 我说这个例子并不是为展现我们公司的同事们专业能力的强大、做的产品棒、安全性高、性能牛逼， 连非技术的同事也懂得技术上的细节。事实上我只是想说明，「数据库」和「数据库索引」这两个东西是在服务器端开发领域应用最为广泛的两个概念，熟练使用数据库和数据库索引是开发人员在行业内生存的必备技能，而整天和技术人员打交道的非技术人员们，由于耳濡目染久了，自然也就能讲个头头是道了。 使用索引很简单，只要能写创建表的语句，就肯定能写创建索引的语句，要知道这个世界上是不存在不会创建表的服务器端程序员的。然而， 会使用索引是一回事， 而深入理解索引原理又能恰到好处使用索引又是另一回事，这完全是两个天差地别的境界（我自己也还没有达到这层境界）。很大一部份程序员对索引的了解仅限于到“加索引能使查询变快”这个概念为止。 为什么要给表加上主键？ 为什么加索引后会使查询变快？ 为什么加索引后会使写入、修改、删除变慢？ 什么情况下要同时在两个字段上建索引？ 这些问题他们可能不一定能说出答案。知道这些问题的答案有什么好处呢？如果开发的应用使用的数据库表中只有1万条数据，那么了解与不了解真的没有差别， 然而， 如果开发的应用有几百上千万甚至亿级别的数据，那么不深入了解索引的原理， 写出来程序就根本跑不动，就好比如果给货车装个轿车的引擎，这货车还能拉的动货吗？ 接下来就讲解一下上面提出的几个问题，希望对阅读者有帮助。 网上很多讲解索引的文章对索引的描述是这样的「索引就像书的目录， 通过书的目录就准确的定位到了书籍具体的内容」，这句话描述的非常正确， 但就像脱了裤子放屁，说了跟没说一样，通过目录查找书的内容自然是要比一页一页的翻书找来的快，同样使用的索引的人难到会不知道，通过索引定位到数据比直接一条一条的查询来的快，不然他们为什么要建索引。 想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是b tree或者 b+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然， 有的数据库也使用哈希桶作用索引的数据结构 ， 然而， 主流的RDBMS都是把平衡树当做数据表默认的索引数据结构的。 我们平时建表的时候都会为表加上主键， 在某些关系数据库中， 如果建表时不指定主键，数据库会拒绝建表的语句执行。 事实上， 一个加了主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐， 跟我认知中的「表」很接近。如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错， 再说一遍， 整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键， 一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。图画的不是很好， 将就着看。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。 假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。 这里不讲解平衡树的运行细节， 但是从上图能看出，树一共有三层， 从根节点至叶节点只需要经过三次查找就能得到结果。如下图 假如一张表有一亿条数据 ，需要查找其中某一条数据，按照常规逻辑， 一条一条的去匹配的话， 最坏的情况下需要匹配一亿次才能得到结果，用大O标记法就是O(n)最坏时间复杂度，这是无法接受的，而且这一亿条数据显然不能一次性读入内存供程序使用， 因此， 这一亿次匹配在不经缓存优化的情况下就是一亿次IO开销，以现在磁盘的IO能力和CPU的运算能力， 有可能需要几个月才能得出结果 。如果把这张表转换成平衡树结构（一棵非常茂盛和节点非常多的树），假设这棵树有10层，那么只需要10次IO开销就能查找到所需要的数据， 速度以指数级别提升，用大O标记法就是O(log n)，n是记录总树，底数是树的分叉数，结果就是树的层次数。换言之，查找次数是以树的分叉数为底，记录总数的对数，用公式来表示就是 用程序来表示就是Math.Log(100000000,10)，100000000是记录数，10是树的分叉数（真实环境下分叉数远不止10）， 结果就是查找次数，这里的结果从亿降到了个位数。因此，利用索引会使数据库查询有惊人的性能提升。 然而， 事物都是有两面的， 索引能让数据库查询数据的速度上升， 而使写入数据的速度下降，原因很简单的， 因为平衡树这个结构必须一直维持在一个正确的状态， 增删改数据都会改变平衡树各节点中的索引数据内容，破坏树结构， 因此，在每次数据改变时， DBMS必须去重新梳理树（索引）的结构以确保它的正确，这会带来不小的性能开销，也就是为什么索引会给查询以外的操作带来副作用的原因。 讲完聚集索引 ， 接下来聊一下非聚集索引， 也就是我们平时经常提起和使用的常规索引。 非聚集索引和聚集索引一样， 同样是采用平衡树作为索引的数据结构。索引树结构中各节点的值来自于表中的索引字段， 假如给user表的name字段加上索引 ， 那么索引就是由name字段中的值构成，在数据改变时， DBMS需要一直维护索引结构的正确性。如果给表中多个字段加上索引 ， 那么就会出现多个独立的索引结构，每个索引（非聚集索引）互相之间不存在关联。 如下图 每次给字段建一个新索引， 字段中的数据就会被复制一份出来， 用于生成索引。 因此， 给表添加索引，会增加表的体积， 占用磁盘存储空间。 非聚集索引和聚集索引的区别在于， 通过聚集索引可以查到需要查找的数据， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据，如下图 不管以任何方式查询表， 最终都会利用主键通过聚集索引来定位到数据， 聚集索引（主键）是通往真实数据所在的唯一路径。 然而， 有一种例外可以不使用聚集索引就能查询出所需要的数据， 这种非主流的方法 称之为「覆盖索引」查询， 也就是平时所说的复合索引或者多字段索引查询。 文章上面的内容已经指出， 当为字段建立索引以后， 字段中的内容会被同步到索引之中， 如果为一个索引指定两个字段， 那么这个两个字段的内容都会被同步至索引之中。 先看下面这个SQL语句 //建立索引 create index index_birthday on user_info(birthday); //查询生日在1991年11月1日出生用户的用户名 select user_name from user_info where birthday = ‘1991-11-1’ 这句SQL语句的执行过程如下 首先，通过非聚集索引index_birthday查找birthday等于1991-11-1的所有记录的主键ID值 然后，通过得到的主键ID值执行聚集索引查找，找到主键ID值对就的真实数据（数据行）存储的位置 最后， 从得到的真实数据中取得user_name字段的值返回， 也就是取得最终的结果 我们把birthday字段上的索引改成双字段的覆盖索引 create index index_birthday_and_user_name on user_info(birthday, user_name); 这句SQL语句的执行过程就会变为 通过非聚集索引index_birthday_and_user_name查找birthday等于1991-11-1的叶节点的内容，然而， 叶节点中除了有user_name表主键ID的值以外， user_name字段的值也在里面， 因此不需要通过主键ID值的查找数据行的真实所在， 直接取得叶节点中user_name的值返回即可。 通过这种覆盖索引直接查找的方式， 可以省略不使用覆盖索引查找的后面两个步骤， 大大的提高了查询性能，如下图 数据库索引的大致工作原理就是像文中所述， 然而细节方面可能会略有偏差，这但并不会对概念阐述的结果产生影响 。","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"Redis集群部署与集成总结","slug":"Redis集群部署与集成总结","date":"2016-12-19T02:05:54.000Z","updated":"2017-08-14T06:51:08.000Z","comments":true,"path":"2016/12/19/Redis集群部署与集成总结/","link":"","permalink":"http://yoursite.com/2016/12/19/Redis集群部署与集成总结/","excerpt":"16年末开始了解redis并加入当时所做电商项目中。遇到过一些问题，于当时做了记录。","text":"16年末开始了解redis并加入当时所做电商项目中。遇到过一些问题，于当时做了记录。 jedis客户端调用redis集群异常总结在虚拟机以及远程服务器同时测试节点全部开启，集群check命令显示主从均正常 于控制台开启某一节点的cli，set测试，正常。 在类文件中连接集群某一结点，调用，报错如下no reachable node in cluster 于application.xml中配置启动tomcat抱如下错：org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘JedisCluster’ defined in class path resource [conf/applicationContext.xml]: Could not resolve matching constructor (hint: specify index/type/name arguments for simple parameters to avoid type ambiguities) 最终解决：发现三个错误：连接池配置错误，集群不能使用jedispooljar包冲突，spring session与jedis2.7冲突，但集群又必须使用2.7，以前使用单机版redisredis集群服务器防火墙设置问题，导致no rechable nodes异常 #####后来又报了一个异常： too many nodes 解决方案：重启redis服务器还有增大连接数","categories":[],"tags":[{"name":"操作记录","slug":"操作记录","permalink":"http://yoursite.com/tags/操作记录/"}]},{"title":"LinkedHashmap源码剖析","slug":"LinkedHashMap源码剖析","date":"2016-12-03T09:37:31.000Z","updated":"2017-08-25T10:49:45.000Z","comments":true,"path":"2016/12/03/LinkedHashMap源码剖析/","link":"","permalink":"http://yoursite.com/2016/12/03/LinkedHashMap源码剖析/","excerpt":"Redis常见问题整理","text":"Redis常见问题整理转载：http://blog.csdn.net/ns_code/article/details/37867985 LinkedHashMap的源码理解起来也不难（当然，要建立在对HashMap源码有较好理解的基础上）。 ##LinkedHashMap简介 LinkedHashMap是HashMap的子类，与HashMap有着同样的存储结构，但它加入了一个双向链表的头结点，将所有put到LinkedHashmap的节点一一串成了一个双向循环链表，因此它保留了节点插入的顺序，可以使节点的输出顺序与输入顺序相同。LinkedHashMap可以用来实现LRU算法（这会在下面的源码中进行分析）。 LinkedHashMap同样是非线程安全的，只在单线程环境下使用。 ##LinkedHashMap源码剖析 LinkedHashMap源码如下（加入了详细的注释）： [java] view plain copy 1. package java.util; 2. import java.io.*; 3. 4. 5. public class LinkedHashMap&lt;K,V&gt; 6. extends HashMap&lt;K,V&gt; 7. implements Map&lt;K,V&gt; 8. { 9. 10. private static final long serialVersionUID = 3801124242820219131L; 11. 12. //双向循环链表的头结点，整个LinkedHa只哟shMap中只有一个header， 13. //它将哈希表中所有的Entry贯穿起来，header中不保存key-value对，只保存前后节点的引用 14. private transient Entry&lt;K,V&gt; header; 15. 16. //双向链表中元素排序规则的标志位。 17. //accessOrder为false，表示按插入顺序排序 18. //accessOrder为true，表示按访问顺序排序 19. private final boolean accessOrder; 20. 21. //调用HashMap的构造方法来构造底层的数组 22. public LinkedHashMap(int initialCapacity, float loadFactor) { 23. super(initialCapacity, loadFactor); 24. accessOrder = false; //链表中的元素默认按照插入顺序排序 25. } 26. 27. //加载因子取默认的0.75f 28. public LinkedHashMap(int initialCapacity) { 29. super(initialCapacity); 30. accessOrder = false; 31. } 32. 33. //加载因子取默认的0.75f，容量取默认的16 34. public LinkedHashMap() { 35. super(); 36. accessOrder = false; 37. } 38. 39. //含有子Map的构造方法，同样调用HashMap的对应的构造方法 40. public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) { 41. super(m); 42. accessOrder = false; 43. } 44. 45. //该构造方法可以指定链表中的元素排序的规则 46. public LinkedHashMap(int initialCapacity,float loadFactor,boolean accessOrder) { 47. super(initialCapacity, loadFactor); 48. this.accessOrder = accessOrder; 49. } 50. 51. //覆写父类的init()方法（HashMap中的init方法为空）， 52. //该方法在父类的构造方法和Clone、readObject中在插入元素前被调用， 53. //初始化一个空的双向循环链表，头结点中不保存数据，头结点的下一个节点才开始保存数据。 54. void init() { 55. header = new Entry&lt;K,V&gt;(-1, null, null, null); 56. header.before = header.after = header; 57. } 58. 59. 60. //覆写HashMap中的transfer方法，它在父类的resize方法中被调用， 61. //扩容后，将key-value对重新映射到新的newTable中 62. //覆写该方法的目的是为了提高复制的效率， 63. //这里充分利用双向循环链表的特点进行迭代，不用对底层的数组进行for循环。 64. void transfer(HashMap.Entry[] newTable) { 65. int newCapacity = newTable.length; 66. for (Entry&lt;K,V&gt; e = header.after; e != header; e = e.after) { 67. int index = indexFor(e.hash, newCapacity); 68. e.next = newTable[index]; 69. newTable[index] = e; 70. } 71. } 72. 73. 74. //覆写HashMap中的containsValue方法， 75. //覆写该方法的目的同样是为了提高查询的效率， 76. //利用双向循环链表的特点进行查询，少了对数组的外层for循环 77. public boolean containsValue(Object value) { 78. // Overridden to take advantage of faster iterator 79. if (value==null) { 80. for (Entry e = header.after; e != header; e = e.after) 81. if (e.value==null) 82. return true; 83. } else { 84. for (Entry e = header.after; e != header; e = e.after) 85. if (value.equals(e.value)) 86. return true; 87. } 88. return false; 89. } 90. 91. 92. //覆写HashMap中的get方法，通过getEntry方法获取Entry对象。 93. //注意这里的recordAccess方法， 94. //如果链表中元素的排序规则是按照插入的先后顺序排序的话，该方法什么也不做， 95. //如果链表中元素的排序规则是按照访问的先后顺序排序的话，则将e移到链表的末尾处。 96. public V get(Object key) { 97. Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); 98. if (e == null) 99. return null; 100. e.recordAccess(this); 101. return e.value; 102. } 103. 104. //清空HashMap，并将双向链表还原为只有头结点的空链表 105. public void clear() { 106. super.clear(); 107. header.before = header.after = header; 108. } 109. 110. //Enty的数据结构，多了两个指向前后节点的引用 111. private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; { 112. // These fields comprise the doubly linked list used for iteration. 113. Entry&lt;K,V&gt; before, after; 114. 115. //调用父类的构造方法 116. Entry(int hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) { 117. super(hash, key, value, next); 118. } 119. 120. //双向循环链表中，删除当前的Entry 121. private void remove() { 122. before.after = after; 123. after.before = before; 124. } 125. 126. //双向循环立链表中，将当前的Entry插入到existingEntry的前面 127. private void addBefore(Entry&lt;K,V&gt; existingEntry) { 128. after = existingEntry; 129. before = existingEntry.before; 130. before.after = this; 131. after.before = this; 132. } 133. 134. 135. //覆写HashMap中的recordAccess方法（HashMap中该方法为空）， 136. //当调用父类的put方法，在发现插入的key已经存在时，会调用该方法， 137. //调用LinkedHashmap覆写的get方法时，也会调用到该方法， 138. //该方法提供了LRU算法的实现，它将最近使用的Entry放到双向循环链表的尾部， 139. //accessOrder为true时，get方法会调用recordAccess方法 140. //put方法在覆盖key-value对时也会调用recordAccess方法 141. //它们导致Entry最近使用，因此将其移到双向链表的末尾 142. void recordAccess(HashMap&lt;K,V&gt; m) { 143. LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; 144. //如果链表中元素按照访问顺序排序，则将当前访问的Entry移到双向循环链表的尾部， 145. //如果是按照插入的先后顺序排序，则不做任何事情。 146. if (lm.accessOrder) { 147. lm.modCount++; 148. //移除当前访问的Entry 149. remove(); 150. //将当前访问的Entry插入到链表的尾部 151. addBefore(lm.header); 152. } 153. } 154. 155. void recordRemoval(HashMap&lt;K,V&gt; m) { 156. remove(); 157. } 158. } 159. 160. //迭代器 161. private abstract class LinkedHashIterator&lt;T&gt; implements Iterator&lt;T&gt; { 162. Entry&lt;K,V&gt; nextEntry = header.after; 163. Entry&lt;K,V&gt; lastReturned = null; 164. 165. /** 166. * The modCount value that the iterator believes that the backing 167. * List should have. If this expectation is violated, the iterator 168. * has detected concurrent modification. 169. */ 170. int expectedModCount = modCount; 171. 172. public boolean hasNext() { 173. return nextEntry != header; 174. } 175. 176. public void remove() { 177. if (lastReturned == null) 178. throw new IllegalStateException(); 179. if (modCount != expectedModCount) 180. throw new ConcurrentModificationException(); 181. 182. LinkedHashMap.this.remove(lastReturned.key); 183. lastReturned = null; 184. expectedModCount = modCount; 185. } 186. 187. //从head的下一个节点开始迭代 188. Entry&lt;K,V&gt; nextEntry() { 189. if (modCount != expectedModCount) 190. throw new ConcurrentModificationException(); 191. if (nextEntry == header) 192. throw new NoSuchElementException(); 193. 194. Entry&lt;K,V&gt; e = lastReturned = nextEntry; 195. nextEntry = e.after; 196. return e; 197. } 198. } 199. 200. //key迭代器 201. private class KeyIterator extends LinkedHashIterator&lt;K&gt; { 202. public K next() { return nextEntry().getKey(); } 203. } 204. 205. //value迭代器 206. private class ValueIterator extends LinkedHashIterator&lt;V&gt; { 207. public V next() { return nextEntry().value; } 208. } 209. 210. //Entry迭代器 211. private class EntryIterator extends LinkedHashIterator&lt;Map.Entry&lt;K,V&gt;&gt; { 212. public Map.Entry&lt;K,V&gt; next() { return nextEntry(); } 213. } 214. 215. // These Overrides alter the behavior of superclass view iterator() methods 216. Iterator&lt;K&gt; newKeyIterator() { return new KeyIterator(); } 217. Iterator&lt;V&gt; newValueIterator() { return new ValueIterator(); } 218. Iterator&lt;Map.Entry&lt;K,V&gt;&gt; newEntryIterator() { return new EntryIterator(); } 219. 220. 221. //覆写HashMap中的addEntry方法，LinkedHashmap并没有覆写HashMap中的put方法， 222. //而是覆写了put方法所调用的addEntry方法和recordAccess方法， 223. //put方法在插入的key已存在的情况下，会调用recordAccess方法， 224. //在插入的key不存在的情况下，要调用addEntry插入新的Entry 225. void addEntry(int hash, K key, V value, int bucketIndex) { 226. //创建新的Entry，并插入到LinkedHashMap中 227. createEntry(hash, key, value, bucketIndex); 228. 229. //双向链表的第一个有效节点（header后的那个节点）为近期最少使用的节点 230. Entry&lt;K,V&gt; eldest = header.after; 231. //如果有必要，则删除掉该近期最少使用的节点， 232. //这要看对removeEldestEntry的覆写,由于默认为false，因此默认是不做任何处理的。 233. if (removeEldestEntry(eldest)) { 234. removeEntryForKey(eldest.key); 235. } else { 236. //扩容到原来的2倍 237. if (size &gt;= threshold) 238. resize(2 * table.length); 239. } 240. } 241. 242. void createEntry(int hash, K key, V value, int bucketIndex) { 243. //创建新的Entry，并将其插入到数组对应槽的单链表的头结点处，这点与HashMap中相同 244. HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; 245. Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); 246. table[bucketIndex] = e; 247. //每次插入Entry时，都将其移到双向链表的尾部， 248. //这便会按照Entry插入LinkedHashMap的先后顺序来迭代元素， 249. //同时，新put进来的Entry是最近访问的Entry，把其放在链表末尾 ，符合LRU算法的实现 250. e.addBefore(header); 251. size++; 252. } 253. 254. //该方法是用来被覆写的，一般如果用LinkedHashmap实现LRU算法，就要覆写该方法， 255. //比如可以将该方法覆写为如果设定的内存已满，则返回true，这样当再次向LinkedHashMap中put 256. //Entry时，在调用的addEntry方法中便会将近期最少使用的节点删除掉（header后的那个节点）。 257. protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { 258. return false; 259. } 260. } ##几点总结 关于LinkedHashMap的源码，给出以下几点比较重要的总结： 1、从源码中可以看出，LinkedHashMap中加入了一个head头结点，将所有插入到该LinkedHashMap中的Entry按照插入的先后顺序依次加入到以head为头结点的双向循环链表的尾部。实际上就是HashMap和LinkedList两个集合类的存储结构的结合。在LinkedHashMapMap中，所有put进来的Entry都保存在如第一个图所示的哈希表中，但它又额外定义了一个以head为头结点的空的双向循环链表，每次put进来Entry，除了将其保存到对哈希表中对应的位置上外，还要将其插入到双向循环链表的尾部。2、LinkedHashMap由于继承自HashMap，因此它具有HashMap的所有特性，同样允许key和value为null。3、注意源码中的accessOrder标志位，当它false时，表示双向链表中的元素按照Entry插入LinkedHashMap到中的先后顺序排序，即每次put到LinkedHashMap中的Entry都放在双向链表的尾部，这样遍历双向链表时，Entry的输出顺序便和插入的顺序一致，这也是默认的双向链表的存储顺序；当它为true时，表示双向链表中的元素按照访问的先后顺序排列，可以看到，虽然Entry插入链表的顺序依然是按照其put到LinkedHashMap中的顺序，但put和get方法均有调用recordAccess方法（put方法在key相同，覆盖原有的Entry的情况下调用recordAccess方法），该方法判断accessOrder是否为true，如果是，则将当前访问的Entry（put进来的Entry或get出来的Entry）移到双向链表的尾部（key不相同时，put新Entry时，会调用addEntry，它会调用creatEntry，该方法同样将新插入的元素放入到双向链表的尾部，既符合插入的先后顺序，又符合访问的先后顺序，因为这时该Entry也被访问了），否则，什么也不做。4、注意构造方法，前四个构造方法都将accessOrder设为false，说明默认是按照插入顺序排序的，而第五个构造方法可以自定义传入的accessOrder的值，因此可以指定双向循环链表中元素的排序规则，一般要用LinkedHashMap实现LRU算法，就要用该构造方法，将accessOrder置为true。5、LinkedHashMap并没有覆写HashMap中的put方法，而是覆写了put方法中调用的addEntry方法和recordAccess方法，我们回过头来再看下HashMap的put方法：[java] view plain copy 1. // 将“key-value”添加到HashMap中 2. public V put(K key, V value) { 3. // 若“key为null”，则将该键值对添加到table[0]中。 4. if (key == null) 5. return putForNullKey(value); 6. // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 7. int hash = hash(key.hashCode()); 8. int i = indexFor(hash, table.length); 9. for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { 10. Object k; 11. // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ 12. if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { 13. V oldValue = e.value; 14. e.value = value; 15. e.recordAccess(this); 16. return oldValue; 17. } 18. } 19. 20. // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 21. modCount++; 22. //将key-value添加到table[i]处 23. addEntry(hash, key, value, i); 24. return null; 25. } 当要put进来的Entry的key在哈希表中已经在存在时，会调用recordAccess方法，当该key不存在时，则会调用addEntry方法将新的Entry插入到对应槽的单链表的头部。 我们先来看recordAccess方法： [java] view plain copy 1. //覆写HashMap中的recordAccess方法（HashMap中该方法为空）， 2. //当调用父类的put方法，在发现插入的key已经存在时，会调用该方法， 3. //调用LinkedHashmap覆写的get方法时，也会调用到该方法， 4. //该方法提供了LRU算法的实现，它将最近使用的Entry放到双向循环链表的尾部， 5. //accessOrder为true时，get方法会调用recordAccess方法 6. //put方法在覆盖key-value对时也会调用recordAccess方法 7. //它们导致Entry最近使用，因此将其移到双向链表的末尾 8. void recordAccess(HashMap&lt;K,V&gt; m) { 9. LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; 10. //如果链表中元素按照访问顺序排序，则将当前访问的Entry移到双向循环链表的尾部， 11. //如果是按照插入的先后顺序排序，则不做任何事情。 12. if (lm.accessOrder) { 13. lm.modCount++; 14. //移除当前访问的Entry 15. remove(); 16. //将当前访问的Entry插入到链表的尾部 17. addBefore(lm.header); 18. } 19. } 该方法会判断accessOrder是否为true，如果为true，它会将当前访问的Entry（在这里指put进来的Entry）移动到双向循环链表的尾部，从而实现双向链表中的元素按照访问顺序来排序（最近访问的Entry放到链表的最后，这样多次下来，前面就是最近没有被访问的元素，在实现、LRU算法时，当双向链表中的节点数达到最大值时，将前面的元素删去即可，因为前面的元素是最近最少使用的），否则什么也不做。 再来看addEntry方法： [java] view plain copy 1. //覆写HashMap中的addEntry方法，LinkedHashmap并没有覆写HashMap中的put方法， 2. //而是覆写了put方法所调用的addEntry方法和recordAccess方法， 3. //put方法在插入的key已存在的情况下，会调用recordAccess方法， 4. //在插入的key不存在的情况下，要调用addEntry插入新的Entry 5. void addEntry(int hash, K key, V value, int bucketIndex) { 6. //创建新的Entry，并插入到LinkedHashMap中 7. createEntry(hash, key, value, bucketIndex); 8. 9. //双向链表的第一个有效节点（header后的那个节点）为近期最少使用的节点 10. Entry&lt;K,V&gt; eldest = header.after; 11. //如果有必要，则删除掉该近期最少使用的节点， 12. //这要看对removeEldestEntry的覆写,由于默认为false，因此默认是不做任何处理的。 13. if (removeEldestEntry(eldest)) { 14. removeEntryForKey(eldest.key); 15. } else { 16. //扩容到原来的2倍 17. if (size &gt;= threshold) 18. resize(2 * table.length); 19. } 20. } 21. 22. void createEntry(int hash, K key, V value, int bucketIndex) { 23. //创建新的Entry，并将其插入到数组对应槽的单链表的头结点处，这点与HashMap中相同 24. HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; 25. Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); 26. table[bucketIndex] = e; 27. //每次插入Entry时，都将其移到双向链表的尾部， 28. //这便会按照Entry插入LinkedHashMap的先后顺序来迭代元素， 29. //同时，新put进来的Entry是最近访问的Entry，把其放在链表末尾 ，符合LRU算法的实现 30. e.addBefore(header); 31. size++; 32. } 同样是将新的Entry插入到table中对应槽所对应单链表的头结点中，但可以看出，在createEntry中，同样把新put进来的Entry插入到了双向链表的尾部，从插入顺序的层面来说，新的Entry插入到双向链表的尾部，可以实现按照插入的先后顺序来迭代Entry，而从访问顺序的层面来说，新put进来的Entry又是最近访问的Entry，也应该将其放在双向链表的尾部。 上面还有个removeEldestEntry方法，该方法如下： [java] view plain copy 1. //该方法是用来被覆写的，一般如果用LinkedHashmap实现LRU算法，就要覆写该方法， 2. //比如可以将该方法覆写为如果设定的内存已满，则返回true，这样当再次向LinkedHashMap中put 3. //Entry时，在调用的addEntry方法中便会将近期最少使用的节点删除掉（header后的那个节点）。 4. protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { 5. return false; 6. } 7. } 该方法默认返回false，我们一般在用LinkedHashMap实现LRU算法时，要覆写该方法，一般的实现是，当设定的内存（这里指节点个数）达到最大值时，返回true，这样put新的Entry（该Entry的key在哈希表中没有已经存在）时，就会调用removeEntryForKey方法，将最近最少使用的节点删除（head后面的那个节点，实际上是最近没有使用）。 6、LinkedHashMap覆写了HashMap的get方法： [java] view plain copy 1. //覆写HashMap中的get方法，通过getEntry方法获取Entry对象。 2. //注意这里的recordAccess方法， 3. //如果链表中元素的排序规则是按照插入的先后顺序排序的话，该方法什么也不做， 4. //如果链表中元素的排序规则是按照访问的先后顺序排序的话，则将e移到链表的末尾处。 5. public V get(Object key) { 6. Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); 7. if (e == null) 8. return null; 9. e.recordAccess(this); 10. return e.value; 11. } 先取得Entry，如果不为null，一样调用recordAccess方法，上面已经说得很清楚，这里不在多解释了。 7、最后说说LinkedHashMap是如何实现LRU的。首先，当accessOrder为true时，才会开启按访问顺序排序的模式，才能用来实现LRU算法。我们可以看到，无论是put方法还是get方法，都会导致目标Entry成为最近访问的Entry，因此便把该Entry加入到了双向链表的末尾（get方法通过调用recordAccess方法来实现，put方法在覆盖已有key的情况下，也是通过调用recordAccess方法来实现，在插入新的Entry时，则是通过createEntry中的addBefore方法来实现），这样便把最近使用了的Entry放入到了双向链表的后面，多次操作后，双向链表前面的Entry便是最近没有使用的，这样当节点个数满的时候，删除的最前面的Entry(head后面的那个Entry)便是最近最少使用的Entry。","categories":[],"tags":[{"name":"Java容器相关","slug":"Java容器相关","permalink":"http://yoursite.com/tags/Java容器相关/"}]},{"title":"Redis常见问题整理","slug":"Redis常见问题整理","date":"2016-12-03T09:37:31.000Z","updated":"2017-08-25T07:52:40.000Z","comments":true,"path":"2016/12/03/Redis常见问题整理/","link":"","permalink":"http://yoursite.com/2016/12/03/Redis常见问题整理/","excerpt":"Redis常见问题整理","text":"Redis常见问题整理 整理自三篇博客： https://www.52pojie.cn/thread-558953-1-1.html http://www.cnblogs.com/jiahaoJAVA/p/6244278.html http://blog.csdn.net/guchuanyun111/article/details/52064870 什么是redis?Redis 是一个基于内存的高性能key-value数据库。 (有空再补充，有理解错误或不足欢迎指正) Reids的特点Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。 Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能，比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。 Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis支持的数据类型Redis通过Key-Value的单值不同类型来区分, 以下是支持的类型: Strings Lists Sets 求交集、并集 Sorted Set hashes 为什么redis需要把所有数据放到内存中？Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。 Redis是单进程单线程的redis利用队列技术将并发访问变为串行访问，消除了传统数据库串行控制的开销 虚拟内存当你的key很小而value很大时,使用VM的效果会比较好.因为这样节约的内存比较大. 当你的key不小时,可以考虑使用一些非常方法将很大的key变成很大的value,比如你可以考虑将key,value组合成一个新的value. vm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.自己测试的时候发现用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库 分布式redis支持主从的模式。原则：Master会将数据同步到slave，而slave不会将数据同步到master。Slave启动时会连接master来同步数据。 这是一个典型的分布式读写分离模型。我们可以利用master来插入数据，slave提供检索服务。这样可以有效减少单个机器的并发访问数量 读写分离模型通过增加Slave DB的数量，读的性能可以线性增长。为了避免Master DB的单点故障，集群一般都会采用两台Master DB做双机热备，所以整个集群的读和写的可用性都非常高。 读写分离架构的缺陷在于，不管是Master还是Slave，每个节点都必须保存完整的数据，如果在数据量很大的情况下，集群的扩展能力还是受限于单个节点的存储能力，而且对于Write-intensive类型的应用，读写分离架构并不适合。 数据分片模型为了解决读写分离模型的缺陷，可以将数据分片模型应用进来。 可以将每个节点看成都是独立的master，然后通过业务实现数据分片。 结合上面两种模型，可以将每个master设计成由一个master和多个slave组成的模型。 Redis的五种数据类型字符串string：字符串类型是Redis中最为基础的数据存储类型，是一个由字节组成的序列，他在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据货Json对象描述信息等，是标准的key-value，一般来存字符串，整数和浮点数。Value最多可以容纳的数据长度为512MB应用场景：很常见的场景用于统计网站访问数量，当前在线人数等。incr命令(++操作) 列表list：Redis的列表允许用户从序列的两端推入或者弹出元素，列表由多个字符串值组成的有序可重复的序列，是链表结构。好比Java的linkedList，在往两端插入和删除数据时，效率是非常高的，往中间插入数据效率是很低下的。List中可以包含的最大元素数量是4294967295。应用场景：1.最新消息排行榜。2.消息队列，以完成多程序之间的消息交换。可以用push操作将任务存在list中（生产者），然后线程在用pop操作将任务取出进行执行。（消费者） 集合set：Redis的集合是无序不可重复的，和列表一样，在执行插入和删除和判断是否存在某元素时，效率是很高的。集合最大的优势在于可以进行交集并集差集操作。Set可包含的最大元素数量是4294967295。应用场景：1.利用交集求共同好友。2.利用唯一性，可以统计访问网站的所有独立IP。3.好友推荐的时候根据tag求交集，大于某个threshold（临界值的）就可以推荐。 散列hash：Redis中的散列可以看成具有String key和String value的map容器，可以将多个key-value存储到一个key中。每一个Hash可以存储4294967295个键值对。应用场景：例如存储、读取、修改用户属性（name，age，pwd等） 有序集合zset：和set很像，都是字符串的集合，都不允许重复的成员出现在一个set中。他们之间差别在于有序集合中每一个成员都会有一个分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。尽管有序集合中的成员必须是卫衣的，但是分数(score)却可以重复。应用场景：可以用于一个大型在线游戏的积分排行榜，每当玩家的分数发生变化时，可以执行zadd更新玩家分数(score)，此后在通过zrange获取几分top ten的用户信息。 使用Redis有哪些好处？(1) 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) (2) 支持丰富数据类型，支持string，list，set，sorted set，hash (3) 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 (4) 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 redis相比memcached有哪些优势？(1) memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 (2) redis的速度比memcached快很多 (3) redis可以持久化其数据 redis常见性能问题和解决方案：(1) Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件 (2) 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次 (3) 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内 (4) 尽量避免在压力很大的主库上增加从库 (5) 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… 这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变。 MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据 相关知识：redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。redis 提供 6种数据淘汰策略： voltile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 Memcache与Redis的区别都有哪些？1)、存储方式Memecache把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis有部份存在硬盘上，这样能保证数据的持久性。 2)、数据支持类型Memcache对数据类型支持相对简单。 Redis有复杂的数据类型。 3)、使用底层模型不同它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。 Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 4），value大小redis最大可以达到1GB，而memcache只有1MB Redis 常见的性能问题都有哪些？如何解决？1).Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以Master最好不要写内存快照。 2).Master AOF持久化，如果不重写AOF文件，这个持久化方式对性能的影响是最小的，但是AOF文件会不断增大，AOF文件过大会影响Master重启的恢复速度。Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化,如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 3).Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 4). Redis主从复制的性能问题，为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内 redis 最适合的场景Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： 1 、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 2 、Redis支持数据的备份，即master-slave模式的数据备份。 3 、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 （1）、会话缓存（Session Cache）最常用的一种使用Redis的情景是会话缓存（session cache）。用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用Redis来缓存会话的文档。甚至广为人知的商业平台Magento也提供Redis的插件。 （2）、全页缓存（FPC） 除基本的会话token之外，Redis还提供很简便的FPC平台。回到一致性问题，即使重启了Redis实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。 再次以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。 此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）、队列 Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。Redis作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。 如果你快速的在Google中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用Redis创建非常好的后端工具，以满足各种队列需求。例如，Celery有一个后台就是使用Redis作为broker，你可以从这里去查看。 （4），排行榜/计数器 Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的10个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行： ZRANGE user_scores 0 10 WITHSCORES Agora Games就是一个很好的例子，用Ruby实现的，它的排行榜就是使用Redis来存储数据的，你可以在这里看到。 （5）、发布/订阅 最后（但肯定不是最不重要的）是Redis的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用Redis的发布/订阅功能来建立聊天系统！（不，这是真的，你可以去核实）。 Redis提供的所有特性中，我感觉这个是喜欢的人最少的一个，虽然它为用户提供如果此多功能。","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"synchronized关键字、ReentrantLock与原子类比较","slug":"synchronized关键字、ReentrantLock与原子类比较","date":"2016-12-03T09:37:31.000Z","updated":"2017-08-25T10:20:20.000Z","comments":true,"path":"2016/12/03/synchronized关键字、ReentrantLock与原子类比较/","link":"","permalink":"http://yoursite.com/2016/12/03/synchronized关键字、ReentrantLock与原子类比较/","excerpt":"synchronized关键字、ReentrantLock与原子类比较","text":"synchronized关键字、ReentrantLock与原子类比较我们先比较ReentrantLock与synchronized的用法 1、ReentrantLock拥有synchronized相同的并发性和内存语义，此外还多了锁投票，定时锁等候和中断等候。线程A和B都要获取对象O的锁定，假设A获取了对象O锁，B将等待A释放对O的锁定。 使用synchronized时，如果A不释放，B将一直等待下去，无法中断。 使用ReentrantLock时，如果A不释放，B可以在等待足够长时间后，停止等待，继续执行其他事务。 ReentrantLock获取锁定的三种方式：lock()，如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于休眠状态，直至获取锁。tryLock()，如果获取了锁立即返回true，如果别的线程下持有，立即返回false;tryLock(long timeout, TimeUnit unit)，如果获取了锁定立即返回true，如果别的线程正持有锁，将等待参数给定的时间，在等待的过程中，如果获取了锁定，返回true，如果等待超时，返回false，所以lock()方法相当trylock传递个无限大的时间参数;lockInteruptibly，如果获取了锁定立即返回，反之，当前线程处理休眠，直至获取锁，或者当前线程线程被其他线程中断。 2、synchronized是在JVM层面实现，不但可以通过一些监控工具监控锁定，而且在代码执行出现异常，JVM自动释放锁定;Lock是通过代码实现，为了保证锁定一定会被释放，一般会将unLock()放到flyinal{}中。 3、在资源竞争不激烈的情况下，synchronized的性能要优于ReentrantLock，但在资源竞争很激烈的情况下，synchronized的性能会下降几十倍，但是ReentrantLock的性能能维持常态。 ReentrantLock例子如下： [java] view plain copy 1. package test.lock; 2. 3. import java.util.concurrent.locks.Lock; 4. import java.util.concurrent.locks.ReentrantLock; 5. 6. public class LockTest1 extends Thread { 7. 8. private int threadNo; 9. private static Lock lock = new ReentrantLock(); 10. 11. public LockTest1(int threadNo) { 12. this.threadNo = threadNo; 13. } 14. 15. public static void main(String[] args) throws InterruptedException { 16. for (int i = 0; i &lt; 10; i++) { 17. new LockTest1(i).start(); 18. } 19. } 20. 21. @Override 22. public void run() { 23. lock.lock(); 24. try { 25. for (int i = 0; i &lt; 100; i++) { 26. System.out.println(&quot;No.&quot; + (threadNo + 1) + &quot;: &quot; + (i + 1)); 27. } 28. } finally { 29. lock.unlock(); 30. } 31. } 32. } 下面讨论原子类：synchronized关键字、Lock可以控制程序片段的同步，原子类只能保证单个变量的同步。线程竞争不激烈时，原子类性能比synchronized略低，当竞争激烈时，也能维持常态。 下面是一个多线程共同计数的代码代码。 [java] view plain copy 1. package test.lock; 2. 3. 4. public class LockTest2 extends Thread { 5. 6. private static int race = 0; 7. private int threadNo; 8. 9. public LockTest2(int threadNo) { 10. this.threadNo = threadNo; 11. } 12. 13. public static void main(String[] args) throws InterruptedException { 14. for (int i = 0; i &lt; 10; i++) { 15. new LockTest2(i).start(); 16. } 17. 18. while (Thread.activeCount() &gt; 1) { 19. Thread.yield(); 20. } 21. System.out.println(race); 22. } 23. 24. @Override 25. public void run() { 26. for (int i = 0; i &lt; 1000; i++) { 27. race++; 28. } 29. } 30. } 上面程序执行后，并没有得到期望的原子类。我们用ActomicInteger实现代码如下： [java] view plain copy 1. package test.lock; 2. 3. import java.util.concurrent.atomic.AtomicInteger; 4. 5. public class LockTest2 extends Thread { 6. 7. private static AtomicInteger race = new AtomicInteger(); 8. private int threadNo; 9. 10. public LockTest2(int threadNo) { 11. this.threadNo = threadNo; 12. } 13. 14. public static void main(String[] args) throws InterruptedException { 15. for (int i = 0; i &lt; 10; i++) { 16. new LockTest2(i).start(); 17. } 18. 19. while (Thread.activeCount() &gt; 1) { 20. Thread.yield(); 21. } 22. System.out.println(race); 23. } 24. 25. @Override 26. public void run() { 27. for (int i = 0; i &lt; 1000; i++) { 28. race.addAndGet(1); 29. } 30. } 31. } 转载地址：http://blog.csdn.net/lanxiangru/article/details/53384767","categories":[],"tags":[{"name":"并发相关","slug":"并发相关","permalink":"http://yoursite.com/tags/并发相关/"}]},{"title":"TreeMap源码剖析","slug":"TreeMap源码剖析","date":"2016-12-03T09:37:31.000Z","updated":"2017-08-26T03:36:44.000Z","comments":true,"path":"2016/12/03/TreeMap源码剖析/","link":"","permalink":"http://yoursite.com/2016/12/03/TreeMap源码剖析/","excerpt":"","text":"几点总结 本文对TreeMap的分析较前几篇文章有些浅尝辄止，TreeMap用的没有HashMap那么多，我们有个宏观上的把我和比较即可。 1、TreeMap是基于红黑树实现的TreeMap是根据key进行排序的，它的排序和定位需要依赖比较器或覆写Comparable接口，也因此不需要key覆写hashCode方法和equals方法，就可以排除掉重复的key，而HashMap的key则需要通过覆写hashCode方法和equals方法来确保没有重复的key。 2、TreeMap的查询、插入、删除效率均没有HashMap高，一般只有要对key排序时才使用TreeMap。 3、TreeMap的key不能为null，而HashMap的key可以为null。 注：对TreeSet和HashSet的源码不再进行剖析，二者分别是基于TreeMap和HashMap实现的，只是对应的节点中只有key，而没有value，因此对TreeMap和HashMap比较了解的话，对TreeSet和HashSet的理解就会非常容易。 前言 本文不打算延续前几篇的风格（对所有的源码加入注释），因为要理解透TreeMap的所有源码，对博主来说，确实需要耗费大量的时间和经历，目前看来不大可能有这么多时间的投入，故这里意在通过于阅读源码对TreeMap有个宏观上的把握，并就其中一些方法的实现做比较深入的分析。 红黑树简介 TreeMap是基于红黑树实现的，这里只对红黑树做个简单的介绍，红黑树是一种特殊的二叉排序树，关于二叉排序树，参见：http://blog.csdn.net/ns_code/article/details/19823463，红黑树通过一些限制，使其不会出现二叉树排序树中极端的一边倒的情况，相对二叉排序树而言，这自然提高了查询的效率。 二叉排序树的基本性质如下： 1、每个节点都只能是红色或者黑色 2、根节点是黑色 3、每个叶节点（NIL节点，空节点）是黑色的。 4、如果一个结点是红的，则它两个子节点都是黑的。也就是说在一条路径上不能出现相邻的两个红色结点。 5、从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 正是这些性质的限制，使得红黑树中任一节点到其子孙叶子节点的最长路径不会长于最短路径的2倍，因此它是一种接近平衡的二叉树。 说到红黑树，自然不免要和AVL树对比一番。相比较而言，AVL树是严格的平衡二叉树，而红黑树不算严格意义上的平衡二叉树，只是接近平衡，不会让树的高度如BST极端情况那样等于节点的个数。其实能用到红黑树的地方，也都可以用AVL树来实现，但红黑树的应用却非常广泛，而AVL树则很少被使用。在执行插入、删除操作时，AVL树需要调整的次数一般要比红黑树多（红黑树的旋转调整最多只需三次），效率相对较低，且红黑树的统计性能较AVL树要好，当然AVL树在查询效率上可能更胜一筹，但实际上也高不了多少。 红黑树的插入删除操作很简单，就是单纯的二叉排序树的插入删除操作。红黑树被认为比较变态的地方自然在于插入删除后对红黑树的调整操作（旋转和着色），主要是情况分的很多，限于篇幅及博主的熟悉程度优先，这里不打算详细介绍插入删除后调整红黑树的各种情况及其实现，我们有个宏观上的了解即可，如须详细了解，参见算法导论或一些相关的资料。 TreeMap源码剖析 存储结构 TreeMap的排序是基于对key的排序实现的，它的每一个Entry代表红黑树的一个节点，Entry的数据结构如下： [java] view plain copy 1. static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { 2. // 键 3. K key; 4. // 值 5. V value; 6. // 左孩子 7. Entry&lt;K,V&gt; left = null; 8. // 右孩子 9. Entry&lt;K,V&gt; right = null; 10. // 父节点 11. Entry&lt;K,V&gt; parent; 12. // 当前节点颜色 13. boolean color = BLACK; 14. 15. // 构造函数 16. Entry(K key, V value, Entry&lt;K,V&gt; parent) { 17. this.key = key; 18. this.value = value; 19. this.parent = parent; 20. } 21. 22. 。。。。。 23. } 构造方法 先来看下TreeMap的构造方法。TreeMap一共有4个构造方法。 1、无参构造方法 [java] view plain copy public TreeMap() { comparator = null; } 采用无参构造方法，不指定比较器，这时候，排序的实现要依赖key.compareTo()方法，因此key必须实现Comparable接口，并覆写其中的compareTo方法。 2、带有比较器的构造方法 [java] view plain copy public TreeMap(Comparator&lt;? super K&gt; comparator) { this.comparator = comparator; } 采用带比较器的构造方法，这时候，排序依赖该比较器，key可以不用实现Comparable接口。 3、带Map的构造方法 [java] view plain copy 1. public TreeMap(Map&lt;? extends K, ? extends V&gt; m) { 2. comparator = null; 3. putAll(m); 4. } 该构造方法同样不指定比较器，调用putAll方法将Map中的所有元素加入到TreeMap中。putAll的源码如下： [java] view plain copy 1. // 将map中的全部节点添加到TreeMap中 2. public void putAll(Map&lt;? extends K, ? extends V&gt; map) { 3. // 获取map的大小 4. int mapSize = map.size(); 5. // 如果TreeMap的大小是0,且map的大小不是0,且map是已排序的“key-value对” 6. if (size==0 &amp;&amp; mapSize!=0 &amp;&amp; map instanceof SortedMap) { 7. Comparator c = ((SortedMap)map).comparator(); 8. // 如果TreeMap和map的比较器相等； 9. // 则将map的元素全部拷贝到TreeMap中，然后返回！ 10. if (c == comparator || (c != null &amp;&amp; c.equals(comparator))) { 11. ++modCount; 12. try { 13. buildFromSorted(mapSize, map.entrySet().iterator(), 14. null, null); 15. } catch (java.io.IOException cannotHappen) { 16. } catch (ClassNotFoundException cannotHappen) { 17. } 18. return; 19. } 20. } 21. // 调用AbstractMap中的putAll(); 22. // AbstractMap中的putAll()又会调用到TreeMap的put() 23. super.putAll(map); 24. } 显然，如果Map里的元素是排好序的，就调用buildFromSorted方法来拷贝Map中的元素，这在下一个构造方法中会重点提及，而如果Map中的元素不是排好序的，就调用AbstractMap的putAll(map)方法，该方法源码如下： [java] view plain copy 1. public void putAll(Map&lt;? extends K, ? extends V&gt; m) { 2. for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) 3. put(e.getKey(), e.getValue()); 4. } 很明显它是将Map中的元素一个个put（插入）到TreeMap中的，主要因为Map中的元素是无序存放的，因此要一个个插入到红黑树中，使其有序存放，并满足红黑树的性质。 4、带有SortedMap的构造方法 [java] view plain copy 1. public TreeMap(SortedMap&lt;K, ? extends V&gt; m) { 2. comparator = m.comparator(); 3. try { 4. buildFromSorted(m.size(), m.entrySet().iterator(), null, null); 5. } catch (java.io.IOException cannotHappen) { 6. } catch (ClassNotFoundException cannotHappen) { 7. } 8. } 首先将比较器指定为m的比较器，这取决于生成m时调用构造方法是否传入了指定的构造器，而后调用buildFromSorted方法，将SortedMap中的元素插入到TreeMap中，由于SortedMap中的元素师有序的，实际上它是根据SortedMap创建的TreeMap，将SortedMap中对应的元素添加到TreeMap中。 插入删除 插入操作即对应TreeMap的put方法，put操作实际上只需按照二叉排序树的插入步骤来操作即可，插入到指定位置后，再做调整，使其保持红黑树的特性。put源码的实现： [java] view plain copy 1. public V put(K key, V value) { 2. Entry&lt;K,V&gt; t = root; 3. // 若红黑树为空，则插入根节点 4. if (t == null) { 5. // TBD: 6. // 5045147: (coll) Adding null to an empty TreeSet should 7. // throw NullPointerException 8. // 9. // compare(key, key); // type check 10. root = new Entry&lt;K,V&gt;(key, value, null); 11. size = 1; 12. modCount++; 13. return null; 14. } 15. int cmp; 16. Entry&lt;K,V&gt; parent; 17. // split comparator and comparable paths 18. Comparator&lt;? super K&gt; cpr = comparator; 19. // 找出(key, value)在二叉排序树中的插入位置。 20. // 红黑树是以key来进行排序的，所以这里以key来进行查找。 21. if (cpr != null) { 22. do { 23. parent = t; 24. cmp = cpr.compare(key, t.key); 25. if (cmp &lt; 0) 26. t = t.left; 27. else if (cmp &gt; 0) 28. t = t.right; 29. else 30. return t.setValue(value); 31. } while (t != null); 32. } 33. else { 34. if (key == null) 35. throw new NullPointerException(); 36. Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; 37. do { 38. parent = t; 39. cmp = k.compareTo(t.key); 40. if (cmp &lt; 0) 41. t = t.left; 42. else if (cmp &gt; 0) 43. t = t.right; 44. else 45. return t.setValue(value); 46. } while (t != null); 47. } 48. // 为（key-value）新建节点 49. Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(key, value, parent); 50. if (cmp &lt; 0) 51. parent.left = e; 52. else 53. parent.right = e; 54. // 插入新的节点后，调用fixAfterInsertion调整红黑树。 55. fixAfterInsertion(e); 56. size++; 57. modCount++; 58. return null; 59. } 这里的fixAfterInsertion便是节点插入后对树进行调整的方法，这里不做介绍。删除操作及对应TreeMap的deleteEntry方法，deleteEntry方法同样也只需按照二叉排序树的操作步骤实现即可，删除指定节点后，再对树进行调整即可。deleteEntry方法的实现源码如下： [java] view plain copy 1. // 删除“红黑树的节点p” 2. private void deleteEntry(Entry&lt;K,V&gt; p) { 3. modCount++; 4. size--; 5. 6. if (p.left != null &amp;&amp; p.right != null) { 7. Entry&lt;K,V&gt; s = successor (p); 8. p.key = s.key; 9. p.value = s.value; 10. p = s; 11. } 12. 13. Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); 14. 15. if (replacement != null) { 16. replacement.parent = p.parent; 17. if (p.parent == null) 18. root = replacement; 19. else if (p == p.parent.left) 20. p.parent.left = replacement; 21. else 22. p.parent.right = replacement; 23. 24. p.left = p.right = p.parent = null; 25. 26. if (p.color == BLACK) 27. fixAfterDeletion(replacement); 28. } else if (p.parent == null) { 29. root = null; 30. } else { 31. if (p.color == BLACK) 32. fixAfterDeletion(p); 33. 34. if (p.parent != null) { 35. if (p == p.parent.left) 36. p.parent.left = null; 37. else if (p == p.parent.right) 38. p.parent.right = null; 39. p.parent = null; 40. } 41. } 42. } 后面的fixAfterDeletion方法便是节点删除后对树进行调整的方法，这里不做介绍。 其他很多方法这里不再一一介绍。 几点总结 本文对TreeMap的分析较前几篇文章有些浅尝辄止，TreeMap用的没有HashMap那么多，我们有个宏观上的把我和比较即可。 1、TreeMap是根据key进行排序的，它的排序和定位需要依赖比较器或覆写Comparable接口，也因此不需要key覆写hashCode方法和equals方法，就可以排除掉重复的key，而HashMap的key则需要通过覆写hashCode方法和equals方法来确保没有重复的key。 2、TreeMap的查询、插入、删除效率均没有HashMap高，一般只有要对key排序时才使用TreeMap。 3、TreeMap的key不能为null，而HashMap的key可以为null。 注：对TreeSet和HashSet的源码不再进行剖析，二者分别是基于TreeMap和HashMap实现的，只是对应的节点中只有key，而没有value，因此对TreeMap和HashMap比较了解的话，对TreeSet和HashSet的理解就会非常容易。 转载：http://blog.csdn.net/ns_code/article/details/36421085","categories":[],"tags":[{"name":"Java容器相关","slug":"Java容器相关","permalink":"http://yoursite.com/tags/Java容器相关/"}]},{"title":"springMVC工作流程","slug":"SpringMVC工作流程","date":"2016-12-03T09:37:31.000Z","updated":"2017-08-25T08:31:30.000Z","comments":true,"path":"2016/12/03/SpringMVC工作流程/","link":"","permalink":"http://yoursite.com/2016/12/03/SpringMVC工作流程/","excerpt":"","text":"1.Spring MVC概述：Spring MVC是Spring提供的一个强大而灵活的web框架。借助于注解，Spring MVC提供了几乎是POJO的开发模式，使得控制器的开发和测试更加简单。这些控制器一般不直接处理请求，而是将其委托给Spring上下文中的其他bean，通过Spring的依赖注入功能，这些bean被注入到控制器中。 Spring MVC主要由DispatcherServlet、处理器映射、处理器(控制器)、视图解析器、视图组成。他的两个核心是两个核心：处理器映射：选择使用哪个控制器来处理请求视图解析器：选择结果应该如何渲染通过以上两点，Spring MVC保证了如何选择控制处理请求和如何选择视图展现输出之间的松耦合。2.SpringMVC运行原理(1) Http请求：客户端请求提交到DispatcherServlet。 (2) 寻找处理器：由DispatcherServlet控制器查询一个或多个HandlerMapping，找到处理请求的Controller。 (3) 调用处理器：DispatcherServlet将请求提交到Controller。 (4)(5)调用业务处理和返回结果：Controller调用业务逻辑处理后，返回ModelAndView。 (6)(7)处理视图映射并返回模型： DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图。 (8) Http响应：视图负责将结果显示到客户端。 3.SpringMVC接口解释（1）DispatcherServlet接口： Spring提供的前端控制器，所有的请求都有经过它来统一分发。在DispatcherServlet将请求分发给Spring Controller之前，需要借助于Spring提供的HandlerMapping定位到具体的Controller。 （2）HandlerMapping接口： 能够完成客户请求到Controller映射。 （3）Controller接口： 需要为并发用户处理上述请求，因此实现Controller接口时，必须保证线程安全并且可重用。 Controller将处理用户请求，这和Struts Action扮演的角色是一致的。一旦Controller处理完用户请求，则返回ModelAndView对象给DispatcherServlet前端控制器，ModelAndView中包含了模型（Model）和视图（View）。 从宏观角度考虑，DispatcherServlet是整个Web应用的控制器；从微观考虑，Controller是单个Http请求处理过程中的控制器，而ModelAndView是Http请求过程中返回的模型（Model）和视图（View）。 （4）ViewResolver接口： Spring提供的视图解析器（ViewResolver）在Web应用中查找View对象，从而将相应结果渲染给客户。4.DispatcherServlet： 是整个Spring MVC的核心。它负责接收HTTP请求组织协调Spring MVC的各个组成部分。其主要工作有以下三项： （1）截获符合特定格式的URL请求。 （2）初始化DispatcherServlet上下文对应WebApplicationContext，并将其与业务层、持久化层的WebApplicationContext建立关联。 （3）初始化Spring MVC的各个组成组件，并装配到DispatcherServlet中。","categories":[],"tags":[{"name":"框架相关","slug":"框架相关","permalink":"http://yoursite.com/tags/框架相关/"}]},{"title":"Spring特点总结","slug":"spring特点总结","date":"2016-11-20T09:37:31.000Z","updated":"2017-08-25T10:49:31.000Z","comments":true,"path":"2016/11/20/spring特点总结/","link":"","permalink":"http://yoursite.com/2016/11/20/spring特点总结/","excerpt":"","text":"什么是IoC？ 我们先来看一下比较官方的解释。 IoC，Inversion of Control的缩写，中文名称为控制反转，意思是将对象的控制权转移至第三方，例如IoC容器，即可由IoC容器来管理对象的生命周期、依赖关系等。 回到我们所说的IoC，首先我们需要肯定的是IoC并不是特指某种技术，而是指一种思想或者说一种设计模式。我们可以简单的理解为我们在进行程序业务逻辑的编程时通常需要大量的对象来协作完成，而这些对象都需要我们通过类似如下语句 Object object=new Object();//对象申请 object.setName(“XXX”);//对象属性初始化赋值的方式申请和初始化，而这些就是所谓的对象的控制权，IoC设计模式的目的就是把这些对象的控制权转移至第三方，由第三方来进行和管理类似对象申请、初始化、销毁对象的控制权工作。 对于开发者来说，对象的控制权的转移意味着我们编程将更加简便，不用再去关心如何申请、初始化对象，甚至是管理对象、销毁等复杂的过程，这些都将由第三方完成，只需要告诉第三方我需要怎样的对象使用即可。 这里还需要解释一个概念，所谓的IoC容器，就是实现了IoC设计模式的框架。 Spring IoC实现了IoC设计模式，所以是IoC容器。所以，Spring IoC主要任务就是创建并且管理JavaBean的生命周期，即之前提到的对象的控制权。 那么对于Spring而言，JavaBean的生命周期包括哪些方面呢？这是我们下一个需要了解的问题。 Spring IoC的JavaBean的生命周期 （1）实例化JavaBean：Spring IoC容器实例化JavaBean （2）初始化JavaBean：Spring IoC容器对JavaBean通过注入依赖进行初始化 （3）使用JavaBean：基于Spring应用对JavaBean实例的使用 （4）销毁JavaBean：Spring IoC容器销毁JavaBean实例 什么是AOP？官方解释 我们先来看一下比较官方的解释。 AOP，Aspect Oriented Programming的缩写，意为面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。回到代码层面，关于AOP我在网上博文中发现了这样一句话：运行时，动态地将代码切入到类的指定方法或者指定位置。是不是豁然开朗了呢？ 这里还想强调一下运行时和动态两个点，运行时不难理解，如果在程序非运行时我们只要将代码写入指定位置再运行就好了，而程序运行时，代码就无法进行更改了，这也体现了不修改源代码的意义；动态这个概念我们可以这样理解，切入的过程并非是事先完成好的，而是在程序运行过程中触发了某个时机而进行的。 此处介绍几个概念，便于读者理解： 通知（advice）：切入到类指定方法或者指定位置的代码片段，即需要增加的功能代码，也就是上述的那片火腿（临时会议）。连接点（join point）：程序运行过程中能够进行插入切面操作的时间点。例如方法调用、异常抛出或字段修改等，可以理解为上述的长面包（老板的整个日程安排）。切入点（pointcut）：描述一个通知将被切入的一系列连接点的集合，即代码片段具体切入到哪些类、哪些方法，也就是上述面包切口处（特指午后的日程）。所以说，切入点规定了哪些连接点可以执行哪些通知。切面（aspect）：AOP中的切面等同于OOP中的类（class），由通知（advice）和切入点（pointcut）组成，其中通知（advice）和切入点（pointcut）既可以是1对1的关系，也可以是1对多的关系。概括的说就是描述了何时何地干何事的基本单元，其中通知（advice）说明了切面干何事，而切入点则说明了切面何时何地切入。关于几者的关系，我们可以这样理解，通知是在连接点上执行的，但是我们不希望通知应用到所有的连接点，所以引入了切入点来匹配特定的连接点，指名我们所希望通知应用的连接点。因此，所谓的AOP（面向切面编程）就是在程序运行过程中的某个时机将代码片段插入到某些类的指定方法和指定位置；换句话说，秘书接到临时会议通知时，将临时会议插入到老板的日程安排中去。 为什么要使用AOP？ 程序的最终目的就是实现业务，但是我们在进行编程的过程中经常会发现除了所谓的业务代码，还存在数量相当的公共代码，类似日志、安全验证、事物、异常处理等问题。这部分代码重要但是与我们编写程序要实现的功能没有关系，具有功能相似、重用性高、使用场景分散等特点。我们姑且称它们为共性问题。 对大多数程序而言，代码都是以纵向结构将各个业务模块串联从而完成功能的。我们提到的共性问题本身不属于业务范围，但是又散落在各个业务模块间，同实现主功能的代码相互杂糅在一起，即如下图所示： AOP程序逻辑图 试想一下，如果将共性问题部分的代码融入业务代码中，一旦涉及到对某个共性问题部分的代码进行更改的时候，例如日志部分发生需求变更，我们可能需要牵涉许许多多其他模块代码。这在小规模程序中也许是可以接受的，可能只修改1、2处；但是如果牵涉的地方数量过多，特别是应用在中大型规模程序中，我们甚至会为了小小的一个功能，修改上千、上万处。这样的方式是十分糟糕的，不仅费时费力，可能还会引起一些不必要的麻烦（回归错误、结构混乱等等）。 AOP的就是为了解决这类共性问题，将散落在程序中的公共部分提取出来，以切面的形式切入业务逻辑中，使程序员只专注于业务的开发，从事务提交等与业务无关的问题中解脱出来。AOP的好处解耦：AOP将程序中的共性问题进行了剥离，毫无疑问地降低了各个业务模块和共性问题之间的耦合。重用性：共性问题散落于业务逻辑的各处，十分难维护，使用AOP进行提取后，能够将相似功能的共性问题收敛，减少重复代码，提高了代码的重用性。拓展性：对于一个程序而言，迭代的重心一定在于业务和功能上。AOP使得每当发生变更时，可以只关注业务逻辑相关的代码，而减少共性问题上带来的变化，大大降低了程序未来拓展的成本。试想一下，如果将共性问题部分的代码融入业务代码中，一旦涉及到对某个共性问题部分的代码进行更改的时候，例如日志部分发生需求变更，我们可能需要牵涉许许多多其他模块代码。这在小规模程序中也许是可以接受的，可能只修改1、2处；但是如果牵涉的地方数量过多，特别是应用在中大型规模程序中，我们甚至会为了小小的一个功能，修改上千、上万处。这样的方式是十分糟糕的，不仅费时费力，可能还会引起一些不必要的麻烦（回归错误、结构混乱等等）。 AOP的就是为了解决这类共性问题，将散落在程序中的公共部分提取出来，以切面的形式切入业务逻辑中，使程序员只专注于业务的开发，从事务提交等与业务无关的问题中解脱出来。","categories":[],"tags":[{"name":"框架相关","slug":"框架相关","permalink":"http://yoursite.com/tags/框架相关/"}]},{"title":"java.util.concurrent指南","slug":"java.util.concurrent指南","date":"2016-11-17T09:37:31.000Z","updated":"2017-08-25T10:49:09.000Z","comments":true,"path":"2016/11/17/java.util.concurrent指南/","link":"","permalink":"http://yoursite.com/2016/11/17/java.util.concurrent指南/","excerpt":"这篇翻译指南很能解决问题，对于初步建立并发包的认识很有帮助，感谢原作者和翻译者","text":"这篇翻译指南很能解决问题，对于初步建立并发包的认识很有帮助，感谢原作者和翻译者Java 并发工具包 java.util.concurrent 用户指南 本指南根据 Jakob Jenkov 最新博客翻译，请随时关注博客更新：http://tutorials.jenkov.com/java-util-concurrent/index.html。本指南已做成中英文对照阅读版的 pdf 文档，有兴趣的朋友可以去 Java并发工具包java.util.concurrent用户指南中英文对照阅读版.pdf[带书签] 进行下载。 转载地址：http://blog.csdn.net/defonds/article/details/44021605 java.util.concurrent - Java 并发工具包 Java 5 添加了一个新的包到 Java 平台，java.util.concurrent 包。这个包包含有一系列能够让 Java 的并发编程变得更加简单轻松的类。在这个包被添加以前，你需要自己去动手实现自己的相关工具类。本文我将带你一一认识 java.util.concurrent 包里的这些类，然后你可以尝试着如何在项目中使用它们。本文中我将使用 Java 6 版本，我不确定这和 Java 5 版本里的是否有一些差异。我不会去解释关于 Java 并发的核心问题 - 其背后的原理，也就是说，如果你对那些东西感兴趣，参考《Java 并发指南》。半成品 本文很大程度上还是个 “半成品”，所以当你发现一些被漏掉的类或接口时，请耐心等待。在我空闲的时候会把它们加进来的。 2. 阻塞队列 BlockingQueuejava.util.concurrent 包里的 BlockingQueue 接口表示一个线程安放入和提取实例的队列。本小节我将给你演示如何使用这个 BlockingQueue。本节不会讨论如何在 Java 中实现一个你自己的 BlockingQueue。如果你对那个感兴趣，参考《Java 并发指南》BlockingQueue 用法 BlockingQueue 通常用于一个线程生产对象，而另外一个线程消费这些对象的场景。下图是对这个原理的阐述： 一个线程往里边放，另外一个线程从里边取的一个 BlockingQueue。一个线程将会持续生产新对象并将其插入到队列之中，直到队列达到它所能容纳的临界点。也就是说，它是有限的。如果该阻塞队列到达了其临界点，负责生产的线程将会在往里边插入新对象时发生阻塞。它会一直处于阻塞之中，直到负责消费的线程从队列中拿走一个对象。负责消费的线程将会一直从该阻塞队列中拿出对象。如果消费线程尝试去从一个空的队列中提取对象的话，这个消费线程将会处于阻塞之中，直到一个生产线程把一个对象丢进队列。BlockingQueue 的方法 BlockingQueue 具有 4 组不同的方法用于插入、移除以及对队列中的元素进行检查。如果请求的操作不能得到立即执行的话，每个方法的表现也不同。这些方法如下： 四组不同的行为方式解释： 抛异常：如果试图的操作无法立即执行，抛一个异常。 特定值：如果试图的操作无法立即执行，返回一个特定的值(常常是 true / false)。 阻塞：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行。 超时：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行，但等待时间不会超过给定值。返回一个特定值以告知该操作是否成功(典型的是 true / false)。无法向一个 BlockingQueue 中插入 null。如果你试图插入 null，BlockingQueue 将会抛出一个 NullPointerException。 可以访问到 BlockingQueue 中的所有元素，而不仅仅是开始和结束的元素。比如说，你将一个对象放入队列之中以等待处理，但你的应用想要将其取消掉。那么你可以调用诸如 remove(o) 方法来将队列之中的特定对象进行移除。但是这么干效率并不高(译者注：基于队列的数据结构，获取除开始或结束位置的其他对象的效率不会太高)，因此你尽量不要用这一类的方法，除非你确实不得不那么做。BlockingQueue 的实现 BlockingQueue 是个接口，你需要使用它的实现之一来使用 BlockingQueue。java.util.concurrent 具有以下 BlockingQueue 接口的实现(Java 6)： ArrayBlockingQueue DelayQueue LinkedBlockingQueue PriorityBlockingQueue SynchronousQueue Java 中使用 BlockingQueue 的例子 这里是一个 Java 中使用 BlockingQueue 的示例。本示例使用的是 BlockingQueue 接口的 ArrayBlockingQueue 实现。首先，BlockingQueueExample 类分别在两个独立的线程中启动了一个 Producer 和 一个 Consumer。Producer 向一个共享的 BlockingQueue 中注入字符串，而 Consumer 则会从中把它们拿出来。 [java] view plain copy print? 1. public class BlockingQueueExample { 2. 3. public static void main(String[] args) throws Exception { 4. 5. BlockingQueue queue = new ArrayBlockingQueue(1024); 6. 7. Producer producer = new Producer(queue); 8. Consumer consumer = new Consumer(queue); 9. 10. new Thread(producer).start(); 11. new Thread(consumer).start(); 12. 13. Thread.sleep(4000); 14. } 15. } 以下是 Producer 类。注意它在每次 put() 调用时是如何休眠一秒钟的。这将导致 Consumer 在等待队列中对象的时候发生阻塞。 [java] view plain copy print? 1. public class Producer implements Runnable{ 2. 3. protected BlockingQueue queue = null; 4. 5. public Producer(BlockingQueue queue) { 6. this.queue = queue; 7. } 8. 9. public void run() { 10. try { 11. queue.put(&quot;1&quot;); 12. Thread.sleep(1000); 13. queue.put(&quot;2&quot;); 14. Thread.sleep(1000); 15. queue.put(&quot;3&quot;); 16. } catch (InterruptedException e) { 17. e.printStackTrace(); 18. } 19. } 20. } 以下是 Consumer 类。它只是把对象从队列中抽取出来，然后将它们打印到 System.out。 [java] view plain copy print? 1. public class Consumer implements Runnable{ 2. 3. protected BlockingQueue queue = null; 4. 5. public Consumer(BlockingQueue queue) { 6. this.queue = queue; 7. } 8. 9. public void run() { 10. try { 11. System.out.println(queue.take()); 12. System.out.println(queue.take()); 13. System.out.println(queue.take()); 14. } catch (InterruptedException e) { 15. e.printStackTrace(); 16. } 17. } 18. } 3. 数组阻塞队列 ArrayBlockingQueueArrayBlockingQueue 类实现了 BlockingQueue 接口。 ArrayBlockingQueue 是一个有界的阻塞队列，其内部实现是将对象放到一个数组里。有界也就意味着，它不能够存储无限多数量的元素。它有一个同一时间能够存储元素数量的上限。你可以在对其初始化的时候设定这个上限，但之后就无法对这个上限进行修改了(译者注：因为它是基于数组实现的，也就具有数组的特性：一旦初始化，大小就无法修改)。 ArrayBlockingQueue 内部以 FIFO(先进先出)的顺序对元素进行存储。队列中的头元素在所有元素之中是放入时间最久的那个，而尾元素则是最短的那个。 以下是在使用 ArrayBlockingQueue 的时候对其初始化的一个示例： [java] view plain copy print? 1. BlockingQueue queue = new ArrayBlockingQueue(1024); 2. 3. queue.put(&quot;1&quot;); 4. 5. Object object = queue.take(); 以下是使用了 Java 泛型的一个 BlockingQueue 示例。注意其中是如何对 String 元素放入和提取的： [java] view plain copy print? 1. BlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(1024); 2. 3. queue.put(&quot;1&quot;); 4. 5. String string = queue.take(); 4. 延迟队列 DelayQueueDelayQueue 实现了 BlockingQueue 接口。 DelayQueue 对元素进行持有直到一个特定的延迟到期。注入其中的元素必须实现 java.util.concurrent.Delayed 接口，该接口定义： [java] view plain copy print? 1. public interface Delayed extends Comparable&lt;Delayed&lt; { 2. 3. public long getDelay(TimeUnit timeUnit); 4. 5. } DelayQueue 将会在每个元素的 getDelay() 方法返回的值的时间段之后才释放掉该元素。如果返回的是 0 或者负值，延迟将被认为过期，该元素将会在 DelayQueue 的下一次 take 被调用的时候被释放掉。 传递给 getDelay 方法的 getDelay 实例是一个枚举类型，它表明了将要延迟的时间段。TimeUnit 枚举将会取以下值： [java] view plain copy print? 1. DAYS 2. HOURS 3. MINUTES 4. SECONDS 5. MILLISECONDS 6. MICROSECONDS 7. NANOSECONDS 正如你所看到的，Delayed 接口也继承了 java.lang.Comparable 接口，这也就意味着 Delayed 对象之间可以进行对比。这个可能在对 DelayQueue 队列中的元素进行排序时有用，因此它们可以根据过期时间进行有序释放。 以下是使用 DelayQueue 的例子： [java] view plain copy print? 1. public class DelayQueueExample { 2. 3. public static void main(String[] args) { 4. DelayQueue queue = new DelayQueue(); 5. 6. Delayed element1 = new DelayedElement(); 7. 8. queue.put(element1); 9. 10. Delayed element2 = queue.take(); 11. } 12. } DelayedElement 是我所创建的一个 DelayedElement 接口的实现类，它不在 Java.util.concurrent 包里。你需要自行创建你自己的 Delayed 接口的实现以使用 DelayQueue 类。 5. 链阻塞队列 LinkedBlockingQueueLinkedBlockingQueue 类实现了 BlockingQueue 接口。 LinkedBlockingQueue 内部以一个链式结构(链接节点)对其元素进行存储。如果需要的话，这一链式结构可以选择一个上限。如果没有定义上限，将使用 Integer.MAX_VALUE 作为上限。 LinkedBlockingQueue 内部以 FIFO(先进先出)的顺序对元素进行存储。队列中的头元素在所有元素之中是放入时间最久的那个，而尾元素则是最短的那个。 以下是 LinkedBlockingQueue 的初始化和使用示例代码： [java] view plain copy print? 1. BlockingQueue&lt;String&gt; unbounded = new LinkedBlockingQueue&lt;String&gt;(); 2. BlockingQueue&lt;String&gt; bounded = new LinkedBlockingQueue&lt;String&gt;(1024); 3. 4. bounded.put(&quot;Value&quot;); 5. 6. String value = bounded.take(); 6. 具有优先级的阻塞队列 PriorityBlockingQueuePriorityBlockingQueue 类实现了 BlockingQueue 接口。 PriorityBlockingQueue 是一个无界的并发队列。它使用了和类 java.util.PriorityQueue 一样的排序规则。你无法向这个队列中插入 null 值。 所有插入到 PriorityBlockingQueue 的元素必须实现 java.lang.Comparable 接口。因此该队列中元素的排序就取决于你自己的 Comparable 实现。 注意 PriorityBlockingQueue 对于具有相等优先级(compare() == 0)的元素并不强制任何特定行为。 同时注意，如果你从一个 PriorityBlockingQueue 获得一个 Iterator 的话，该 Iterator 并不能保证它对元素的遍历是以优先级为序的。以下是使用 PriorityBlockingQueue 的示例： [java] view plain copy print? 1. BlockingQueue queue = new PriorityBlockingQueue(); 2. 3. //String implements java.lang.Comparable 4. queue.put(&quot;Value&quot;); 5. 6. String value = queue.take(); 7. 同步队列 SynchronousQueueSynchronousQueue 类实现了 BlockingQueue 接口。SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素。如果该队列已有一元素的话，试图向队列中插入一个新元素的线程将会阻塞，直到另一个线程将该元素从队列中抽走。同样，如果该队列为空，试图向队列中抽取一个元素的线程将会阻塞，直到另一个线程向队列中插入了一条新的元素。据此，把这个类称作一个队列显然是夸大其词了。它更多像是一个汇合点。 8. 阻塞双端队列 BlockingDequejava.util.concurrent 包里的 BlockingDeque 接口表示一个线程安放入和提取实例的双端队列。本小节我将给你演示如何使用 BlockingDeque。 BlockingDeque 类是一个双端队列，在不能够插入元素时，它将阻塞住试图插入元素的线程；在不能够抽取元素时，它将阻塞住试图抽取的线程。 deque(双端队列) 是 “Double Ended Queue” 的缩写。因此，双端队列是一个你可以从任意一端插入或者抽取元素的队列。 BlockingDeque 的使用 在线程既是一个队列的生产者又是这个队列的消费者的时候可以使用到 BlockingDeque。如果生产者线程需要在队列的两端都可以插入数据，消费者线程需要在队列的两端都可以移除数据，这个时候也可以使用 BlockingDeque。BlockingDeque 图解： 一个 BlockingDeque - 线程在双端队列的两端都可以插入和提取元素。 一个线程生产元素，并把它们插入到队列的任意一端。如果双端队列已满，插入线程将被阻塞，直到一个移除线程从该队列中移出了一个元素。如果双端队列为空，移除线程将被阻塞，直到一个插入线程向该队列插入了一个新元素。BlockingDeque 的方法 BlockingDeque 具有 4 组不同的方法用于插入、移除以及对双端队列中的元素进行检查。如果请求的操作不能得到立即执行的话，每个方法的表现也不同。这些方法如下： 四组不同的行为方式解释： 抛异常：如果试图的操作无法立即执行，抛一个异常。 特定值：如果试图的操作无法立即执行，返回一个特定的值(常常是 true / false)。 阻塞：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行。 超时：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行，但等待时间不会超过给定值。返回一个特定值以告知该操作是否成功(典型的是 true / false)。 BlockingDeque 继承自 BlockingQueue BlockingDeque 接口继承自 BlockingQueue 接口。这就意味着你可以像使用一个 BlockingQueue 那样使用 BlockingDeque。如果你这么干的话，各种插入方法将会把新元素添加到双端队列的尾端，而移除方法将会把双端队列的首端的元素移除。正如 BlockingQueue 接口的插入和移除方法一样。 以下是 BlockingDeque 对 BlockingQueue 接口的方法的具体内部实现： BlockingQueue BlockingDeque add() addLast() offer() x 2 offerLast() x 2 put() putLast() remove() removeFirst() poll() x 2 pollFirst() take() takeFirst() element() getFirst() peek() peekFirst() BlockingDeque 的实现 既然 BlockingDeque 是一个接口，那么你想要使用它的话就得使用它的众多的实现类的其中一个。java.util.concurrent 包提供了以下 BlockingDeque 接口的实现类： LinkedBlockingDequeBlockingDeque 代码示例 以下是如何使用 BlockingDeque 方法的一个简短代码示例： [java] view plain copy print? 1. BlockingDeque&lt;String&gt; deque = new LinkedBlockingDeque&lt;String&gt;(); 2. 3. deque.addFirst(&quot;1&quot;); 4. deque.addLast(&quot;2&quot;); 5. 6. String two = deque.takeLast(); 7. String one = deque.takeFirst(); 9. 链阻塞双端队列 LinkedBlockingDequeLinkedBlockingDeque 类实现了 BlockingDeque 接口。 deque(双端队列) 是 “Double Ended Queue” 的缩写。因此，双端队列是一个你可以从任意一端插入或者抽取元素的队列。(译者注：唐僧啊，受不了。) LinkedBlockingDeque 是一个双端队列，在它为空的时候，一个试图从中抽取数据的线程将会阻塞，无论该线程是试图从哪一端抽取数据。 以下是 LinkedBlockingDeque 实例化以及使用的示例： [java] view plain copy print? 1. BlockingDeque&lt;String&gt; deque = new LinkedBlockingDeque&lt;String&gt;(); 2. 3. deque.addFirst(&quot;1&quot;); 4. deque.addLast(&quot;2&quot;); 5. 6. String two = deque.takeLast(); 7. String one = deque.takeFirst(); 10. 并发 Map(映射) ConcurrentMapjava.util.concurrent.ConcurrentMap java.util.concurrent.ConcurrentMap 接口表示了一个能够对别人的访问(插入和提取)进行并发处理的 java.util.Map。 ConcurrentMap 除了从其父接口 java.util.Map 继承来的方法之外还有一些额外的原子性方法。ConcurrentMap 的实现 既然 ConcurrentMap 是个接口，你想要使用它的话就得使用它的实现类之一。java.util.concurrent 包具备 ConcurrentMap 接口的以下实现类： ConcurrentHashMap ConcurrentHashMap 和 java.util.HashTable 类很相似，但 ConcurrentHashMap 能够提供比 HashTable 更好的并发性能。在你从中读取对象的时候 ConcurrentHashMap 并不会把整个 Map 锁住。此外，在你向其中写入对象的时候，ConcurrentHashMap 也不会锁住整个 Map。它的内部只是把 Map 中正在被写入的部分进行锁定。 另外一个不同点是，在被遍历的时候，即使是 ConcurrentHashMap 被改动，它也不会抛 ConcurrentModificationException。尽管 Iterator 的设计不是为多个线程的同时使用。更多关于 ConcurrentMap 和 ConcurrentHashMap 的细节请参考官方文档。 ConcurrentMap 例子 以下是如何使用 ConcurrentMap 接口的一个例子。本示例使用了 ConcurrentHashMap 实现类： [java] view plain copy print? 1. ConcurrentMap concurrentMap = new ConcurrentHashMap(); 2. 3. concurrentMap.put(&quot;key&quot;, &quot;value&quot;); 4. 5. Object value = concurrentMap.get(&quot;key&quot;); 11. 并发导航映射 ConcurrentNavigableMapjava.util.concurrent.ConcurrentNavigableMap 是一个支持并发访问的 java.util.NavigableMap，它还能让它的子 map 具备并发访问的能力。所谓的 “子 map” 指的是诸如 headMap()，subMap()，tailMap() 之类的方法返回的 map。 NavigableMap 中的方法不再赘述，本小节我们来看一下 ConcurrentNavigableMap 添加的方法。 headMap() headMap(T toKey) 方法返回一个包含了小于给定 toKey 的 key 的子 map。 如果你对原始 map 里的元素做了改动，这些改动将影响到子 map 中的元素(译者注：map 集合持有的其实只是对象的引用)。 以下示例演示了对 headMap() 方法的使用： [java] view plain copy print? 1. ConcurrentNavigableMap map = new ConcurrentSkipListMap(); 2. 3. map.put(&quot;1&quot;, &quot;one&quot;); 4. map.put(&quot;2&quot;, &quot;two&quot;); 5. map.put(&quot;3&quot;, &quot;three&quot;); 6. 7. ConcurrentNavigableMap headMap = map.headMap(&quot;2&quot;); headMap 将指向一个只含有键 “1” 的 ConcurrentNavigableMap，因为只有这一个键小于 “2”。关于这个方法及其重载版本具体是怎么工作的细节请参考 Java 文档。 tailMap() tailMap(T fromKey) 方法返回一个包含了不小于给定 fromKey 的 key 的子 map。 如果你对原始 map 里的元素做了改动，这些改动将影响到子 map 中的元素(译者注：map 集合持有的其实只是对象的引用)。 以下示例演示了对 tailMap() 方法的使用： [java] view plain copy print? 1. ConcurrentNavigableMap map = new ConcurrentSkipListMap(); 2. 3. map.put(&quot;1&quot;, &quot;one&quot;); 4. map.put(&quot;2&quot;, &quot;two&quot;); 5. map.put(&quot;3&quot;, &quot;three&quot;); 6. 7. ConcurrentNavigableMap tailMap = map.tailMap(&quot;2&quot;); tailMap 将拥有键 “2” 和 “3”，因为它们不小于给定键 “2”。关于这个方法及其重载版本具体是怎么工作的细节请参考 Java 文档。 subMap() subMap() 方法返回原始 map 中，键介于 from(包含) 和 to (不包含) 之间的子 map。示例如下： [java] view plain copy print? 1. ConcurrentNavigableMap map = new ConcurrentSkipListMap(); 2. 3. map.put(&quot;1&quot;, &quot;one&quot;); 4. map.put(&quot;2&quot;, &quot;two&quot;); 5. map.put(&quot;3&quot;, &quot;three&quot;); 6. 7. ConcurrentNavigableMap subMap = map.subMap(&quot;2&quot;, &quot;3&quot;); 返回的 submap 只包含键 “2”，因为只有它满足不小于 “2”，比 “3” 小。更多方法 ConcurrentNavigableMap 接口还有其他一些方法可供使用，比如： descendingKeySet() descendingMap() navigableKeySet()关于这些方法更多信息参考官方 Java 文档。 ##12. 闭锁 CountDownLatch java.util.concurrent.CountDownLatch 是一个并发构造，它允许一个或多个线程等待一系列指定操作的完成。 CountDownLatch 以一个给定的数量初始化。countDown() 每被调用一次，这一数量就减一。通过调用 await() 方法之一，线程可以阻塞等待这一数量到达零。 以下是一个简单示例。Decrementer 三次调用 countDown() 之后，等待中的 Waiter 才会从 await() 调用中释放出来。 [java] view plain copy print? 1. CountDownLatch latch = new CountDownLatch(3); 2. 3. Waiter waiter = new Waiter(latch); 4. Decrementer decrementer = new Decrementer(latch); 5. 6. new Thread(waiter) .start(); 7. new Thread(decrementer).start(); 8. 9. Thread.sleep(4000); 10. 11. public class Waiter implements Runnable{ 12. 13. CountDownLatch latch = null; 14. 15. public Waiter(CountDownLatch latch) { 16. this.latch = latch; 17. } 18. 19. public void run() { 20. try { 21. latch.await(); 22. } catch (InterruptedException e) { 23. e.printStackTrace(); 24. } 25. 26. System.out.println(&quot;Waiter Released&quot;); 27. } 28. } 29. 30. public class Decrementer implements Runnable { 31. 32. CountDownLatch latch = null; 33. 34. public Decrementer(CountDownLatch latch) { 35. this.latch = latch; 36. } 37. 38. public void run() { 39. 40. try { 41. Thread.sleep(1000); 42. this.latch.countDown(); 43. 44. Thread.sleep(1000); 45. this.latch.countDown(); 46. 47. Thread.sleep(1000); 48. this.latch.countDown(); 49. } catch (InterruptedException e) { 50. e.printStackTrace(); 51. } 52. } 53. } 13. 栅栏 CyclicBarrierjava.util.concurrent.CyclicBarrier 类是一种同步机制，它能够对处理一些算法的线程实现同步。换句话讲，它就是一个所有线程必须等待的一个栅栏，直到所有线程都到达这里，然后所有线程才可以继续做其他事情。图示如下： 两个线程在栅栏旁等待对方。 通过调用 CyclicBarrier 对象的 await() 方法，两个线程可以实现互相等待。一旦 N 个线程在等待 CyclicBarrier 达成，所有线程将被释放掉去继续运行。 创建一个 CyclicBarrier 在创建一个 CyclicBarrier 的时候你需要定义有多少线程在被释放之前等待栅栏。创建 CyclicBarrier 示例： [java] view plain copy print? 1. CyclicBarrier barrier = new CyclicBarrier(2); 等待一个 CyclicBarrier 以下演示了如何让一个线程等待一个 CyclicBarrier： [java] view plain copy print? 1. barrier.await(); 当然，你也可以为等待线程设定一个超时时间。等待超过了超时时间之后，即便还没有达成 N 个线程等待 CyclicBarrier 的条件，该线程也会被释放出来。以下是定义超时时间示例： [java] view plain copy print? 1. barrier.await(10, TimeUnit.SECONDS); 满足以下任何条件都可以让等待 CyclicBarrier 的线程释放： 最后一个线程也到达 CyclicBarrier(调用 await()) 当前线程被其他线程打断(其他线程调用了这个线程的 interrupt() 方法) 其他等待栅栏的线程被打断 其他等待栅栏的线程因超时而被释放 外部线程调用了栅栏的 CyclicBarrier.reset() 方法 CyclicBarrier 行动 CyclicBarrier 支持一个栅栏行动，栅栏行动是一个 Runnable 实例，一旦最后等待栅栏的线程抵达，该实例将被执行。你可以在 CyclicBarrier 的构造方法中将 Runnable 栅栏行动传给它： [java] view plain copy print? 1. Runnable barrierAction = ... ; 2. CyclicBarrier barrier = new CyclicBarrier(2, barrierAction); CyclicBarrier 示例 以下代码演示了如何使用 CyclicBarrier： [java] view plain copy print? 1. Runnable barrier1Action = new Runnable() { 2. public void run() { 3. System.out.println(&quot;BarrierAction 1 executed &quot;); 4. } 5. }; 6. Runnable barrier2Action = new Runnable() { 7. public void run() { 8. System.out.println(&quot;BarrierAction 2 executed &quot;); 9. } 10. }; 11. 12. CyclicBarrier barrier1 = new CyclicBarrier(2, barrier1Action); 13. CyclicBarrier barrier2 = new CyclicBarrier(2, barrier2Action); 14. 15. CyclicBarrierRunnable barrierRunnable1 = 16. new CyclicBarrierRunnable(barrier1, barrier2); 17. 18. CyclicBarrierRunnable barrierRunnable2 = 19. new CyclicBarrierRunnable(barrier1, barrier2); 20. 21. new Thread(barrierRunnable1).start(); 22. new Thread(barrierRunnable2).start(); CyclicBarrierRunnable 类： [java] view plain copy print? 1. public class CyclicBarrierRunnable implements Runnable{ 2. 3. CyclicBarrier barrier1 = null; 4. CyclicBarrier barrier2 = null; 5. 6. public CyclicBarrierRunnable( 7. CyclicBarrier barrier1, 8. CyclicBarrier barrier2) { 9. 10. this.barrier1 = barrier1; 11. this.barrier2 = barrier2; 12. } 13. 14. public void run() { 15. try { 16. Thread.sleep(1000); 17. System.out.println(Thread.currentThread().getName() + 18. &quot; waiting at barrier 1&quot;); 19. this.barrier1.await(); 20. 21. Thread.sleep(1000); 22. System.out.println(Thread.currentThread().getName() + 23. &quot; waiting at barrier 2&quot;); 24. this.barrier2.await(); 25. 26. System.out.println(Thread.currentThread().getName() + 27. &quot; done!&quot;); 28. 29. } catch (InterruptedException e) { 30. e.printStackTrace(); 31. } catch (BrokenBarrierException e) { 32. e.printStackTrace(); 33. } 34. } 35. } 以上代码控制台输出如下。注意每个线程写入控制台的时序可能会跟你实际执行不一样。比如有时Thread-0 先打印，有时 Thread-1 先打印。 Thread-0 waiting at barrier 1 Thread-1 waiting at barrier 1 BarrierAction 1 executed Thread-1 waiting at barrier 2 Thread-0 waiting at barrier 2 BarrierAction 2 executed Thread-0 done! Thread-1 done! 14. 交换机 Exchangerjava.util.concurrent.Exchanger 类表示一种两个线程可以进行互相交换对象的会和点。这种机制图示如下： 两个线程通过一个 Exchanger 交换对象。交换对象的动作由 Exchanger 的两个 exchange() 方法的其中一个完成。以下是一个示例： [java] view plain copy print? 1. Exchanger exchanger = new Exchanger(); 2. 3. ExchangerRunnable exchangerRunnable1 = 4. new ExchangerRunnable(exchanger, &quot;A&quot;); 5. 6. ExchangerRunnable exchangerRunnable2 = 7. new ExchangerRunnable(exchanger, &quot;B&quot;); 8. 9. new Thread(exchangerRunnable1).start(); 10. new Thread(exchangerRunnable2).start(); ExchangerRunnable 代码： [java] view plain copy print? 1. public class ExchangerRunnable implements Runnable{ 2. 3. Exchanger exchanger = null; 4. Object object = null; 5. 6. public ExchangerRunnable(Exchanger exchanger, Object object) { 7. this.exchanger = exchanger; 8. this.object = object; 9. } 10. 11. public void run() { 12. try { 13. Object previous = this.object; 14. 15. this.object = this.exchanger.exchange(this.object); 16. 17. System.out.println( 18. Thread.currentThread().getName() + 19. &quot; exchanged &quot; + previous + &quot; for &quot; + this.object 20. ); 21. } catch (InterruptedException e) { 22. e.printStackTrace(); 23. } 24. } 25. } 以上程序输出： Thread-0 exchanged A for B Thread-1 exchanged B for A 15. 信号量 Semaphorejava.util.concurrent.Semaphore 类是一个计数信号量。这就意味着它具备两个主要方法： acquire() release()计数信号量由一个指定数量的 “许可” 初始化。每调用一次 acquire()，一个许可会被调用线程取走。每调用一次 release()，一个许可会被返还给信号量。因此，在没有任何 release() 调用时，最多有 N 个线程能够通过 acquire() 方法，N 是该信号量初始化时的许可的指定数量。这些许可只是一个简单的计数器。这里没啥奇特的地方。Semaphore 用法 信号量主要有两种用途： 保护一个重要(代码)部分防止一次超过 N 个线程进入。 在两个线程之间发送信号。保护重要部分 如果你将信号量用于保护一个重要部分，试图进入这一部分的代码通常会首先尝试获得一个许可，然后才能进入重要部分(代码块)，执行完之后，再把许可释放掉。比如这样： [java] view plain copy print? 1. Semaphore semaphore = new Semaphore(1); 2. 3. //critical section 4. semaphore.acquire(); 5. 6. ... 7. 8. semaphore.release(); 在线程之间发送信号 如果你将一个信号量用于在两个线程之间传送信号，通常你应该用一个线程调用 acquire() 方法，而另一个线程调用 release() 方法。如果没有可用的许可，acquire() 调用将会阻塞，直到一个许可被另一个线程释放出来。同理，如果无法往信号量释放更多许可时，一个 release() 调用也会阻塞。通过这个可以对多个线程进行协调。比如，如果线程 1 将一个对象插入到了一个共享列表(list)之后之后调用了 acquire()，而线程 2 则在从该列表中获取一个对象之前调用了 release()，这时你其实已经创建了一个阻塞队列。信号量中可用的许可的数量也就等同于该阻塞队列能够持有的元素个数。公平 没有办法保证线程能够公平地可从信号量中获得许可。也就是说，无法担保掉第一个调用 acquire() 的线程会是第一个获得一个许可的线程。如果第一个线程在等待一个许可时发生阻塞，而第二个线程前来索要一个许可的时候刚好有一个许可被释放出来，那么它就可能会在第一个线程之前获得许可。如果你想要强制公平，Semaphore 类有一个具有一个布尔类型的参数的构造子，通过这个参数以告知 Semaphore 是否要强制公平。强制公平会影响到并发性能，所以除非你确实需要它否则不要启用它。以下是如何在公平模式创建一个 Semaphore 的示例： [java] view plain copy print? 1. Semaphore semaphore = new Semaphore(1, true); 更多方法 java.util.concurrent.Semaphore 类还有很多方法，比如： availablePermits() acquireUninterruptibly() drainPermits() hasQueuedThreads() getQueuedThreads() tryAcquire() 等等这些方法的细节请参考 Java 文档。 16. 执行器服务 ExecutorServicejava.util.concurrent.ExecutorService 接口表示一个异步执行机制，使我们能够在后台执行任务。因此一个 ExecutorService 很类似于一个线程池。实际上，存在于 java.util.concurrent 包里的 ExecutorService 实现就是一个线程池实现。ExecutorService 例子 以下是一个简单的 ExecutorService 例子： [java] view plain copy print? 1. ExecutorService executorService = Executors.newFixedThreadPool(10); 2. 3. executorService.execute(new Runnable() { 4. public void run() { 5. System.out.println(&quot;Asynchronous task&quot;); 6. } 7. }); 8. 9. executorService.shutdown(); 首先使用 newFixedThreadPool() 工厂方法创建一个 ExecutorService。这里创建了一个十个线程执行任务的线程池。然后，将一个 Runnable 接口的匿名实现类传递给 execute() 方法。这将导致 ExecutorService 中的某个线程执行该 Runnable。任务委派 下图说明了一个线程是如何将一个任务委托给一个 ExecutorService 去异步执行的： 一个线程将一个任务委派给一个 ExecutorService 去异步执行。一旦该线程将任务委派给 ExecutorService，该线程将继续它自己的执行，独立于该任务的执行。ExecutorService 实现 既然 ExecutorService 是个接口，如果你想用它的话就得去使用它的实现类之一。java.util.concurrent 包提供了 ExecutorService 接口的以下实现类： ThreadPoolExecutor ScheduledThreadPoolExecutor 创建一个 ExecutorService ExecutorService 的创建依赖于你使用的具体实现。但是你也可以使用 Executors 工厂类来创建 ExecutorService 实例。以下是几个创建 ExecutorService 实例的例子： [java] view plain copy print? 1. ExecutorService executorService1 = Executors.newSingleThreadExecutor(); 2. 3. ExecutorService executorService2 = Executors.newFixedThreadPool(10); 4. 5. ExecutorService executorService3 = Executors.newScheduledThreadPool(10); ExecutorService 使用 有几种不同的方式来将任务委托给 ExecutorService 去执行： execute(Runnable) submit(Runnable) submit(Callable) invokeAny(…) invokeAll(…) 接下来我们挨个看一下这些方法。 execute(Runnable) execute(Runnable) 方法要求一个 java.lang.Runnable 对象，然后对它进行异步执行。以下是使用 ExecutorService 执行一个 Runnable 的示例： [java] view plain copy print? 1. ExecutorService executorService = Executors.newSingleThreadExecutor(); 2. 3. executorService.execute(new Runnable() { 4. public void run() { 5. System.out.println(&quot;Asynchronous task&quot;); 6. } 7. }); 8. 9. executorService.shutdown(); 没有办法得知被执行的 Runnable 的执行结果。如果有需要的话你得使用一个 Callable(以下将做介绍)。 submit(Runnable) submit(Runnable) 方法也要求一个 Runnable 实现类，但它返回一个 Future 对象。这个 Future 对象可以用来检查 Runnable 是否已经执行完毕。以下是 ExecutorService submit() 示例： [java] view plain copy print? 1. Future future = executorService.submit(new Runnable() { 2. public void run() { 3. System.out.println(&quot;Asynchronous task&quot;); 4. } 5. }); 6. 7. future.get(); //returns null if the task has finished correctly. submit(Callable) submit(Callable) 方法类似于 submit(Runnable) 方法，除了它所要求的参数类型之外。Callable 实例除了它的 call() 方法能够返回一个结果之外和一个 Runnable 很相像。Runnable.run() 不能够返回一个结果。Callable 的结果可以通过 submit(Callable) 方法返回的 Future 对象进行获取。以下是一个 ExecutorService Callable 示例： [java] view plain copy print? 1. Future future = executorService.submit(new Callable(){ 2. public Object call() throws Exception { 3. System.out.println(&quot;Asynchronous Callable&quot;); 4. return &quot;Callable Result&quot;; 5. } 6. }); 7. 8. System.out.println(&quot;future.get() = &quot; + future.get()); 以上代码输出： Asynchronous Callable future.get() = Callable Result invokeAny() invokeAny() 方法要求一系列的 Callable 或者其子接口的实例对象。调用这个方法并不会返回一个 Future，但它返回其中一个 Callable 对象的结果。无法保证返回的是哪个 Callable 的结果 - 只能表明其中一个已执行结束。 如果其中一个任务执行结束(或者抛了一个异常)，其他 Callable 将被取消。以下是示例代码： [java] view plain copy print? 1. ExecutorService executorService = Executors.newSingleThreadExecutor(); 2. 3. Set&lt;Callable&lt;String&gt;&gt; callables = new HashSet&lt;Callable&lt;String&gt;&gt;(); 4. 5. callables.add(new Callable&lt;String&gt;() { 6. public String call() throws Exception { 7. return &quot;Task 1&quot;; 8. } 9. }); 10. callables.add(new Callable&lt;String&gt;() { 11. public String call() throws Exception { 12. return &quot;Task 2&quot;; 13. } 14. }); 15. callables.add(new Callable&lt;String&gt;() { 16. public String call() throws Exception { 17. return &quot;Task 3&quot;; 18. } 19. }); 20. 21. String result = executorService.invokeAny(callables); 22. 23. System.out.println(&quot;result = &quot; + result); 24. 25. executorService.shutdown(); 上述代码将会打印出给定 Callable 集合中的一个的执行结果。我自己试着执行了它几次，结果始终在变。有时是 “Task 1”，有时是 “Task 2” 等等。invokeAll() invokeAll() 方法将调用你在集合中传给 ExecutorService 的所有 Callable 对象。 invokeAll() 返回一系列的 Future 对象，通过它们你可以获取每个 Callable 的执行结果。记住，一个任务可能会由于一个异常而结束，因此它可能没有 “成功”。无法通过一个 Future 对象来告知我们是两种结束中的哪一种。以下是一个代码示例： [java] view plain copy print? 1. ExecutorService executorService = Executors.newSingleThreadExecutor(); 2. 3. Set&lt;Callable&lt;String&gt;&gt; callables = new HashSet&lt;Callable&lt;String&gt;&gt;(); 4. 5. callables.add(new Callable&lt;String&gt;() { 6. public String call() throws Exception { 7. return &quot;Task 1&quot;; 8. } 9. }); 10. callables.add(new Callable&lt;String&gt;() { 11. public String call() throws Exception { 12. return &quot;Task 2&quot;; 13. } 14. }); 15. callables.add(new Callable&lt;String&gt;() { 16. public String call() throws Exception { 17. return &quot;Task 3&quot;; 18. } 19. }); 20. 21. List&lt;Future&lt;String&gt;&gt; futures = executorService.invokeAll(callables); 22. 23. for(Future&lt;String&gt; future : futures){ 24. System.out.println(&quot;future.get = &quot; + future.get()); 25. } 26. 27. executorService.shutdown(); ExecutorService 关闭 使用完 ExecutorService 之后你应该将其关闭，以使其中的线程不再运行。比如，如果你的应用是通过一个 main() 方法启动的，之后 main 方法退出了你的应用，如果你的应用有一个活动的 ExexutorService 它将还会保持运行。ExecutorService 里的活动线程阻止了 JVM 的关闭。要终止 ExecutorService 里的线程你需要调用 ExecutorService 的 shutdown() 方法。ExecutorService 并不会立即关闭，但它将不再接受新的任务，而且一旦所有线程都完成了当前任务的时候，ExecutorService 将会关闭。在 shutdown() 被调用之前所有提交给 ExecutorService 的任务都被执行。如果你想要立即关闭 ExecutorService，你可以调用 shutdownNow() 方法。这样会立即尝试停止所有执行中的任务，并忽略掉那些已提交但尚未开始处理的任务。无法担保执行任务的正确执行。可能它们被停止了，也可能已经执行结束。 17. 线程池执行者 ThreadPoolExecutorjava.util.concurrent.ThreadPoolExecutor 是 ExecutorService 接口的一个实现。ThreadPoolExecutor 使用其内部池中的线程执行给定任务(Callable 或者 Runnable)。ThreadPoolExecutor 包含的线程池能够包含不同数量的线程。池中线程的数量由以下变量决定： corePoolSize maximumPoolSize 当一个任务委托给线程池时，如果池中线程数量低于 corePoolSize，一个新的线程将被创建，即使池中可能尚有空闲线程。如果内部任务队列已满，而且有至少 corePoolSize 正在运行，但是运行线程的数量低于 maximumPoolSize，一个新的线程将被创建去执行该任务。ThreadPoolExecutor 图解： 一个 ThreadPoolExecutor创建一个 ThreadPoolExecutor ThreadPoolExecutor 有若干个可用构造子。比如： [java] view plain copy print? 1. int corePoolSize = 5; 2. int maxPoolSize = 10; 3. long keepAliveTime = 5000; 4. 5. ExecutorService threadPoolExecutor = 6. new ThreadPoolExecutor( 7. corePoolSize, 8. maxPoolSize, 9. keepAliveTime, 10. TimeUnit.MILLISECONDS, 11. new LinkedBlockingQueue&lt;Runnable&gt;() 12. ); 但是，除非你确实需要显式为 ThreadPoolExecutor 定义所有参数，使用 java.util.concurrent.Executors 类中的工厂方法之一会更加方便，正如 ExecutorService 小节所述。 18. 定时执行者服务 ScheduledExecutorServicejava.util.concurrent.ScheduledExecutorService 是一个 ExecutorService， 它能够将任务延后执行，或者间隔固定时间多次执行。 任务由一个工作者线程异步执行，而不是由提交任务给 ScheduledExecutorService 的那个线程执行。 ScheduledExecutorService 例子 以下是一个简单的 ScheduledExecutorService 示例： [java] view plain copy print? 1. ScheduledExecutorService scheduledExecutorService = 2. Executors.newScheduledThreadPool(5); 3. 4. ScheduledFuture scheduledFuture = 5. scheduledExecutorService.schedule(new Callable() { 6. public Object call() throws Exception { 7. System.out.println(&quot;Executed!&quot;); 8. return &quot;Called!&quot;; 9. } 10. }, 11. 5, 12. TimeUnit.SECONDS); 首先一个内置 5 个线程的 ScheduledExecutorService 被创建。之后一个 Callable 接口的匿名类示例被创建然后传递给 schedule() 方法。后边的俩参数定义了 Callable 将在 5 秒钟之后被执行。 ScheduledExecutorService 实现 既然 ScheduledExecutorService 是一个接口，你要用它的话就得使用 java.util.concurrent 包里对它的某个实现类。ScheduledExecutorService 具有以下实现类： ScheduledThreadPoolExecutor 创建一个 ScheduledExecutorService 如何创建一个 ScheduledExecutorService 取决于你采用的它的实现类。但是你也可以使用 Executors 工厂类来创建一个 ScheduledExecutorService 实例。比如： [java] view plain copy print? 1. ScheduledExecutorService scheduledExecutorService = 2. 3. Executors.newScheduledThreadPool(5); ScheduledExecutorService 使用 一旦你创建了一个 ScheduledExecutorService，你可以通过调用它的以下方法： schedule (Callable task, long delay, TimeUnit timeunit) schedule (Runnable task, long delay, TimeUnit timeunit) scheduleAtFixedRate (Runnable, long initialDelay, long period, TimeUnit timeunit) scheduleWithFixedDelay (Runnable, long initialDelay, long period, TimeUnit timeunit) 下面我们就简单看一下这些方法。schedule (Callable task, long delay, TimeUnit timeunit) 这个方法计划指定的 Callable 在给定的延迟之后执行。这个方法返回一个 ScheduledFuture，通过它你可以在它被执行之前对它进行取消，或者在它执行之后获取结果。 以下是一个示例： [java] view plain copy print? 1. ScheduledExecutorService scheduledExecutorService = 2. Executors.newScheduledThreadPool(5); 3. 4. ScheduledFuture scheduledFuture = 5. scheduledExecutorService.schedule(new Callable() { 6. public Object call() throws Exception { 7. System.out.println(&quot;Executed!&quot;); 8. return &quot;Called!&quot;; 9. } 10. }, 11. 5, 12. TimeUnit.SECONDS); 13. 14. System.out.println(&quot;result = &quot; + scheduledFuture.get()); 15. 16. scheduledExecutorService.shutdown(); 示例输出结果： Executed! result = Called! schedule (Runnable task, long delay, TimeUnit timeunit) 除了 Runnable 无法返回一个结果之外，这一方法工作起来就像以一个 Callable 作为一个参数的那个版本的方法一样，因此 ScheduledFuture.get() 在任务执行结束之后返回 null。scheduleAtFixedRate (Runnable, long initialDelay, long period, TimeUnit timeunit) 这一方法规划一个任务将被定期执行。该任务将会在首个 initialDelay 之后得到执行，然后每个 period 时间之后重复执行。如果给定任务的执行抛出了异常，该任务将不再执行。如果没有任何异常的话，这个任务将会持续循环执行到 ScheduledExecutorService 被关闭。如果一个任务占用了比计划的时间间隔更长的时候，下一次执行将在当前执行结束执行才开始。计划任务在同一时间不会有多个线程同时执行。 scheduleWithFixedDelay (Runnable, long initialDelay, long period, TimeUnit timeunit) 除了 period 有不同的解释之外这个方法和 scheduleAtFixedRate() 非常像。scheduleAtFixedRate() 方法中，period 被解释为前一个执行的开始和下一个执行的开始之间的间隔时间。而在本方法中，period 则被解释为前一个执行的结束和下一个执行的结束之间的间隔。因此这个延迟是执行结束之间的间隔，而不是执行开始之间的间隔。ScheduledExecutorService 关闭 正如 ExecutorService，在你使用结束之后你需要把 ScheduledExecutorService 关闭掉。否则他将导致 JVM 继续运行，即使所有其他线程已经全被关闭。你可以使用从 ExecutorService 接口继承来的 shutdown() 或 shutdownNow() 方法将 ScheduledExecutorService 关闭。参见 ExecutorService 关闭部分以获取更多信息。 ##19. 使用 ForkJoinPool 进行分叉和合并 ForkJoinPool 在 Java 7 中被引入。它和 ExecutorService 很相似，除了一点不同。 ForkJoinPool 让我们可以很方便地把任务分裂成几个更小的任务，这些分裂出来的任务也将会提交给 ForkJoinPool。任务可以继续分割成更小的子任务，只要它还能分割。可能听起来有些抽象，因此本节中我们将会解释 ForkJoinPool 是如何工作的，还有任务分割是如何进行的。 分叉和合并解释 在我们开始看 ForkJoinPool 之前我们先来简要解释一下分叉和合并的原理。分叉和合并原理包含两个递归进行的步骤。两个步骤分别是分叉步骤和合并步骤。分叉 一个使用了分叉和合并原理的任务可以将自己分叉(分割)为更小的子任务，这些子任务可以被并发执行。如下图所示： 通过把自己分割成多个子任务，每个子任务可以由不同的 CPU 并行执行，或者被同一个 CPU 上的不同线程执行。只有当给的任务过大，把它分割成几个子任务才有意义。把任务分割成子任务有一定开销，因此对于小型任务，这个分割的消耗可能比每个子任务并发执行的消耗还要大。什么时候把一个任务分割成子任务是有意义的，这个界限也称作一个阀值。这要看每个任务对有意义阀值的决定。很大程度上取决于它要做的工作的种类。合并 当一个任务将自己分割成若干子任务之后，该任务将进入等待所有子任务的结束之中。一旦子任务执行结束，该任务可以把所有结果合并到同一个结果。图示如下： 当然，并非所有类型的任务都会返回一个结果。如果这个任务并不返回一个结果，它只需等待所有子任务执行完毕。也就不需要结果的合并啦。 ForkJoinPool ForkJoinPool 是一个特殊的线程池，它的设计是为了更好的配合 分叉-和-合并 任务分割的工作。ForkJoinPool 也在 java.util.concurrent 包中，其完整类名为 java.util.concurrent.ForkJoinPool。 创建一个 ForkJoinPool 你可以通过其构造子创建一个 ForkJoinPool。作为传递给 ForkJoinPool 构造子的一个参数，你可以定义你期望的并行级别。并行级别表示你想要传递给 ForkJoinPool 的任务所需的线程或 CPU 数量。以下是一个 ForkJoinPool 示例： [java] view plain copy print? 1. ForkJoinPool forkJoinPool = new ForkJoinPool(4); 这个示例创建了一个并行级别为 4 的 ForkJoinPool。提交任务到 ForkJoinPool 就像提交任务到 ExecutorService 那样，把任务提交到 ForkJoinPool。你可以提交两种类型的任务。一种是没有任何返回值的(一个 “行动”)，另一种是有返回值的(一个”任务”)。这两种类型分别由 RecursiveAction 和 RecursiveTask 表示。接下来介绍如何使用这两种类型的任务，以及如何对它们进行提交。RecursiveAction RecursiveAction 是一种没有任何返回值的任务。它只是做一些工作，比如写数据到磁盘，然后就退出了。一个 RecursiveAction 可以把自己的工作分割成更小的几块，这样它们可以由独立的线程或者 CPU 执行。你可以通过继承来实现一个 RecursiveAction。示例如下： [java] view plain copy print? 1. import java.util.ArrayList; 2. import java.util.List; 3. import java.util.concurrent.RecursiveAction; 4. 5. public class MyRecursiveAction extends RecursiveAction { 6. 7. private long workLoad = 0; 8. 9. public MyRecursiveAction(long workLoad) { 10. this.workLoad = workLoad; 11. } 12. 13. @Override 14. protected void compute() { 15. 16. //if work is above threshold, break tasks up into smaller tasks 17. if(this.workLoad &gt; 16) { 18. System.out.println(&quot;Splitting workLoad : &quot; + this.workLoad); 19. 20. List&lt;MyRecursiveAction&gt; subtasks = 21. new ArrayList&lt;MyRecursiveAction&gt;(); 22. 23. subtasks.addAll(createSubtasks()); 24. 25. for(RecursiveAction subtask : subtasks){ 26. subtask.fork(); 27. } 28. 29. } else { 30. System.out.println(&quot;Doing workLoad myself: &quot; + this.workLoad); 31. } 32. } 33. 34. private List&lt;MyRecursiveAction&gt; createSubtasks() { 35. List&lt;MyRecursiveAction&gt; subtasks = 36. new ArrayList&lt;MyRecursiveAction&gt;(); 37. 38. MyRecursiveAction subtask1 = new MyRecursiveAction(this.workLoad / 2); 39. MyRecursiveAction subtask2 = new MyRecursiveAction(this.workLoad / 2); 40. 41. subtasks.add(subtask1); 42. subtasks.add(subtask2); 43. 44. return subtasks; 45. } 46. 47. } 例子很简单。MyRecursiveAction 将一个虚构的 workLoad 作为参数传给自己的构造子。如果 workLoad 高于一个特定阀值，该工作将被分割为几个子工作，子工作继续分割。如果 workLoad 低于特定阀值，该工作将由 MyRecursiveAction 自己执行。你可以这样规划一个 MyRecursiveAction 的执行： [java] view plain copy print? 1. MyRecursiveAction myRecursiveAction = new MyRecursiveAction(24); 2. 3. forkJoinPool.invoke(myRecursiveAction); RecursiveTask RecursiveTask 是一种会返回结果的任务。它可以将自己的工作分割为若干更小任务，并将这些子任务的执行结果合并到一个集体结果。可以有几个水平的分割和合并。以下是一个 RecursiveTask 示例： [java] view plain copy print? 1. import java.util.ArrayList; 2. import java.util.List; 3. import java.util.concurrent.RecursiveTask; 4. 5. 6. public class MyRecursiveTask extends RecursiveTask&lt;Long&gt; { 7. 8. private long workLoad = 0; 9. 10. public MyRecursiveTask(long workLoad) { 11. this.workLoad = workLoad; 12. } 13. 14. protected Long compute() { 15. 16. //if work is above threshold, break tasks up into smaller tasks 17. if(this.workLoad &gt; 16) { 18. System.out.println(&quot;Splitting workLoad : &quot; + this.workLoad); 19. 20. List&lt;MyRecursiveTask&gt; subtasks = 21. new ArrayList&lt;MyRecursiveTask&gt;(); 22. subtasks.addAll(createSubtasks()); 23. 24. for(MyRecursiveTask subtask : subtasks){ 25. subtask.fork(); 26. } 27. 28. long result = 0; 29. for(MyRecursiveTask subtask : subtasks) { 30. result += subtask.join(); 31. } 32. return result; 33. 34. } else { 35. System.out.println(&quot;Doing workLoad myself: &quot; + this.workLoad); 36. return workLoad * 3; 37. } 38. } 39. 40. private List&lt;MyRecursiveTask&gt; createSubtasks() { 41. List&lt;MyRecursiveTask&gt; subtasks = 42. new ArrayList&lt;MyRecursiveTask&gt;(); 43. 44. MyRecursiveTask subtask1 = new MyRecursiveTask(this.workLoad / 2); 45. MyRecursiveTask subtask2 = new MyRecursiveTask(this.workLoad / 2); 46. 47. subtasks.add(subtask1); 48. subtasks.add(subtask2); 49. 50. return subtasks; 51. } 52. } 除了有一个结果返回之外，这个示例和 RecursiveAction 的例子很像。MyRecursiveTask 类继承自 RecursiveTask，这也就意味着它将返回一个 Long 类型的结果。 MyRecursiveTask 示例也会将工作分割为子任务，并通过 fork() 方法对这些子任务计划执行。此外，本示例还通过调用每个子任务的 join() 方法收集它们返回的结果。子任务的结果随后被合并到一个更大的结果，并最终将其返回。对于不同级别的递归，这种子任务的结果合并可能会发生递归。你可以这样规划一个 RecursiveTask： [java] view plain copy print? 1. MyRecursiveTask myRecursiveTask = new MyRecursiveTask(128); 2. 3. long mergedResult = forkJoinPool.invoke(myRecursiveTask); 4. 5. System.out.println(&quot;mergedResult = &quot; + mergedResult); 注意是如何通过 ForkJoinPool.invoke() 方法的调用来获取最终执行结果的。ForkJoinPool评论 貌似并非每个人都对 Java 7 里的 ForkJoinPool 满意：《一个 Java 分叉-合并 带来的灾祸》。在你计划在自己的项目里使用 ForkJoinPool 之前最好读一下该篇文章。 20. 锁 Lockjava.util.concurrent.locks.Lock 是一个类似于 synchronized 块的线程同步机制。但是 Lock 比 synchronized 块更加灵活、精细。顺便说一下，在我的《Java 并发指南》中我对如何实现你自己的锁进行了描述。Java Lock 例子 既然 Lock 是一个接口，在你的程序里需要使用它的实现类之一来使用它。以下是一个简单示例： [java] view plain copy print? 1. Lock lock = new ReentrantLock(); 2. 3. lock.lock(); 4. 5. //critical section 6. 7. lock.unlock(); 首先创建了一个 Lock 对象。之后调用了它的 lock() 方法。这时候这个 lock 实例就被锁住啦。任何其他再过来调用 lock() 方法的线程将会被阻塞住，直到锁定 lock 实例的线程调用了 unlock() 方法。最后 unlock() 被调用了，lock 对象解锁了，其他线程可以对它进行锁定了。 Java Lock 实现 java.util.concurrent.locks 包提供了以下对 Lock 接口的实现类： ReentrantLock Lock 和 synchronized 代码块的主要不同点 一个 Lock 对象和一个 synchronized 代码块之间的主要不同点是： synchronized 代码块不能够保证进入访问等待的线程的先后顺序。 你不能够传递任何参数给一个 synchronized 代码块的入口。因此，对于 synchronized 代码块的访问等待设置超时时间是不可能的事情。 synchronized 块必须被完整地包含在单个方法里。而一个 Lock 对象可以把它的 lock() 和 unlock() 方法的调用放在不同的方法里。 Lock 的方法 Lock 接口具有以下主要方法： lock() lockInterruptibly() tryLock() tryLock(long timeout, TimeUnit timeUnit) unlock() lock() 将 Lock 实例锁定。如果该 Lock 实例已被锁定，调用 lock() 方法的线程将会阻塞，直到 Lock 实例解锁。 lockInterruptibly() 方法将会被调用线程锁定，除非该线程被打断。此外，如果一个线程在通过这个方法来锁定 Lock 对象时进入阻塞等待，而它被打断了的话，该线程将会退出这个方法调用。 tryLock() 方法试图立即锁定 Lock 实例。如果锁定成功，它将返回 true，如果 Lock 实例已被锁定该方法返回 false。这一方法永不阻塞。 tryLock(long timeout, TimeUnit timeUnit) 的工作类似于 tryLock() 方法，除了它在放弃锁定 Lock 之前等待一个给定的超时时间之外。 unlock() 方法对 Lock 实例解锁。一个 Lock 实现将只允许锁定了该对象的线程来调用此方法。其他(没有锁定该 Lock 对象的线程)线程对 unlock() 方法的调用将会抛一个未检查异常(RuntimeException)。 21. 读写锁 ReadWriteLockjava.util.concurrent.locks.ReadWriteLock 读写锁是一种先进的线程锁机制。它能够允许多个线程在同一时间对某特定资源进行读取，但同一时间内只能有一个线程对其进行写入。 读写锁的理念在于多个线程能够对一个共享资源进行读取，而不会导致并发问题。并发问题的发生场景在于对一个共享资源的读和写操作的同时进行，或者多个写操作并发进行。 本节只讨论 Java 内置 ReadWriteLock。如果你想了解 ReadWriteLock 背后的实现原理，请参考我的《Java 并发指南》主题中的《读写锁》小节。 ReadWriteLock 锁规则 一个线程在对受保护资源在读或者写之前对 ReadWriteLock 锁定的规则如下： 读锁：如果没有任何写操作线程锁定 ReadWriteLock，并且没有任何写操作线程要求一个写锁(但还没有获得该锁)。因此，可以有多个读操作线程对该锁进行锁定。 写锁：如果没有任何读操作或者写操作。因此，在写操作的时候，只能有一个线程对该锁进行锁定。ReadWriteLock 实现 ReadWriteLock 是个接口，如果你想用它的话就得去使用它的实现类之一。 java.util.concurrent.locks 包提供了 ReadWriteLock 接口的以下实现类： ReentrantReadWriteLock ReadWriteLock 代码示例 以下是 ReadWriteLock 的创建以及如何使用它进行读、写锁定的简单示例代码： [java] view plain copy print? 1. ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); 2. 3. 4. readWriteLock.readLock().lock(); 5. 6. // multiple readers can enter this section 7. // if not locked for writing, and not writers waiting 8. // to lock for writing. 9. 10. readWriteLock.readLock().unlock(); 11. 12. 13. readWriteLock.writeLock().lock(); 14. 15. // only one writer can enter this section, 16. // and only if no threads are currently reading. 17. 18. readWriteLock.writeLock().unlock(); 注意如何使用 ReadWriteLock 对两种锁实例的持有。一个对读访问进行保护，一个队写访问进行保护。 22. 原子性布尔 AtomicBooleanAtomicBoolean 类为我们提供了一个可以用原子方式进行读和写的布尔值，它还拥有一些先进的原子性操作，比如 compareAndSet()。AtomicBoolean 类位于 java.util.concurrent.atomic 包，完整类名是为 java.util.concurrent.atomic.AtomicBoolean。本小节描述的 AtomicBoolean 是 Java 8 版本里的，而不是它第一次被引入的 Java 5 版本。 AtomicBoolean 背后的设计理念在我的《Java 并发指南》主题的《比较和交换》小节有解释。 创建一个 AtomicBoolean 你可以这样创建一个 AtomicBoolean： [java] view plain copy print? 1. AtomicBoolean atomicBoolean = new AtomicBoolean(); 以上示例新建了一个默认值为 false 的 AtomicBoolean。 如果你想要为 AtomicBoolean 实例设置一个显式的初始值，那么你可以将初始值传给 AtomicBoolean 的构造子： [java] view plain copy print? 1. AtomicBoolean atomicBoolean = new AtomicBoolean(true); 获取 AtomicBoolean 的值 你可以通过使用 get() 方法来获取一个 AtomicBoolean 的值。示例如下： [java] view plain copy print? 1. AtomicBoolean atomicBoolean = new AtomicBoolean(true); 2. 3. boolean value = atomicBoolean.get(); 以上代码执行后 value 变量的值将为 true。设置 AtomicBoolean 的值 你可以通过使用 set() 方法来设置一个 AtomicBoolean 的值。示例如下： [java] view plain copy print? 1. AtomicBoolean atomicBoolean = new AtomicBoolean(true); 2. 3. atomicBoolean.set(false); 以上代码执行后 AtomicBoolean 的值为 false。交换 AtomicBoolean 的值 你可以通过 getAndSet() 方法来交换一个 AtomicBoolean 实例的值。getAndSet() 方法将返回 AtomicBoolean 当前的值，并将为 AtomicBoolean 设置一个新值。示例如下： [java] view plain copy print? 1. AtomicBoolean atomicBoolean = new AtomicBoolean(true); 2. 3. boolean oldValue = atomicBoolean.getAndSet(false); 以上代码执行后 oldValue 变量的值为 true，atomicBoolean 实例将持有 false 值。代码成功将 AtomicBoolean 当前值 ture 交换为 false。 比较并设置 AtomicBoolean 的值 compareAndSet() 方法允许你对 AtomicBoolean 的当前值与一个期望值进行比较，如果当前值等于期望值的话，将会对 AtomicBoolean 设定一个新值。compareAndSet() 方法是原子性的，因此在同一时间之内有单个线程执行它。因此 compareAndSet() 方法可被用于一些类似于锁的同步的简单实现。 以下是一个 compareAndSet() 示例： [java] view plain copy print? 1. AtomicBoolean atomicBoolean = new AtomicBoolean(true); 2. 3. boolean expectedValue = true; 4. boolean newValue = false; 5. 6. boolean wasNewValueSet = atomicBoolean.compareAndSet( 7. expectedValue, newValue); 本示例对 AtomicBoolean 的当前值与 true 值进行比较，如果相等，将 AtomicBoolean 的值更新为 false。 23. 原子性整型 AtomicIntegerAtomicInteger 类为我们提供了一个可以进行原子性读和写操作的 int 变量，它还包含一系列先进的原子性操作，比如 compareAndSet()。AtomicInteger 类位于 java.util.concurrent.atomic 包，因此其完整类名为 java.util.concurrent.atomic.AtomicInteger。本小节描述的 AtomicInteger 是 Java 8 版本里的，而不是它第一次被引入的 Java 5 版本。AtomicInteger 背后的设计理念在我的《Java 并发指南》主题的《比较和交换》小节有解释。 创建一个 AtomicInteger 创建一个 AtomicInteger 示例如下： [java] view plain copy print? 1. AtomicInteger atomicInteger = new AtomicInteger(); 本示例将创建一个初始值为 0 的 AtomicInteger。如果你想要创建一个给定初始值的 AtomicInteger，你可以这样： [java] view plain copy print? 1. AtomicInteger atomicInteger = new AtomicInteger(123); 本示例将 123 作为参数传给 AtomicInteger 的构造子，它将设置 AtomicInteger 实例的初始值为 123。 获取 AtomicInteger 的值 你可以使用 get() 方法获取 AtomicInteger 实例的值。示例如下： [java] view plain copy print? 1. AtomicInteger atomicInteger = new AtomicInteger(123); 2. 3. int theValue = atomicInteger.get(); 设置 AtomicInteger 的值 你可以通过 set() 方法对 AtomicInteger 的值进行重新设置。以下是 AtomicInteger.set() 示例： [java] view plain copy print? 1. AtomicInteger atomicInteger = new AtomicInteger(123); 2. 3. atomicInteger.set(234); 以上示例创建了一个初始值为 123 的 AtomicInteger，而在第二行将其值更新为 234。 比较并设置 AtomicInteger 的值 AtomicInteger 类也通过了一个原子性的 compareAndSet() 方法。这一方法将 AtomicInteger 实例的当前值与期望值进行比较，如果二者相等，为 AtomicInteger 实例设置一个新值。AtomicInteger.compareAndSet() 代码示例： [java] view plain copy print? 1. AtomicInteger atomicInteger = new AtomicInteger(123); 2. 3. int expectedValue = 123; 4. int newValue = 234; 5. atomicInteger.compareAndSet(expectedValue, newValue); 本示例首先新建一个初始值为 123 的 AtomicInteger 实例。然后将 AtomicInteger 与期望值 123 进行比较，如果相等，将 AtomicInteger 的值更新为 234。 增加 AtomicInteger 值 AtomicInteger 类包含有一些方法，通过它们你可以增加 AtomicInteger 的值，并获取其值。这些方法如下： addAndGet() getAndAdd() getAndIncrement() incrementAndGet() 第一个 addAndGet() 方法给 AtomicInteger 增加了一个值，然后返回增加后的值。getAndAdd() 方法为 AtomicInteger 增加了一个值，但返回的是增加以前的 AtomicInteger 的值。具体使用哪一个取决于你的应用场景。以下是这两种方法的示例： [java] view plain copy print? 1. AtomicInteger atomicInteger = new AtomicInteger(); 2. 3. 4. System.out.println(atomicInteger.getAndAdd(10)); 5. System.out.println(atomicInteger.addAndGet(10)); 本示例将打印出 0 和 20。例子中，第二行拿到的是加 10 之前的 AtomicInteger 的值。加 10 之前的值是 0。第三行将 AtomicInteger 的值再加 10，并返回加操作之后的值。该值现在是为 20。 你当然也可以使用这俩方法为 AtomicInteger 添加负值。结果实际是一个减法操作。getAndIncrement() 和 incrementAndGet() 方法类似于 getAndAdd() 和 addAndGet()，但每次只将 AtomicInteger 的值加 1。 减小 AtomicInteger 的值 AtomicInteger 类还提供了一些减小 AtomicInteger 的值的原子性方法。这些方法是： decrementAndGet() getAndDecrement() decrementAndGet() 将 AtomicInteger 的值减一，并返回减一后的值。getAndDecrement() 也将 AtomicInteger 的值减一，但它返回的是减一之前的值。 24. 原子性长整型 AtomicLongAtomicLong 类为我们提供了一个可以进行原子性读和写操作的 long 变量，它还包含一系列先进的原子性操作，比如 compareAndSet()AtomicLong 类位于 java.util.concurrent.atomic 包，因此其完整类名为 java.util.concurrent.atomic.AtomicLong。本小节描述的 AtomicLong 是 Java 8 版本里的，而不是它第一次被引入的 Java 5 版本。 AtomicLong 背后的设计理念在我的《Java 并发指南》主题的《比较和交换》小节有解释。 创建一个 AtomicLong 创建一个 AtomicLong 如下： [java] view plain copy print? 1. AtomicLong atomicLong = new AtomicLong(); 将创建一个初始值为 0 的 AtomicLong。如果你想创建一个指定初始值的 AtomicLong，可以： [java] view plain copy print? 1. AtomicLong atomicLong = new AtomicLong(123); 本示例将 123 作为参数传递给 AtomicLong 的构造子，后者将 AtomicLong 实例的初始值设置为 123。 获取 AtomicLong 的值 你可以通过 get() 方法获取 AtomicLong 的值。AtomicLong.get() 示例： [java] view plain copy print? 1. AtomicLong atomicLong = new AtomicLong(123); 2. 3. long theValue = atomicLong.get(); 设置 AtomicLong 的值 你可以通过 set() 方法设置 AtomicLong 实例的值。一个 AtomicLong.set() 的示例： [java] view plain copy print? 1. AtomicLong atomicLong = new AtomicLong(123); 2. 3. atomicLong.set(234); 本示例新建了一个初始值为 123 的 AtomicLong，第二行将其值设置为 234。 比较并设置 AtomicLong 的值 AtomicLong 类也有一个原子性的 compareAndSet() 方法。这一方法将 AtomicLong 实例的当前值与一个期望值进行比较，如果两种相等，为 AtomicLong 实例设置一个新值。 AtomicLong.compareAndSet() 使用示例： [java] view plain copy print? 1. AtomicLong atomicLong = new AtomicLong(123); 2. 3. long expectedValue = 123; 4. long newValue = 234; 5. atomicLong.compareAndSet(expectedValue, newValue); 本示例新建了一个初始值为 123 的 AtomicLong。然后将 AtomicLong 的当前值与期望值 123 进行比较，如果相等的话，AtomicLong 的新值将变为 234。 增加 AtomicLong 值 AtomicLong 具备一些能够增加 AtomicLong 的值并返回自身值的方法。这些方法如下： addAndGet() getAndAdd() getAndIncrement() incrementAndGet() 第一个方法 addAndGet() 将 AtomicLong 的值加一个数字，并返回增加后的值。第二个方法 getAndAdd() 也将 AtomicLong 的值加一个数字，但返回的是增加前的 AtomicLong 的值。具体使用哪一个取决于你自己的场景。示例如下： [java] view plain copy print? 1. AtomicLong atomicLong = new AtomicLong(); 2. 3. 4. System.out.println(atomicLong.getAndAdd(10)); 5. System.out.println(atomicLong.addAndGet(10)); 本示例将打印出 0 和 20。例子中，第二行拿到的是加 10 之前的 AtomicLong 的值。加 10 之前的值是 0。第三行将 AtomicLong 的值再加 10，并返回加操作之后的值。该值现在是为 20。你当然也可以使用这俩方法为 AtomicLong 添加负值。结果实际是一个减法操作。getAndIncrement() 和 incrementAndGet() 方法类似于 getAndAdd() 和 addAndGet()，但每次只将 AtomicLong 的值加 1。减小 AtomicLong 的值 AtomicLong 类还提供了一些减小 AtomicLong 的值的原子性方法。这些方法是： decrementAndGet() getAndDecrement() decrementAndGet() 将 AtomicLong 的值减一，并返回减一后的值。getAndDecrement() 也将 AtomicLong 的值减一，但它返回的是减一之前的值。 25. 原子性引用型 AtomicReferenceAtomicReference 提供了一个可以被原子性读和写的对象引用变量。原子性的意思是多个想要改变同一个 AtomicReference 的线程不会导致 AtomicReference 处于不一致的状态。AtomicReference 还有一个 compareAndSet() 方法，通过它你可以将当前引用于一个期望值(引用)进行比较，如果相等，在该 AtomicReference 对象内部设置一个新的引用。 创建一个 AtomicReference 创建 AtomicReference 如下： [java] view plain copy print? 1. AtomicReference atomicReference = new AtomicReference(); 如果你需要使用一个指定引用创建 AtomicReference，可以： [java] view plain copy print? 1. String initialReference = &quot;the initially referenced string&quot;; 2. AtomicReference atomicReference = new AtomicReference(initialReference); 创建泛型 AtomicReference 你可以使用 Java 泛型来创建一个泛型 AtomicReference。示例： [java] view plain copy print? 1. AtomicReference&lt;String&gt; atomicStringReference = 2. new AtomicReference&lt;String&gt;(); 你也可以为泛型 AtomicReference 设置一个初始值。示例： [java] view plain copy print? 1. String initialReference = &quot;the initially referenced string&quot;; 2. AtomicReference&lt;String&gt; atomicStringReference = 3. new AtomicReference&lt;String&gt;(initialReference); 获取 AtomicReference 引用 你可以通过 AtomicReference 的 get() 方法来获取保存在 AtomicReference 里的引用。如果你的 AtomicReference 是非泛型的，get() 方法将返回一个 Object 类型的引用。如果是泛型化的，get() 将返回你创建 AtomicReference 时声明的那个类型。 先来看一个非泛型的 AtomicReference get() 示例： [java] view plain copy print? 1. AtomicReference atomicReference = new AtomicReference(&quot;first value referenced&quot;); 2. 3. String reference = (String) atomicReference.get(); 注意如何对 get() 方法返回的引用强制转换为 String。泛型化的 AtomicReference 示例： [java] view plain copy print? 1. AtomicReference&lt;String&gt; atomicReference = 2. new AtomicReference&lt;String&gt;(&quot;first value referenced&quot;); 3. 4. String reference = atomicReference.get(); 编译器知道了引用的类型，所以我们无需再对 get() 返回的引用进行强制转换了。设置 AtomicReference 引用 你可以使用 get() 方法对 AtomicReference 里边保存的引用进行设置。如果你定义的是一个非泛型 AtomicReference，set() 将会以一个 Object 引用作为参数。如果是泛型化的 AtomicReference，set() 方法将只接受你定义给的类型。AtomicReference set() 示例： [java] view plain copy print? 1. AtomicReference atomicReference = 2. new AtomicReference(); 3. 4. atomicReference.set(&quot;New object referenced&quot;); 这个看起来非泛型和泛型化的没啥区别。真正的区别在于编译器将对你能够设置给一个泛型化的 AtomicReference 参数类型进行限制。 比较并设置 AtomicReference 引用 AtomicReference 类具备了一个很有用的方法：compareAndSet()。compareAndSet() 可以将保存在 AtomicReference 里的引用于一个期望引用进行比较，如果两个引用是一样的(并非 equals() 的相等，而是 == 的一样)，将会给 AtomicReference 实例设置一个新的引用。如果 compareAndSet() 为 AtomicReference 设置了一个新的引用，compareAndSet() 将返回 true。否则 compareAndSet() 返回 false。 AtomicReference compareAndSet() 示例： [java] view plain copy print? 1. String initialReference = &quot;initial value referenced&quot;; 2. 3. AtomicReference&lt;String&gt; atomicStringReference = 4. new AtomicReference&lt;String&gt;(initialReference); 5. 6. String newReference = &quot;new value referenced&quot;; 7. boolean exchanged = atomicStringReference.compareAndSet(initialReference, newReference); 8. System.out.println(&quot;exchanged: &quot; + exchanged); 9. 10. exchanged = atomicStringReference.compareAndSet(initialReference, newReference); 11. System.out.println(&quot;exchanged: &quot; + exchanged); 本示例创建了一个带有一个初始引用的泛型化的 AtomicReference。之后两次调用 comparesAndSet()来对存储值和期望值进行对比，如果二者一致，为 AtomicReference 设置一个新的引用。第一次比较，存储的引用(initialReference)和期望的引用(initialReference)一致，所以一个新的引用(newReference)被设置给 AtomicReference，compareAndSet() 方法返回 true。第二次比较时，存储的引用(newReference)和期望的引用(initialReference)不一致，因此新的引用没有被设置给 AtomicReference，compareAndSet() 方法返回 false。 原文链接：http://tutorials.jenkov.com/java-util-concurrent/index.html。","categories":[],"tags":[{"name":"并发相关","slug":"并发相关","permalink":"http://yoursite.com/tags/并发相关/"}]},{"title":"JVM GC要点整理与总结","slug":"JVM GC要点整理与总结（转载）","date":"2016-11-15T06:41:09.000Z","updated":"2017-08-18T09:25:24.000Z","comments":true,"path":"2016/11/15/JVM GC要点整理与总结（转载）/","link":"","permalink":"http://yoursite.com/2016/11/15/JVM GC要点整理与总结（转载）/","excerpt":"这篇文章是我偶然看到的一篇对JVM GC部分的要点整理的比较全面比较详细的一篇文章，所以大胆转载了过来，也是给我个人做一个备忘和参考，或许以后也会在这个基础之上进行补充。这是原文的地址：http://blog.leanote.com/post/shiwei/Java-GC?spm=5176.100239.blogcont91017.9.3Qo1pk请大家支持原作者，感谢原作者的认真和辛勤整理。","text":"这篇文章是我偶然看到的一篇对JVM GC部分的要点整理的比较全面比较详细的一篇文章，所以大胆转载了过来，也是给我个人做一个备忘和参考，或许以后也会在这个基础之上进行补充。这是原文的地址：http://blog.leanote.com/post/shiwei/Java-GC?spm=5176.100239.blogcont91017.9.3Qo1pk请大家支持原作者，感谢原作者的认真和辛勤整理。 范围：要回收哪些区域在JVM五种内存模型中，有三个是不需要进行垃圾回收的：程序计数器、JVM栈、本地方法栈。因为它们的生命周期是和线程同步的，随着线程的销毁，它们占用的内存会自动释放，所以只有方法区和堆需要进行GC。 前提：如何判断对象已死所有的垃圾收集算法都面临同一个问题，那就是找出应用程序不可到达的内存块，将其释放，这里面讲的不可达主要是指应用程序已经没有内存块的引用了， 在Java中，某个对象对应用程序是可到达的是指：这个对象被根（根主要是指类的静态变量，或者活跃在所有线程栈的对象的引用）引用或者对象被另一个可到达的对象引用。 引用计数算法引用计数是最简单直接的一种方式，这种方式在每一个对象中增加一个引用的计数，这个计数代表当前程序有多少个引用引用了此对象，如果此对象的引用计数变为0，那么此对象就可以作为垃圾收集器的目标对象来收集。优点：简单，直接，不需要暂停整个应用缺点：1.需要编译器的配合，编译器要生成特殊的指令来进行引用计数的操作；2.不能处理循环引用的问题因此这种方法是垃圾收集的早期策略，现在很少使用。Sun的JVM并没有采用引用计数算法来进行垃圾回收，而是基于根搜索算法的。 可达性分析算法（根搜索算法）通过一系列的名为“GC Root”的对象作为起点，从这些节点向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Root没有任何引用链相连时，则该对象不可达，该对象是不可使用的，垃圾收集器将回收其所占的内存。 在java语言中，可作为GCRoot的对象包括以下几种：a. java虚拟机栈(栈帧中的本地变量表)中的引用的对象。b.方法区中的类静态属性引用的对象。c.方法区中的常量引用的对象。d.本地方法栈中JNI本地方法的引用对象。 ###四种引用GC在收集一个对象的时候会判断是否有引用指向对象，在JAVA中的引用主要有四种： 强引用（Strong Reference）强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。 软引用（Soft Reference）如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。下面举个例子，假如有一个应用需要读取大量的本地图片，如果每次读取图片都从硬盘读取，则会严重影响性能，但是如果全部加载到内存当中，又有可能造成内存溢出，此时使用软引用可以解决这个问题。设计思路是：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了内存溢出的问题。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 弱引用（Weak Reference）弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 虚引用（Phantom Reference）“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用主要用于检测对象是否已经从内存中删除，跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。虚引用的唯一目的是当对象被回收时收到一个系统通知。 finalize() 方法通过可达性分析，那些不可达的对象并不是立即被销毁，他们还有被拯救的机会。如果要回收一个不可达的对象，要经历两次标记过程。首先是第一次标记，并判断对象是否覆写了 finalize 方法，如果没有覆写，则直接进行第二次标记并被回收。如果对象有覆写finalize 方法，则会将改对象加入一个叫“F-Queue”的队列中，虚拟机会建立一个低优先级的 Finalizer 线程去执行它，这里说的“执行”是指该线程会去触发 finalize 方法，但是并不会等待 finalize 方法执行完成。主要是因为 finalize 方法的不确定性，它可能要花很长时间才能执行完成，甚至死循环，永远不结束，这将导致整个 GC 工作的异常，甚至崩溃。关于拯救，可以在 finalize 方法中将自己（this关键字）赋值给类变量或其他对象的成员变量，则第二次标记时它将被移出回收的集合，如果对象并未被拯救，则最终被回收。finalize 方法只会被调用一次，如果一个在 finalize 被拯救的对象再次需要回收，则它的 finalize 将不会再被触发了。不建议使用finalize 方法，它的运行代价高，不确定性大，GC 也不会等待它执行完成，它的功能完全可以被 try-finally 代替。 方法区的回收方法区也会被回收，其被回收的内存有：废弃常量、无用的类。在 HotSpot 虚拟机规范里，将永久带作为方法区的实现。废弃常量：没有被引用的常量，如 String。判断无用的类：(1).该类的所有实例都已经被回收，即java堆中不存在该类的实例对象。(2).加载该类的类加载器已经被回收。(3).该类所对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射机制访问该类的方法。 各种垃圾收集算法标记-清除算法步骤：1、标记：从根集合开始扫描，标记存活对象；2、清除：再次扫描真个内存空间，回收未被标记的对象。此算法一般没有虚拟机采用优点1：解决了循环引用的问题优点2：与复制算法相比，不需要对象移动，效率较高，而且还不需要额外的空间不足1：每个活跃的对象都要进行扫描，而且要扫描两次，效率较低，收集暂停的时间比较长。不足2：产生不连续的内存碎片 标记-整理（压缩）算法对标记-清除算法的改进标记过程与标记-清除算法一样，但是标记完成后，存活对象向一端移动，然后清理边界的内存步骤：1、标记：从根集合开始扫描，标记存活对象；2、整理：再次扫描真个内存空间，并往内存一段移动存活对象，再清理掉边界的对象。不会产生内存碎片，但是依旧移动对象的成本。适合老年代还有一种算法是标记-清除-整理（压缩），是在多次标记清除后，再进行一次整理，这样就减少了移动对象的成本。 复制算法将内存分成两块容量大小相等的区域，每次只使用其中一块，当这一块内存用完了，就将所有存活对象复制到另一块内存空间，然后清除前一块内存空间。此种方法实现简单、效率较高，优点：1、不会产生内存碎；2、没有了先标记再删除的步骤，而是通过Tracing从 From内存中找到存活对象，复制到另一块To内存区域，From只要移动堆顶指针便可再次使用。缺点：1、复制的代价较高，所有适合新生代，因为新生代的对象存活率较低，需要复制的对象较少；2、需要双倍的内存空间，而且总是有一块内存空闲，浪费空间。 分代收集算法所有商业虚拟机都采用这种方式，将堆分成新生代和老年代，新生代使用复制算法，老年代使用标记-整理算法 GC 类型1.Minor GC 针对新生代的 GC2.Major GC 针对老年代的 GC3.Full GC 针对新生代、老年代、永久带的 GC 为什么要分不同的 GC 类型，主要是1、对象有不同的生命周期，经研究，98%的对象都是临时对象；2、根据各代的特点应用不同的 GC 算法，提高 GC 效率。 各种垃圾收集器###串行收集器（Serial Collector）单线程，会发生停顿适用场景：1.单 CPU、新生代小、对停顿时间要求不高的应用2.client 模式下或32位 Windows 上的默认收集器新生代均采用复制算法，老年代用标记-整理算法（Serial Old Collector）在单核 CPU 上面的运行效果较好，甚至可能超过并行垃圾收集器，因为并行垃圾收集器有线程的切换消耗。当 Eden 空间分配不足时触发原理：1.拷贝 Eden 和 From 空间的存活对象到 To 空间2.部分对象可能晋升到老年代（大对象、达到年龄的对象、To 空间不足时）3.清空 Eden、From 空间，From 与 To 空间交换角色 ParNew（Serial 收集器的多线程版本）新生代收集器，是 Serial 的多线程版，是 Server 模式下的虚拟机中首选的新生代收集器，不是默认收集器。除了 Serial 外，是唯一能与 CMS 收集器配合工作的收集器。多线程下，性能较好，单线程下，并不会比 Serial 好。 并行收集器（Parallel Scavenge）特性：1.并行、停顿2.并行线程数：CPU &lt;= 8 := 8,CPU &gt; 8 := (3+ cpu * 5) / 8,也可强制指定 GC 线程数3.自适应调节策略，如果把该策略打开，则虚拟机会自动调整新生代的大小比例和晋升老年代的对象大小、年龄等细节参数4.吞吐量优先收集器，即可用设置一个 GC 时间，收集器将尽可能的在该时间内完成 GC 吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间），即吞吐量越高，则垃圾收集时间就要求越短用户可以设置最大垃圾收集停顿时间或者吞吐量但并不是把最大垃圾收集停顿时间设置得越短越好，因为它是以牺牲吞吐量和新生代空间的代价来换取的，比如收集300M 空间总会比收集500M 空间更快，再如收集频率加高，本来10秒收集一次，每次停顿100毫秒，但是现在改成了5秒收集一次，每次停顿70毫秒，停顿时间是小了，但是吞吐量确也降下来了。 适用场景：1.多 CPU、对停顿时间要求高的应用2.是 Server 端的默认新生代收集器 Serial Old是 Serial 收集器的老年代版本，依旧是单线程收集器，采用标记-整理算法， Parallel Old略 CMS（并发-标记-清除）CMS 是一种以获取最短回收停顿时间为目标的收集器。步骤：1.初始标记此阶段仅仅是标记一下 GC Roots 能直接关联到的对象，速度很快，但是会停顿 注意：这里不是 GC Roots Tracing 的过程2.并发标记GC Roots Tracing 的过程，这个阶段可以与用户线程一起工作，不会造成停顿,从而导致整个停顿时间大大降低3.重新标记是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录4.并发清除优点：停顿时间短，但是总的 GC 时间长缺点：1.并发程序都是 CPU 敏感的，并发标记和并发清除可能会抢占应用 CPU2.总的 GC 时间长3.无法处理浮动垃圾 浮动垃圾：在并发清除过程中，程序还在运行，可能产生新的垃圾，但是本次 GC 确不可能清除掉这些新产生的垃圾了，所以这些新产生垃圾就叫浮动垃圾，也就是说在一次 CMS 的 GC 后，用户获取不到一个完全干净的内存空间，还是或多或少存在浮动垃圾的。4.由于在并发标记和并发清除阶段，用户程序依旧在运行，所以也就需要为用户程序的运行预留一定空间，而不能想其他收集器一样会暂停用户程序的运行。在此期间，就可能发生预留空间不足，导致程序异常的情况。5.是基于标记-清除的收集器，所以会产生内存碎片 G1这款开发了10多年的收集器还比较年轻，目前还很少听说有人在生产环境使用。此款收集器可以独立管理整个 java heap 空间，而不需要其他收集器的配合。步骤： 初始标记与CMS 一样，只是标记一下 GC Roots 能直接关联到的对象，速度很快，但是需要停顿 并发标记GC Roots Tracing 过程，并发执行 最终标记并行执行，需要停顿 筛选回收并行执行，需要停顿 G1收集器把 Heap 分为多个大小相等的 Region，G1可以有计划的避免进行全区域的垃圾收集。G1跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先收集价值大的 Regin，保证 G1收集器在有限时间内获取最大的收集效率。 优点： 存在并发与并行操作，最大化利用硬件资源，提升收集效率 分代收集，虽然 G1可以独立管理整个 Heap，但是它还是保留了分代的概念，实际上,在分区时，这些区域(regions)被映射为逻辑上的 Eden, Survivor, 和 old generation(老年代)空间，使其有目的的收集特定区域的内存。 空间整合，G1回收内存时，是将某个或多个区域的存活对象拷贝至其他空区域，同时释放被拷贝的内存区域，这种方式在整体上看是标记-整理，在局部看（两个 Region 之间）是复制算法，所以不会产生内存碎片 可预测的停顿时间 内存分配策略对象优先在 Eden 区分配大对象直接进入老年代长期存活的对象将进入老年代动态对象年龄判断。并不是新生代对象的年龄一定要达到某个值，才会进入老年代。Survivor空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半，那么年龄等于或大于该年龄的对象就直接进入老年代，无须等待设置的年龄空间分配担保","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"链式存储线性表（LinkedList）数据结构解析","slug":"链式存储线性表（LinkedList）数据结构解析","date":"2016-11-03T09:37:31.000Z","updated":"2017-08-25T10:45:11.000Z","comments":true,"path":"2016/11/03/链式存储线性表（LinkedList）数据结构解析/","link":"","permalink":"http://yoursite.com/2016/11/03/链式存储线性表（LinkedList）数据结构解析/","excerpt":"Redis常见问题整理","text":"Redis常见问题整理 一、节点分析LinkedList内部是通过链表来实现的，那么就少不了节点，所以在源码中必然能找到这样一个节点。 private static class Node&lt;E&gt; { E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; } } 节点中定义了三个成员变量：E item（节点的存储内容）、Node next（记录下一个节点的指针）、Node prev（记录后一个节点的指针），其构造方法我觉得很巧妙，该构造函数的三个参数中就包含了它的前一个节点，节点保存的内容，和它的后一个节点，只要通过这个构造函数new出的新节点就自动实现了节点间的链接，在后面的增删改查操作中我们会发现，通过这个构造方法我们可以省去很多Node next和Node prev指针指来指去的操作。 二、LinkedList的核心操作方法在LinkedList中有可以看到这样两个成员变量Node first和Node last /** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */ transient Node&lt;E&gt; first; /** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */ transient Node&lt;E&gt; last; 这个两个成员变量很关键，主要用来记录链表的头和尾，这样方便我们在CRUD操作的过程中来查找到相应位置的节点。通过分析源码可以知道LinkedList其实是用的是双向链表来实现的。 在分析一个数据结构的时候，从相关add方法分析走能很好的理清数据结构的脉络。 linkFirst方法的分析可以看到在addFirst的方法中其实是调用的linkFirst方法。 /** * Inserts the specified element at the beginning of this list. * * @param e the element to add */ public void addFirst(E e) { linkFirst(e); } 接下来看看linkFirst方法是如何实现节点间操作的： /** * Links e as first element. */ private void linkFirst(E e) { final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); first = newNode; if (f == null) last = newNode; else f.prev = newNode; size++; modCount++; } linkFirst顾名思义，就是将节点链接到第一个。该方法首先是拿到链表的first(第一个）节点，然后通过那个巧妙的节点构造函数构造出一个新节点，然后将记录链表头的first指向这个新的节点，如果之前那个记录链表头的first节点等于null，说明当前链表中还没有一个节点（空链表）,所以就将记录链表尾的last节点也指向这个新节点；如果之前那个记录链表头的first节点不为null，那么就将之前的第一个节点的prev指针指向新节点，在节点的构造函数中就完成了新节点的next指针指向之前的第一个节点，所以这样就形成了节点间的双向记录。 linkLast方法的分析可以看到在addLast的方法中其实是调用的linkLast方法。 /** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to { @link #add}. * * @param e the element to add */ public void addLast(E e) { linkLast(e); } 再来看看linkLast方法是如何实现的： /** * Links e as last element. */ void linkLast(E e) { final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; } 这个方法是不是和linkFirst方法很像，它首先是拿到记录链表的last节点，然后又通过那个巧妙的构造方法构造一个新的节点，最后同样是判断之前记录链表的last节点为不为null，如果为null说明链表依然是空的，所以就将记录链表头的first指向该新节点，如果不为null说明链表之前已经有节点了，此时只需要将之前的那个尾节点的next指针指向当前新节点即可，同样是构造方法帮助我们完成了新节点的prev指针指向前一个节点。所以我觉得那个节点的构造函数很巧妙。 linkBefore方法的分析这个方法是比较重要也比较难理解的方法，先来看看这个函数的代码： /** * Inserts element e before non-null Node succ. */ void linkBefore(E e, Node&lt;E&gt; succ) { // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++; } 虽然代码和简洁，但却比较难理解，这个方法的两个参数分别表示：插入新节点的元素、需要在哪个节点前插入的节点。结合下面的这张图来分析：比如我现在想在Node3前面插入一个节点，那么当前的succ = Node3，所以这句代码final Node pred = succ.prev;执行后pred = Node2，再通过那个巧妙的节点构造函数就将新节点链接上去了，如图：这时候再通这句代码succ.prev = newNode;就Node3的prev指针指向了插入的新节点。后面的判读pred为不为null是为了知道是不是再第一个节点前插入新节点，如果是在第一个节点前插入新节点，那么就将记录链表头的first指针指向新节点，否则就pred的next指针指向插入的新节点，这样就完成了 新节点的插入操作。如图： unlinkFirst方法的分析 /** * Unlinks non-null first node f. */ private E unlinkFirst(Node&lt;E&gt; f) { // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element; } 该方法是移除第一个节点，首先是通过传入的first指针拿到第一个节点的内容，然后拿到它的下一个节点，再将第一个节点的内容和指向下个节点的next指针置空，方便GC回收。下一步便是将记录头节点的first指向final Node next = f.next;拿到的这个节点，如果这个的节点为空，那么last = null（说明链表在移除第一个节点前只有一个节点），否则就将拿到的这个节点中的prev指针置空，表示这个节点就是第一个节点。 unlinkLast方法的分析 /** * Unlinks non-null last node l. */ private E unlinkLast(Node&lt;E&gt; l) { // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element; } 这个方法和unlinkFirst的实现基本差不多，此方法的作用是移除链表中的最后一个节点。只要清楚了unlinkFirst这个方法，那么unlinkLast也就清楚了。 unlink方法的分析 /** * Unlinks non-null node x. */ E unlink(Node&lt;E&gt; x) { // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) { first = next; } else { prev.next = next; x.prev = null; } if (next == null) { last = prev; } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } 此方法是移除链表中指定的节点，在移除这个节点前肯定需要拿到这个节点prev指针和next指针所记录的节点，并需要判断prev指针和next是否为空，prev指针为空表示这个节点就是第一个节点，next指针为空表示这个节点就是最后一个节点。关键代码便是通过判断将拿到的prev节点的next指针指向拿到的next节点，以及将拿到的next节点的prev指针指向拿到的prev节点。 三、LinkedList中的经典算法在LinkedList中有一个根据索引查找相应节点的方法，此方法的源码如下： /** * Returns the (non-null) Node at the specified element index. */ Node&lt;E&gt; node(int index) { // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) { Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; } } 在这个方法中可以看到用到了折半查找的算法，当传入一个索引后会判断index &lt; (size &gt;&gt; 1)，如果index小于size的一半，则从前往后找节点；否则就从后往前找节点。 通过对LinkedList的分析后，对数据结构中的链表有了新的认识，在LinkedList中用的链表是双向链表，其实通过双向循环链表也可以来实现，如果是通过双向循环链表可以不需要last这个记录链表尾的变量了，只需要一个first变量记录链表的头，也可以实现从前往后和从后往前的查找等操作。 转载地址：https://my.oschina.net/devbird/blog/807571感谢原作者🙏","categories":[],"tags":[{"name":"Java容器相关","slug":"Java容器相关","permalink":"http://yoursite.com/tags/Java容器相关/"}]},{"title":"Mybatis特点总结","slug":"myBatis特点总结","date":"2016-10-24T09:37:31.000Z","updated":"2017-08-25T08:17:54.000Z","comments":true,"path":"2016/10/24/myBatis特点总结/","link":"","permalink":"http://yoursite.com/2016/10/24/myBatis特点总结/","excerpt":"","text":"简单来说，他跟你直接用一个sqlUtil的实现是一样，只不过很多复杂的util优化的事情，提前有其他程序员做了。 Mybatis是一个映射封装，他与你用util的区别就是，他将在代码块中的sql存在统一的xml文件也就是sqlmaper中。同时他将你执行sql的传参也就是执行变量进行了通配，然后映射到你的model中。 Mybatis大概的执行过程：通过factory方法获取sqlsession—-通过MapperProxy代理到dao–执行底层数据库操作，简单说就是跟楼上说的“据经过controller 再经过service 然后执行service中的相关方法并关联到mapper 再执行mapper.xml中的sql语句=== @象厂喜剧 ” 我们以JDBC为例看看他们的区别： JDBC：（1） 加载JDBC驱动,建立并获取数据库连接 ，创建statement对象（2） 设置SQL语句的传入参数 （3） 执行SQL语句并获得查询结果 （4） 对查询结果进行转换处理并将处理结果返回 （5） 释放资源 Mybatis：1：使用连接池，datasource，在驱动并连接的这个过程中优化并解耦JDBC第一步其实从效率角度来看是不合适的，因为无论什么数据库都不可能支撑随机和庞大的连接数，而且不可避免的存在连接浪费的情况，Mybatis就封装了这些优化的方法。 2：统一sql存取到XML如果代码写在java块中，在团队合作中很可能出现两个交叉业务的代码使用类似的sql语句，而开发人员的工作本身没有交集，那就代表sql语句肯定是无法复用的。而且对sql的修改，就代表着对java文件的修改，需要重新编译和打包部署（比如常见的状态值更改，sql修改随着业务变化必然存在修改）。 mybatis将sql统一存取到xml中，就算存在业务交叉，但因为统一配置的缘故，sql在xml中一目了然，两个跨team的程序员可以看到对方的sql，来判断自己是否需要重用。并且使用xml配置可以减少代码编译。 还有就是在java中拼写长sql太恶心了。 3：参数和结果集映射sql的方式需要传入参数，如果存在多条件“或类型”的查询（列表查询的查询条件允许空），那就代表你必须传参进行sql拼接，就算使用xml的方式也不行。要么每个业务独立配置xml中的sql，要么还是写入java代码中，或者以工具的方式进行自动拼接。 Mybatis使用映射的方式，方便model管理参数，同时以解析器的方式将参数动态拼接到sql（sqlmaper里那些标签），由于是model映射，连查询结果都可以统一映射，方便取出和运算。而且mybatis对查询结果集进行了缓存处理，使得重复查询进一步进行了优化。 4：对多重复sql进行复用封装比如模板方法，将常用sql模块化，直接调用。比如通用的save和getID之类的，只有表名和字段名有变化。 转载地址：http://www.cnblogs.com/zhuwoyao88/p/6549535.html 感谢原作者🙏","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"http://yoursite.com/tags/分布式缓存/"}]},{"title":"JAVA多线程构件（java.util.concurrent包下高级工具）","slug":"Java多线程构件","date":"2016-10-06T03:34:54.000Z","updated":"2017-08-25T10:44:39.000Z","comments":true,"path":"2016/10/06/Java多线程构件/","link":"","permalink":"http://yoursite.com/2016/10/06/Java多线程构件/","excerpt":"Java1.5提供了一个非常高效实用的多线程包：java.util.concurrent, 提供了大量高级工具，可以帮助开发者编写高效、易维护、结构清晰的Java多线程程序。这篇文章对java.util.concurrent中的高级工具进行总结和梳理。","text":"Java1.5提供了一个非常高效实用的多线程包：java.util.concurrent, 提供了大量高级工具，可以帮助开发者编写高效、易维护、结构清晰的Java多线程程序。这篇文章对java.util.concurrent中的高级工具进行总结和梳理。 原文地址：http://janeky.iteye.com/blog/769965，在此向原作者表示感谢 1. CountDownLatch我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。 用给定的计数 初始化 CountDownLatch。由于调用了 countDown() 方法，所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。这种现象只出现一次——计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。” 这就是说，CountDownLatch可以用来管理一组相关的线程执行，只需在主线程中调用CountDownLatch 的await方法（一直阻塞），让各个线程调用countDown方法。当所有的线程都只需完countDown了，await也顺利返回，不再阻塞了。在这样情况下尤其适用：将一个任务分成若干线程执行，等到所有线程执行完，再进行汇总处理。 下面我举一个非常简单的例子。假设我们要打印1-100，最后再输出“Ok“。1-100的打印顺序不要求统一，只需保证“Ok“是在最后出现即可。 解决方案：我们定义一个CountDownLatch，然后开10个线程分别打印（n-1）10+1至（n-1）10+10。主线程中调用await方法等待所有线程的执行完毕，每个线程执行完毕后都调用countDown方法。最后再await返回后打印“Ok”。 具体代码如下（本代码参考了JDK示例代码）： import java.util.concurrent.CountDownLatch; /** * 示例：CountDownLatch的使用举例 * Mail: ken@iamcoding.com * @author janeky */ public class TestCountDownLatch { private static final int N = 10; public static void main(String[] args) throws InterruptedException { CountDownLatch doneSignal = new CountDownLatch(N); CountDownLatch startSignal = new CountDownLatch(1);//开始执行信号 for (int i = 1; i &lt;= N; i++) { new Thread(new Worker(i, doneSignal, startSignal)).start();//线程启动了 } System.out.println(&quot;begin------------&quot;); startSignal.countDown();//开始执行啦 doneSignal.await();//等待所有的线程执行完毕 System.out.println(&quot;Ok&quot;); } static class Worker implements Runnable { private final CountDownLatch doneSignal; private final CountDownLatch startSignal; private int beginIndex; Worker(int beginIndex, CountDownLatch doneSignal, CountDownLatch startSignal) { this.startSignal = startSignal; this.beginIndex = beginIndex; this.doneSignal = doneSignal; } public void run() { try { startSignal.await(); //等待开始执行信号的发布 beginIndex = (beginIndex - 1) * 10 + 1; for (int i = beginIndex; i &lt;= beginIndex + 10; i++) { System.out.println(i); } } catch (InterruptedException e) { e.printStackTrace(); } finally { doneSignal.countDown(); } } } } 总结：CounDownLatch对于管理一组相关线程非常有用。上述示例代码中就形象地描述了两种使用情况。第一种是计算器为1，代表了两种状态，开关。第二种是计数器为N，代表等待N个操作完成。今后我们在编写多线程程序时，可以使用这个构件来管理一组独立线程的执行。 2. CyclicBarrier我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。 CyclicBarrier 支持一个可选的 Runnable 命令，在一组线程中的最后一个线程到达之后（但在释放所有线程之前），该命令只在每个屏障点运行一次。若在继续所有参与线程之前更新共享状态，此屏障操作 很有用。 我们在学习CountDownLatch的时候就提到了CyclicBarrier。两者究竟有什么联系呢？引用[JCIP]中的描述“The key difference is that with a barrier, all the threads must come together at a barrier point at the same time in order to proceed. Latches are for waiting for events; barriers are for waiting for other threads。CyclicBarrier等待所有的线程一起完成后再执行某个动作。这个功能CountDownLatch也同样可以实现。但是CountDownLatch更多时候是在等待某个事件的发生。在CyclicBarrier中，所有的线程调用await方法，等待其他线程都执行完。 举一个很简单的例子，今天晚上我们哥们4个去Happy。就互相通知了一下：晚上八点准时到xx酒吧门前集合，不见不散！。有个哥们住的近，早早就到了。有的事务繁忙，刚好踩点到了。无论怎样，先来的都不能独自行动，只能等待所有人 代码如下（参考了网上给的一些教程） import java.util.Random; import java.util.concurrent.BrokenBarrierException; import java.util.concurrent.CyclicBarrier; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class TestCyclicBarrier { public static void main(String[] args) { ExecutorService exec = Executors.newCachedThreadPool(); final Random random=new Random(); final CyclicBarrier barrier=new CyclicBarrier(4,new Runnable(){ @Override public void run() { System.out.println(&quot;大家都到齐了，开始happy去&quot;); }}); for(int i=0;i&lt;4;i++){ exec.execute(new Runnable(){ @Override public void run() { try { Thread.sleep(random.nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+&quot;到了，其他哥们呢&quot;); try { barrier.await();//等待其他哥们 } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }}); } exec.shutdown(); } } 关于await方法要特别注意一下，它有可能在阻塞的过程中由于某些原因被中断 总结：CyclicBarrier就是一个栅栏，等待所有线程到达后再执行相关的操作。barrier 在释放等待线程后可以重用。 3. Semaphore我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “一个计数信号量。从概念上讲，信号量维护了一个许可集。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release() 添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore 只对可用许可的号码进行计数，并采取相应的行动。” 我们一般用它来控制某个对象的线程访问对象 例如，对于某个容器，我们规定，最多只能容纳n个线程同时操作使用信号量来模拟实现 具体代码如下（参考 [JCIP]） import java.util.Collections; import java.util.HashSet; import java.util.Set; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Semaphore; public class TestSemaphore { public static void main(String[] args) { ExecutorService exec = Executors.newCachedThreadPool(); TestSemaphore t = new TestSemaphore(); final BoundedHashSet&lt;String&gt; set = t.getSet(); for (int i = 0; i &lt; 3; i++) {//三个线程同时操作add exec.execute(new Runnable() { public void run() { try { set.add(Thread.currentThread().getName()); } catch (InterruptedException e) { e.printStackTrace(); } } }); } for (int j = 0; j &lt; 3; j++) {//三个线程同时操作remove exec.execute(new Runnable() { public void run() { set.remove(Thread.currentThread().getName()); } }); } exec.shutdown(); } public BoundedHashSet&lt;String&gt; getSet() { return new BoundedHashSet&lt;String&gt;(2);//定义一个边界约束为2的线程 } class BoundedHashSet&lt;T&gt; { private final Set&lt;T&gt; set; private final Semaphore semaphore; public BoundedHashSet(int bound) { this.set = Collections.synchronizedSet(new HashSet&lt;T&gt;()); this.semaphore = new Semaphore(bound, true); } public void add(T o) throws InterruptedException { semaphore.acquire();//信号量控制可访问的线程数目 set.add(o); System.out.printf(&quot;add:%s%n&quot;,o); } public void remove(T o) { if (set.remove(o)) semaphore.release();//释放掉信号量 System.out.printf(&quot;remove:%s%n&quot;,o); } } } 总结：Semaphore通常用于对象池的控制 4．FutureTask我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “取消的异步计算。利用开始和取消计算的方法、查询计算是否完成的方法和获取计算结果的方法，此类提供了对 Future 的基本实现。仅在计算完成时才能获取结果；如果计算尚未完成，则阻塞 get 方法。一旦计算完成，就不能再重新开始或取消计算。可使用 FutureTask 包装 Callable 或 Runnable 对象。因为 FutureTask 实现了 Runnable，所以可将 FutureTask 提交给 Executor 执行。除了作为一个独立的类外，此类还提供了 protected 功能，这在创建自定义任务类时可能很有用。 “ 应用举例：我们的算法中有一个很耗时的操作，在编程的是，我们希望将它独立成一个模块，调用的时候当做它是立刻返回的，并且可以随时取消的 具体代码如下（参考 [JCIP]） import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.FutureTask; public class TestFutureTask { public static void main(String[] args) { ExecutorService exec=Executors.newCachedThreadPool(); FutureTask&lt;String&gt; task=new FutureTask&lt;String&gt;(new Callable&lt;String&gt;(){//FutrueTask的构造参数是一个Callable接口 @Override public String call() throws Exception { return Thread.currentThread().getName();//这里可以是一个异步操作 }}); try { exec.execute(task);//FutureTask实际上也是一个线程 String result=task.get();//取得异步计算的结果，如果没有返回，就会一直阻塞等待 System.out.printf(&quot;get:%s%n&quot;,result); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } } 总结：FutureTask其实就是新建了一个线程单独执行，使得线程有一个返回值，方便程序的编写 5. Exchanger我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “可以在pair中对元素进行配对和交换的线程的同步点。每个线程将条目上的某个方法呈现给 exchange 方法，与伙伴线程进行匹配，并且在返回时接收其伙伴的对象。Exchanger 可能被视为 SynchronousQueue 的双向形式。Exchanger 可能在应用程序（比如遗传算法和管道设计）中很有用。 “ 应用举例：有两个缓存区，两个线程分别向两个缓存区fill和take，当且仅当一个满了，两个缓存区交换 代码如下（参考了网上给的示例 http://hi.baidu.com/webidea/blog/item/2995e731e53ad5a55fdf0e7d.html） import java.util.ArrayList; import java.util.concurrent.Exchanger; public class TestExchanger { public static void main(String[] args) { final Exchanger&lt;ArrayList&lt;Integer&gt;&gt; exchanger = new Exchanger&lt;ArrayList&lt;Integer&gt;&gt;(); final ArrayList&lt;Integer&gt; buff1 = new ArrayList&lt;Integer&gt;(10); final ArrayList&lt;Integer&gt; buff2 = new ArrayList&lt;Integer&gt;(10); new Thread(new Runnable() { @Override public void run() { ArrayList&lt;Integer&gt; buff = buff1; try { while (true) { if (buff.size() &gt;= 10) { buff = exchanger.exchange(buff);//开始跟另外一个线程交互数据 System.out.println(&quot;exchange buff1&quot;); buff.clear(); } buff.add((int)(Math.random()*100)); Thread.sleep((long)(Math.random()*1000)); } } catch (InterruptedException e) { e.printStackTrace(); } } }).start(); new Thread(new Runnable(){ @Override public void run() { ArrayList&lt;Integer&gt; buff=buff2; while(true){ try { for(Integer i:buff){ System.out.println(i); } Thread.sleep(1000); buff=exchanger.exchange(buff);//开始跟另外一个线程交换数据 System.out.println(&quot;exchange buff2&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } }}).start(); } } 总结：Exchanger在特定的使用场景比较有用（两个伙伴线程之间的数据交互） 6. ScheduledThreadPoolExecutor我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “可另行安排在给定的延迟后运行命令，或者定期执行命令。需要多个辅助线程时，或者要求 ThreadPoolExecutor 具有额外的灵活性或功能时，此类要优于 Timer。 一旦启用已延迟的任务就执行它，但是有关何时启用，启用后何时执行则没有任何实时保证。按照提交的先进先出 (FIFO) 顺序来启用那些被安排在同一执行时间的任务。 虽然此类继承自 ThreadPoolExecutor，但是几个继承的调整方法对此类并无作用。特别是，因为它作为一个使用 corePoolSize 线程和一个无界队列的固定大小的池，所以调整 maximumPoolSize 没有什么效果。” 在JDK1.5之前，我们关于定时/周期操作都是通过Timer来实现的。但是Timer有以下几种危险[JCIP] a. Timer是基于绝对时间的。容易受系统时钟的影响。 b. Timer只新建了一个线程来执行所有的TimeTask。所有TimeTask可能会相关影响 c. Timer不会捕获TimerTask的异常，只是简单地停止。这样势必会影响其他TimeTask的执行。 如果你是使用JDK1.5以上版本，建议用ScheduledThreadPoolExecutor代替Timer。它基本上解决了上述问题。它采用相对时间，用线程池来执行TimerTask，会出来TimerTask异常。 下面通过一个简单的实例来阐述ScheduledThreadPoolExecutor的使用。 我们定期让定时器抛异常 我们定期从控制台打印系统时间 代码如下（参考了网上的一些代码，在此表示感谢） import java.util.concurrent.ScheduledThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class TestScheduledThreadPoolExecutor { public static void main(String[] args) { ScheduledThreadPoolExecutor exec=new ScheduledThreadPoolExecutor(1); exec.scheduleAtFixedRate(new Runnable(){//每隔一段时间就触发异常 @Override public void run() { throw new RuntimeException(); }}, 1000, 5000, TimeUnit.MILLISECONDS); exec.scheduleAtFixedRate(new Runnable(){//每隔一段时间打印系统时间，证明两者是互不影响的 @Override public void run() { System.out.println(System.nanoTime()); }}, 1000, 2000, TimeUnit.MILLISECONDS); } } 总结：是时候把你的定时器换成 ScheduledThreadPoolExecutor了 7.BlockingQueue“支持两个附加操作的 Queue，这两个操作是：获取元素时等待队列变为非空，以及存储元素时等待空间变得可用。“ 这里我们主要讨论BlockingQueue的最典型实现：LinkedBlockingQueue 和ArrayBlockingQueue。两者的不同是底层的数据结构不够，一个是链表，另外一个是数组。 后面将要单独解释其他类型的BlockingQueue和SynchronousQueue BlockingQueue的经典用途是 生产者-消费者模式 代码如下： import java.util.Random; import java.util.concurrent.BlockingQueue; import java.util.concurrent.LinkedBlockingQueue; public class TestBlockingQueue { public static void main(String[] args) { final BlockingQueue&lt;Integer&gt; queue=new LinkedBlockingQueue&lt;Integer&gt;(3); final Random random=new Random(); class Producer implements Runnable{ @Override public void run() { while(true){ try { int i=random.nextInt(100); queue.put(i);//当队列达到容量时候，会自动阻塞的 if(queue.size()==3) { System.out.println(&quot;full&quot;); } } catch (InterruptedException e) { e.printStackTrace(); } } } } class Consumer implements Runnable{ @Override public void run() { while(true){ try { queue.take();//当队列为空时，也会自动阻塞 Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } } new Thread(new Producer()).start(); new Thread(new Consumer()).start(); } } 总结：BlockingQueue使用时候特别注意take 和 put 8. DelayQueue我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “它是包含Delayed 元素的一个无界阻塞队列，只有在延迟期满时才能从中提取元素。该队列的头部 是延迟期满后保存时间最长的 Delayed 元素。如果延迟都还没有期满，则队列没有头部，并且 poll 将返回 null。当一个元素的 getDelay(TimeUnit.NANOSECONDS) 方法返回一个小于等于 0 的值时，将发生到期。即使无法使用 take 或 poll 移除未到期的元素，也不会将这些元素作为正常元素对待。例如，size 方法同时返回到期和未到期元素的计数。此队列不允许使用 null 元素。” 在现实生活中，很多DelayQueue的例子。就拿上海的SB会来说明，很多国家地区的开馆时间不同。你很早就来到园区，然后急急忙忙地跑到一些心仪的馆区，发现有些还没开，你吃了闭门羹。 仔细研究DelayQueue，你会发现它其实就是一个PriorityQueue的封装（按照delay时间排序），里面的元素都实现了Delayed接口，相关操作需要判断延时时间是否到了。 在实际应用中，有人拿它来管理跟实际相关的缓存、session等 下面我就通过 “上海SB会的例子来阐述DelayQueue的用法” 代码如下： import java.util.Random; import java.util.concurrent.DelayQueue; import java.util.concurrent.Delayed; import java.util.concurrent.TimeUnit; public class TestDelayQueue { private class Stadium implements Delayed { long trigger; public Stadium(long i){ trigger=System.currentTimeMillis()+i; } @Override public long getDelay(TimeUnit arg0) { long n=trigger-System.currentTimeMillis(); return n; } @Override public int compareTo(Delayed arg0) { return (int)(this.getDelay(TimeUnit.MILLISECONDS)-arg0.getDelay(TimeUnit.MILLISECONDS)); } public long getTriggerTime(){ return trigger; } } public static void main(String[] args)throws Exception { Random random=new Random(); DelayQueue&lt;Stadium&gt; queue=new DelayQueue&lt;Stadium&gt;(); TestDelayQueue t=new TestDelayQueue(); for(int i=0;i&lt;5;i++){ queue.add(t.new Stadium(random.nextInt(30000))); } Thread.sleep(2000); while(true){ Stadium s=queue.take();//延时时间未到就一直等待 if(s!=null){ System.out.println(System.currentTimeMillis()-s.getTriggerTime());//基本上是等于0 } if(queue.size()==0) break; } } } 总结：适用于需要延时操作的队列管理 9. SynchronousQueue我们先来学习一下JDK1.5 API中关于这个类的详细介绍： “一种阻塞队列，其中每个插入操作必须等待另一个线程的对应移除操作 ，反之亦然。同步队列没有任何内部容量，甚至连一个队列的容量都没有。不能在同步队列上进行 peek，因为仅在试图要移除元素时，该元素才存在；除非另一个线程试图移除某个元素，否则也不能（使用任何方法）插入元素；也不能迭代队列，因为其中没有元素可用于迭代。队列的头 是尝试添加到队列中的首个已排队插入线程的元素；如果没有这样的已排队线程，则没有可用于移除的元素并且 poll() 将会返回 null。对于其他 Collection 方法（例如 contains），SynchronousQueue 作为一个空 collection。此队列不允许 null 元素。 同步队列类似于 CSP 和 Ada 中使用的 rendezvous 信道。它非常适合于传递性设计，在这种设计中，在一个线程中运行的对象要将某些信息、事件或任务传递给在另一个线程中运行的对象，它就必须与该对象同步。 “ 看起来很有意思吧。队列竟然是没有内部容量的。这个队列其实是BlockingQueue的一种实现。每个插入操作必须等待另一个线程的对应移除操作，反之亦然。它给我们提供了在线程之间交换单一元素的极轻量级方法 应用举例：我们要在多个线程中传递一个变量。 代码如下（其实就是生产者消费者模式） import java.util.Arrays; import java.util.List; import java.util.concurrent.BlockingQueue; import java.util.concurrent.SynchronousQueue; public class TestSynchronousQueue { class Producer implements Runnable { private BlockingQueue&lt;String&gt; queue; List&lt;String&gt; objects = Arrays.asList(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;); public Producer(BlockingQueue&lt;String&gt; q) { this.queue = q; } @Override public void run() { try { for (String s : objects) { queue.put(s);// 产生数据放入队列中 System.out.printf(&quot;put:%s%n&quot;,s); } queue.put(&quot;Done&quot;);// 已完成的标志 } catch (InterruptedException e) { e.printStackTrace(); } } } class Consumer implements Runnable { private BlockingQueue&lt;String&gt; queue; public Consumer(BlockingQueue&lt;String&gt; q) { this.queue = q; } @Override public void run() { String obj = null; try { while (!((obj = queue.take()).equals(&quot;Done&quot;))) { System.out.println(obj);//从队列中读取对象 Thread.sleep(3000); //故意sleep，证明Producer是put不进去的 } } catch (InterruptedException e) { e.printStackTrace(); } } } public static void main(String[] args) { BlockingQueue&lt;String&gt; q=new SynchronousQueue&lt;String&gt;(); TestSynchronousQueue t=new TestSynchronousQueue(); new Thread(t.new Producer(q)).start(); new Thread(t.new Consumer(q)).start(); } } 总结：SynchronousQueue主要用于单个元素在多线程之间的传递","categories":[],"tags":[{"name":"并发相关","slug":"并发相关","permalink":"http://yoursite.com/tags/并发相关/"}]},{"title":"《深入理解JVM》阅读笔记以及问题整理","slug":"深入理解JVM阅读笔记以及问题整理","date":"2016-09-17T02:05:54.000Z","updated":"2017-08-18T09:24:44.000Z","comments":true,"path":"2016/09/17/深入理解JVM阅读笔记以及问题整理/","link":"","permalink":"http://yoursite.com/2016/09/17/深入理解JVM阅读笔记以及问题整理/","excerpt":"对阅读周志明先生的《深入理解JVM》产生的疑问与感悟以及要点进行总结。想这种技术书应该反复读，最近又阅览了一次，才对GC部分有了一个大概的框架，可是细节部分依然记不清楚。还是需要再读，并对类加载以及并发从头进行学习。","text":"对阅读周志明先生的《深入理解JVM》产生的疑问与感悟以及要点进行总结。想这种技术书应该反复读，最近又阅览了一次，才对GC部分有了一个大概的框架，可是细节部分依然记不清楚。还是需要再读，并对类加载以及并发从头进行学习。 什么是Native方法本地方法栈是存放native函数的，可是什么是native函数呢？百度之： 参考：http://blog.csdn.net/funneies/article/details/8949660 native关键字说明其修饰的方法是一个原生态方法，方法对应的实现不是在当前文件，而是在用其他语言（如C和C++）实现的文件中。Java语言本身不能对操作系统底层进行访问和操作，但是可以通过JNI接口调用其他语言来实现对底层的访问。 JNI是Java本机接口（Java Native Interface），是一个本机编程接口，它是Java软件开发工具箱（java Software Development Kit，SDK）的一部分。JNI允许Java代码使用以其他语言编写的代码和代码库。Invocation API（JNI的一部分）可以用来将Java虚拟机（JVM）嵌入到本机应用程序中，从而允许程序员从本机代码内部调用Java代码。","categories":[],"tags":[{"name":"JVM相关","slug":"JVM相关","permalink":"http://yoursite.com/tags/JVM相关/"}]},{"title":"ThreadLocal详解","slug":"ThreadLocal详解","date":"2016-09-09T09:37:31.000Z","updated":"2017-08-25T10:22:51.000Z","comments":true,"path":"2016/09/09/ThreadLocal详解/","link":"","permalink":"http://yoursite.com/2016/09/09/ThreadLocal详解/","excerpt":"","text":"ThreadLocal与synchronizedsynchronized这类线程同步的机制可以解决多线程并发问题，在这种解决方案下，多个线程访问到的，都是同一份变量的内容。为了防止在多线程访问的过程中，可能会出现的并发错误。不得不对多个线程的访问进行同步，这样也就意味着，多个线程必须先后对变量的值进行访问或者修改，这是一种以延长访问时间来换取线程安全性的策略。 而ThreadLocal类为每一个线程都维护了自己独有的变量拷贝。每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了，那就没有任何必要对这些线程进行同步，它们也能最大限度的由CPU调度，并发执行。并且由于每个线程在访问该变量时，读取和修改的，都是自己独有的那一份变量拷贝，变量被彻底封闭在每个访问的线程中，并发错误出现的可能也完全消除了。对比前一种方案，这是一种以空间来换取线程安全性的策略。 来看一个运用ThreadLocal来实现数据库连接Connection对象线程隔离的例子。 import java.sql.Connection; import java.sql.DriverManager; import java.sql.SQLException; public class ConnectionManager { private static ThreadLocal&lt;Connection&gt; connectionHolder = new ThreadLocal&lt;Connection&gt;() { @Override protected Connection initialValue() { Connection conn = null; try { conn = DriverManager.getConnection( &quot;jdbc:mysql://localhost:3306/test&quot;, &quot;username&quot;, &quot;password&quot;); } catch (SQLException e) { e.printStackTrace(); } return conn; } }; public static Connection getConnection() { return connectionHolder.get(); } public static void setConnection(Connection conn) { connectionHolder.set(conn); } } 通过调用ConnectionManager.getConnection()方法，每个线程获取到的，都是和当前线程绑定的那个Connection对象，第一次获取时，是通过initialValue()方法的返回值来设置值的。通过ConnectionManager.setConnection(Connection conn)方法设置的Connection对象，也只会和当前线程绑定。这样就实现了Connection对象在多个线程中的完全隔离。在Spring容器中管理多线程环境下的Connection对象时，采用的思路和以上代码非常相似。 那么到底ThreadLocal类是如何实现这种“为每个线程提供不同的变量拷贝”的呢？先来看一下ThreadLocal的set()方法的源码是如何实现的： /** * Sets the current thread&apos;s copy of this thread-local variable * to the specified value. Most subclasses will have no need to * override this method, relying solely on the {@link #initialValue} * method to set the values of thread-locals. * * @param value the value to be stored in the current thread&apos;s copy of * this thread-local. */ public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value) ; } 没有什么魔法，在这个方法内部我们看到，首先通过getMap(Thread t)方法获取一个和当前线程相关的ThreadLocalMap，然后将变量的值设置到这个ThreadLocalMap对象中，当然如果获取到的ThreadLocalMap对象为空，就通过createMap方法创建。 线程隔离的秘密，就在于ThreadLocalMap这个类。ThreadLocalMap是ThreadLocal类的一个静态内部类，它实现了键值对的设置和获取（对比Map对象来理解），每个线程中都有一个独立的ThreadLocalMap副本，它所存储的值，只能被当前线程读取和修改。ThreadLocal类通过操作每一个线程特有的ThreadLocalMap副本，从而实现了变量访问在不同线程中的隔离。因为每个线程的变量都是自己特有的，完全不会有并发错误。还有一点就是，ThreadLocalMap存储的键值对中的键是this对象指向的ThreadLocal对象，而值就是你所设置的对象了。 为了加深理解，我们接着看上面代码中出现的getMap和createMap方法的实现： ThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 代码已经说的非常直白，就是获取和设置Thread内的一个叫threadLocals的变量，而这个变量的类型就是ThreadLocalMap，这样进一步验证了上文中的观点：每个线程都有自己独立的ThreadLocalMap对象。打开java.lang.Thread类的源代码，我们能得到更直观的证明： /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; 那么接下来再看一下ThreadLocal类中的get()方法，代码是这么说的： /** * Returns the value in the current thread&apos;s copy of this * thread-local variable. If the variable has no value for the * current thread, it is first initialized to the value returned * by an invocation of the {@link #initialValue} method. * * @return the current thread&apos;s value of this thread-local */ public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; } return setInitialValue(); } /** * Variant of set() to establish initialValue. Used instead * of set() in case user has overridden the set() method. * * @return the initial value */ private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value) ; return value; } 这两个方法的代码告诉我们，在获取和当前线程绑定的值时，ThreadLocalMap对象是以this指向的ThreadLocal对象为键进行查找的，这当然和前面set()方法的代码是相呼应的。 进一步地，我们可以创建不同的ThreadLocal实例来实现多个变量在不同线程间的访问隔离，为什么可以这么做？因为不同的ThreadLocal对象作为不同键，当然也可以在线程的ThreadLocalMap对象中设置不同的值了。通过ThreadLocal对象，在多线程中共享一个值和多个值的区别，就像你在一个HashMap对象中存储一个键值对和多个键值对一样，仅此而已。 设置到这些线程中的隔离变量，会不会导致内存泄漏呢？ThreadLocalMap对象保存在Thread对象中，当某个线程终止后，存储在其中的线程隔离的变量，也将作为Thread实例的垃圾被回收掉，所以完全不用担心内存泄漏的问题。在多个线程中隔离的变量，光荣的生，合理的死，真是圆满，不是么？ 最后再提一句，ThreadLocal变量的这种隔离策略，也不是任何情况下都能使用的。如果多个线程并发访问的对象实例只允许，也只能创建那么一个，那就没有别的办法了，老老实实的使用同步机制来访问吧。","categories":[],"tags":[{"name":"并发相关","slug":"并发相关","permalink":"http://yoursite.com/tags/并发相关/"}]},{"title":"电商项目物流接口集成总结与开源分享","slug":"电商项目物流接口集成总结与开源分享","date":"2016-07-11T12:20:54.000Z","updated":"2017-08-18T09:24:36.000Z","comments":true,"path":"2016/07/11/电商项目物流接口集成总结与开源分享/","link":"","permalink":"http://yoursite.com/2016/07/11/电商项目物流接口集成总结与开源分享/","excerpt":"迄今为止做了两个电商项目，在物流对接部分总结了一些东西，将可以通用的东西贴出来，供有需要的同学参考。可以少走很多弯路，避一些坑。 在做的过程中遇到很多坑，但是当时没有即时进行记录，只有这个调好了的最终版本。包括顺丰，圆通，EMS，德邦，申通五家物流公司，每家物流公司大致上需要調的接口有下物流订单，物流记录回传，打印物流电子面单，物流地址信息实时查询以及物流地址信息主动接收等接口。","text":"迄今为止做了两个电商项目，在物流对接部分总结了一些东西，将可以通用的东西贴出来，供有需要的同学参考。可以少走很多弯路，避一些坑。 在做的过程中遇到很多坑，但是当时没有即时进行记录，只有这个调好了的最终版本。包括顺丰，圆通，EMS，德邦，申通五家物流公司，每家物流公司大致上需要調的接口有下物流订单，物流记录回传，打印物流电子面单，物流地址信息实时查询以及物流地址信息主动接收等接口。 github物流对接服务开源地址：https://github.com/yuaman/logistics-service","categories":[],"tags":[{"name":"个人开源项目","slug":"个人开源项目","permalink":"http://yoursite.com/tags/个人开源项目/"}]},{"title":"消息队列两种模式：点对点与发布订阅","slug":"消息队列两种模式：点对点与发布订阅","date":"2016-06-09T09:37:31.000Z","updated":"2017-08-25T10:37:22.000Z","comments":true,"path":"2016/06/09/消息队列两种模式：点对点与发布订阅/","link":"","permalink":"http://yoursite.com/2016/06/09/消息队列两种模式：点对点与发布订阅/","excerpt":"","text":"Java消息服务（JavaMessage Service，JMS）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。点对点与发布订阅最初是由JMS定义的。这两种模式主要区别或解决的问题就是发送到队列的消息能否重复消费(多订阅) 1、定义JMS规范目前支持两种消息模型：点对点（point to point， queue）和发布/订阅（publish/subscribe，topic）。 1.1、点对点：Queue，不可重复消费消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 1.2、发布/订阅：Topic，可以重复消费消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。支持订阅组的发布订阅模式：发布订阅模式下，当发布者消息量很大时，显然单个订阅者的处理能力是不足的。实际上现实场景中是多个订阅者节点组成一个订阅组负载均衡消费topic消息即分组订阅，这样订阅者很容易实现消费能力线性扩展。可以看成是一个topic下有多个Queue，每个Queue是点对点的方式，Queue之间是发布订阅方式。 2、区别2.1、点对点模式生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有 一个可用的消费者，所以Queue实现了一个可靠的负载均衡。 2.2、发布订阅模式发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。 3、流行模型比较传统企业型消息队列ActiveMQ遵循了JMS规范，实现了点对点和发布订阅模型，但其他流行的消息队列RabbitMQ、Kafka并没有遵循JMS规范。 3.1、RabbitMQRabbitMQ实现了AQMP协议，AQMP协议定义了消息路由规则和方式。生产端通过路由规则发送消息到不同queue，消费端根据queue名称消费消息。RabbitMQ既支持内存队列也支持持久化队列，消费端为推模型，消费状态和订阅关系由服务端负责维护，消息消费完后立即删除，不保留历史消息。 ###（1）点对点生产端发送一条消息通过路由投递到Queue，只有一个消费者能消费到。 ###（2）多订阅当RabbitMQ需要支持多订阅时，发布者发送的消息通过路由同时写到多个Queue，不同订阅组消费不同的Queue。所以支持多订阅时，消息会多个拷贝。 3.2、KafkaKafka只支持消息持久化，消费端为拉模型，消费状态和订阅关系由客户端端负责维护，消息消费完后不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了。但是可能产生重复消费的情况。 ###（1）点对点&amp;多订阅发布者生产一条消息到topic中，不同订阅组消费此消息。","categories":[],"tags":[{"name":"分布式消息","slug":"分布式消息","permalink":"http://yoursite.com/tags/分布式消息/"}]},{"title":"读《罗马人物语-恺撒时代》","slug":"读《罗马人物语-恺撒时代》","date":"2016-06-04T14:54:57.000Z","updated":"2017-08-25T10:46:54.000Z","comments":true,"path":"2016/06/04/读《罗马人物语-恺撒时代》/","link":"","permalink":"http://yoursite.com/2016/06/04/读《罗马人物语-恺撒时代》/","excerpt":"这周末抽出时间看了盐野七生的恺撒战记，基本上是百度百科“恺撒”词条的详细版。或许是年龄大了，不再像以前读传记那么有代入感容易激动。","text":"这周末抽出时间看了盐野七生的恺撒战记，基本上是百度百科“恺撒”词条的详细版。或许是年龄大了，不再像以前读传记那么有代入感容易激动。 历史纪实性小说或许在专业人士看来，不严谨不入流，有的地方啰嗦。但我的体会是，历史纪实性小说能够让身处现代的人有更强的代入感，能够更深刻对历史上发生的情境感同身受，比一开始读专业的史书会更加给人以启发。当然，类似这种纪实性小说体裁作品通常篇幅很多，读完一遍基本差不多了。以后再读相关历史，最好还是找相应历史人物及事件的权威史书，简洁明了，更多感悟思考。类似孙皓晖的《大秦帝国》，记得一共是五百六十万字左右，14年初的时候先是在手机上看后来买书来看，看的昏天黑地，时不时泪流满面，有时间可以聊聊其中一些故事。其中固然有啰嗦以及根据可考真实历史发挥创作的地方，但更多是感动和震撼以及启发，可以说对我产生一定影响。这两年再没有那样的时候了。 鲁迅一有句诗“岂有豪情似旧时，花开花落两由之“，初读只觉被吸引，这两天体会颇深：当年无知者无畏，敢说敢拚敢闯，“尬点”极低，不在乎。如今经历过世事维艰后，实在难回复从前心境。 想起不久前一位女同事说我“你平时总是看起来很累，是不是早衰啊”，我无话，心里只有一入江湖岁月摧之感，可风云却是还不知道出自哪一辈。嬴政二十一岁早就立志要继往开来成就八百年没有过的奇功伟业；刘邦二十一岁估计还在家乡游手好闲；霍去病二十二岁北驱匈奴，燕然勒功；曹操二十岁时靠家里当上了首都的一个区公安局局长，壮志踌躇，要做大汉朝治世之名臣，一顿棒子打死蹇硕的叔叔，最终被迫回老家；李世民官二代 不说了；赵匡胤估计二十一岁也还是在江湖上游荡；朱重八可能刚刚离开寺庙，开始五年的乞讨流浪；蒋介石这时候是在上海滩炒股还是搞暗杀来着；太祖貌似是刚从北大图书馆回来或者是在长沙的图书馆看书。地球上有过一千亿人存在，也还将有一千亿人存在。我是其中一个。何去何从？随波逐流吗？ 大琐罗亚斯有过这么一句晦涩的话：来如流水昔逝如斯飘飘入世如水之不得不流不知何故来也不知何所终。 最近入了一套资治通鉴，一眼望过去就是四个字：沉闷枯燥。可这就是中国的历史，兴衰更替，帝王将相们的一言一行。想起太祖一句词“一篇读罢头飞雪，但记得斑斑点点，几行陈迹”，中国的事儿或许就都在这么一套书里，几十个百年来你方唱罢我登场，演来演去都是差不多的剧情，差不多的套路，差不多的人心。但是这些大片实际拍出来动作戏等具象的东西却都是花样翻新。太祖的意思或许是读完这套书，再结合斗争实际和现实人生境遇反复读，等到真读懂读透，或者一生也就差不多了，但书里的东西曾经那么研究，咋一想也想不太起来。 说是读恺撒，先摘录一段盐野七生的话吧： 凯撒这样的男人是拒绝对他人怀有怨恨感情的。因为怨恨是对于自己实力相当的或是比自己地位更高的人才有的绝对优势地位有着充分的自负。当然要拒绝怨恨，这种所谓下等人才有的情感。 忘了是西方哪个人说过：一切伟大的人物最伟大的是恺撒。个人的体会是恺撒是个善于表演的人，是一个有着深刻自我修养的表演艺术家。他在深刻务实的同时也在深刻的务虚，故能成其大。因为纯粹的善良和宽容即使是在日常生活中也是要受到中伤的，更何况是在政治场中。而庞培之所以失败，或许也是败在“务实”上走了极端。 关于恺撒的传记我之前买过逻辑思维出的版本，感觉有些繁琐没时间读。读了盐野七生的版本，仍旧是懵懵懂懂。一直以来我最大的疑问以及最感兴趣的地方在于：恺撒三十岁后正式从政，四十岁开始声名鹊起，打了许多年仗固然是胜多败少极大地满足了当时的人的虚荣心，可能在古罗马时代伟大的人物的确是不少，论战功庞培的战功不见得比恺撒差，论魅力奥古斯都一样有魅力并且比凯撒还要帅，论历史影响屋大维才是真正的建国者，而且屋大维一样的有三头政治，一样的起于不利，一样的沙场所向披靡，为什么独独凯撒被西方世界铭记得最深刻？他身上到底有些什么东西让人着迷？可能不是西方人很难搞懂。也和我国相关历史研究和译作不多有关系。我最近对西方历史比较感兴趣，知己知彼百战不殆，未来是中国的时代，东西方兴衰交替的变革时代，应该对洋人的历史和文化和思维模式以及世界观、审美等有所了解。就书中了解到的情况看，那时候不管是东方的秦国还是西方的罗马，都是有着深刻的建立辉煌所必需的精神和物质条件的，换句话说，值得去了解以及思考以及学习。 因为是此书用kindle看的，时间紧张摘抄不易，以后有机会争取把精华做分享。盐野的文字还是比较浅显，中国人爱作小聪明，也看不惯日本人直白诚恳的表露，但就大多数人在基本历史常识的普及上，还是值得一览。 上文中有对历史人物的妄评，用《邺中歌》中此句作结： 古人做事无巨细，寂寞豪华皆有意。书生轻议冢中人，冢中笑尔书生气！ ————出自《三国演义》第七十八回 曹操死的那一回 得空陪母亲动物园一游，附图一张。其实是这几年来第一次和她一起出去玩，再往前依稀是我十岁左右在旅顺全家人经常一起出去玩。自从我几年前做出抉择要走自己的路，她心里愁苦不解，我时刻想着自己的事，父亲工作，我们都无心这些了。来大连这些年竟从未去过森林动物园看卡，近来原想的是自己漂泊不定，趁在的时候抽空走一走看一看，后来母亲说想要一起去，想请我去玩。我心里其实很欢喜，很期待，是那种久违的欢喜和期待，几年来未曾有。我知道，她是希望我心情好一些，放松放松，放下一些东西，其实自己未必想去。我则想着带着她一起，世事难料，免得将来有遗憾，眼下彼此能互相弥补一些是一些。用一句话来形容或许会让人很奇怪匪夷所思：渡尽劫波兄弟在，相逢一笑泯恩仇。你觉奇怪，我也觉奇怪，但心思确实可以用这句话来形容。 人间事千头万绪，苦思不得解。絮语惹人烦。 不说了，面已凉。 背影寥落的老虎在嘘嘘","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"Spring Boot核心原理－自动配置","slug":"SpringBoot核心原理——自动配置","date":"2016-05-23T09:37:31.000Z","updated":"2017-08-25T09:06:29.000Z","comments":true,"path":"2016/05/23/SpringBoot核心原理——自动配置/","link":"","permalink":"http://yoursite.com/2016/05/23/SpringBoot核心原理——自动配置/","excerpt":"","text":"之前在公司内部推行spring boot时，有同事跟我提到过，感觉换到spring boot这个框架后，好处是小白也能迅速上手写业务代码了。但是呢，这种情况下新手很容易写得云里雾里的，因为完全不知道背后的原理是什么，相对比在学习spring时需要深刻理解ioc、搞一堆繁琐的配置来说，的确缺少了被迫跳出舒适区去学习一些原理的过程，那么今天就讲讲，为什么spring boot能够如此简单的让我们迅速上手。 Spring由于其繁琐的配置，一度被人成为“配置地狱”，各种XML、Annotation配置，让人眼花缭乱，而且如果出错了也很难找出原因。Spring Boot项目就是为了解决配置繁琐的问题，最大化的实现convention over configuration(约定大于配置)。熟悉Ruby On Rails（ROR框架的程序员都知道，借助于ROR的脚手架工具只需简单的几步即可建立起一个Web应用程序。而Spring Boot就相当于Java平台上的ROR。 Spring boot出现之后，得益于“习惯优于配置”这个理念，再也没有繁琐的配置、难以集成的内容（大多数流行第三方技术都被集成在内）。 那么背后实现的核心原理到底是什么呢？ 其实是spring 4.x提供的基于条件配置bean的能力。 Spring boot关于自动配置的源码在spring-boot-autoconfigure-x.x.x.x.jar中，主要包含了如下图所示的配置（并未截全）： 我们可以在这里看见所有spring boot为我们做的自动配置。通过在application.properties中设置属性：debug=true，可以通过控制台的输出观察自动配置启动的情况：(以下有删减，建议自己运行一下看看) =========================AUTO-CONFIGURATION REPORT Positive matches: ———————— - @ConditionalOnClass classes found: org.springframework.web.servlet.DispatcherServlet (OnClassCondition) - found web application StandardServletEnvironment (OnWebApplicationCondition) Negative matches: ----------------- ActiveMQAutoConfiguration did not match - required @ConditionalOnClass classes not found: javax.jms.ConnectionFactory,org.apache.activemq.ActiveMQConnectionFactory (OnClassCondition) 运行原理 在第一次使用spring boot的时候，大家都会惊讶于@SpringBootApplication这个注解，有了它马上就能够让整个应用跑起来。实际上它只是一个组合注解，包含@Configuration、@EnableAutoConfiguration、@ComponentScan这三个注解。 @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Configuration @EnableAutoConfiguration @ComponentScan public @interface SpringBootApplication { Class&lt;?&gt;[] exclude() default {}; String[] excludeName() default {}; @AliasFor( annotation = ComponentScan.class, attribute = &quot;basePackages&quot; ) String[] scanBasePackages() default {}; @AliasFor( annotation = ComponentScan.class, attribute = &quot;basePackageClasses&quot; ) Class&lt;?&gt;[] scanBasePackageClasses() default {}; } 它的核心功能是由@EnableAutoConfiguration这个注解提供的，我们来看看@EnableAutoConfiguration的源代码： @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import({EnableAutoConfigurationImportSelector.class}) public @interface EnableAutoConfiguration { Class&lt;?&gt;[] exclude() default {}; String[] excludeName() default {}; } 这里的关键功能是@Import注解导入的配置功能EnableAutoConfigurationImportSelector使用SpringFactoriesLoader.loadFactoryNames方法来扫描具有META-INF/spring.factories文件的jar包，spring-boot-autoconfigure-x.x.x.x.jar里就有一个spring.factories文件，这个文件中声明了有哪些要自动配置。下面我们来分析一下spring boot autoconfigure里面的MongoAutoConfiguration（MongoDB的自动配置），相信你就会明白这套自动配置机制到底是怎么一回事儿： // // Source code recreated from a .class file by IntelliJ IDEA // (powered by Fernflower decompiler) // package org.springframework.boot.autoconfigure.mongo; import com.mongodb.MongoClient; import com.mongodb.MongoClientOptions; import java.net.UnknownHostException; import javax.annotation.PreDestroy; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.autoconfigure.condition.ConditionalOnClass; import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean; import org.springframework.boot.autoconfigure.mongo.MongoProperties; import org.springframework.boot.context.properties.EnableConfigurationProperties; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.core.env.Environment; @Configuration @ConditionalOnClass({MongoClient.class}) @EnableConfigurationProperties({MongoProperties.class}) //开启属性注入。 @ConditionalOnMissingBean( type = {&quot;org.springframework.data.mongodb.MongoDbFactory&quot;} ) public class MongoAutoConfiguration { @Autowired private MongoProperties properties; @Autowired( required = false ) private MongoClientOptions options; @Autowired private Environment environment; private MongoClient mongo; public MongoAutoConfiguration() { } @PreDestroy public void close() { if(this.mongo != null) { this.mongo.close(); } } @Bean //使用java配置，当容器中没有这个bean的时候执行初始化 @ConditionalOnMissingBean public MongoClient mongo() throws UnknownHostException { this.mongo = this.properties.createMongoClient(this.options, this.environment); return this.mongo; } } 首先这被@Configuration注解了，是一个配置类，当满足以下条件这个bean被装配： 当MongoClient在类路径下。 当容器中没有org.springframework.data.mongodb.MongoDbFactory这类bean的时候。 此外，我们可以看一下通过@EnableConfigurationProperties({MongoProperties.class}) 自动注入的属性（这是习惯优于配置的最终落地点）： @ConfigurationProperties( prefix = &quot;spring.data.mongodb&quot; ) public class MongoProperties { public static final int DEFAULT_PORT = 27017; private String host; private Integer port = null; private String uri = &quot;mongodb://localhost/test&quot;; private String database; private String authenticationDatabase; private String gridFsDatabase; private String username; private char[] password; private Class&lt;?&gt; fieldNamingStrategy; ...... } 所以在我们什么都不干的情况下，只需要引入spring-data-mongodb这个依赖再加上默认的MongoDB server我们就能够快速集成MongoDB，用MongodbTemplate访问数据库。 同时我们可以通过在application.yaml中修改spring.data.mongodb相关的参数就能够修改连接配置，如： spring: data: mongodb: host: localhost port: 27017 username: chingzhu password: test123 database: icekredit 利用这套原理，我们也可以轻松地把目前spring boot还未集成的、我们自己要使用的第三方技术自动集成起来。 现在，不知道你对spring boot的机制有一个清楚的认识了吗？ 附：常见org.springframework.boot.autoconfigure.condition包下的条件注解意思 @ConditionalOnBean：当容器里有指定的bean的条件下。 @ConditionalOnMissingBean：当容器里不存在指定bean的条件下。 @ConditionalOnClass：当类路径下有指定类的条件下。 @ConditionalOnMissingClass：当类路径下不存在指定类的条件下。 @ConditionalOnProperty：指定的属性是否有指定的值，比如@ConditionalOnProperties(prefix=”xxx.xxx”, value=”enable”, matchIfMissing=true)，代表当xxx.xxx为enable时条件的布尔值为true，如果没有设置的情况下也为true。 转载地址：http://blog.csdn.net/xiaobing_122613/article/details/54943448","categories":[],"tags":[{"name":"框架相关","slug":"框架相关","permalink":"http://yoursite.com/tags/框架相关/"}]},{"title":"谈谈Java中equals和==的区别和使用场景","slug":"Java中equals与==的区别与使用场景分析","date":"2016-05-09T04:44:54.000Z","updated":"2017-08-21T03:58:42.000Z","comments":true,"path":"2016/05/09/Java中equals与==的区别与使用场景分析/","link":"","permalink":"http://yoursite.com/2016/05/09/Java中equals与==的区别与使用场景分析/","excerpt":"讨论一下Java中equals和==的区别，这个问题看似浅显，还是有不少情况需要注意。阅览了网上一些文章，都比较片面。在此做一下详细的整理。","text":"讨论一下Java中equals和==的区别，这个问题看似浅显，还是有不少情况需要注意。阅览了网上一些文章，都比较片面。在此做一下详细的整理。 先直接上结论：：1.当比较对象为基本数据类型的时候，”==“比较的是二者在栈内存中的值。object默认的equals方法，是比较两个对象的引用是不是相同 2.当比较对象为复杂数据类型的时候，当且仅当该equals方法参数不是 null，两个变量的类型、内容都相同，比较结果为true。但string类有常量池的缘故较为特殊。 3.当比较对象为实体类的时候，不重写equals方法，采用object默认的equals方法，是比较两个对象的引用是不是相同。比较的是二者在堆内存中的引用地址，无意义，一般在实体类中进行重写equals方法，自定义比较规则。 java中的数据类型，可分为两类：1.基本数据类型，也称原始数据类型的比较。byte,short,char,int,long,float,double,boolean 他们之间的比较，应用双等号（==）,比较的是他们的值。 示例： public class Test { public static void main(String[] args) { int i=5; int j=5; if(i==j) System.out.println(&quot;i和j相等！&quot;); else System.out.println(&quot;不相等！&quot;); } 运行结果： “i和j相等！” 因为此时比较对象为基本数据类型，所以“==”比较的是它们存放于虚拟机栈内存中的值。 2.复杂数据类型的比较在Java API中，有些类重写了equals()方法，它们的比较规则是：当且仅当该equals方法参数不是 null，两个变量的类型、内容都相同，则比较结果为true。这些类包括：String、Double、Float、Long、Integer、Short、Byte、、Boolean、BigDecimal、BigInteger等等，太多太多了，但是常见的就这些了，具体可以查看API中类的equals()方法，就知道了。 深入到内存中。==就是比较堆内存的值是否相等（对象地址存放在堆内存），equals（）就是比较栈内存的值（对象的值存在于栈内存）。String有个常量池。String a=”abc”;String b=”abc”;a==b是返回true的，就是因为常量池的原因，实际上a和b是同一个对象。但是String a=”abc”;String a=new String(“abc”);这样a==b就是返回flase了，a和b就不是同一个对象（他们的地址不等。） 原来，程序在运行的时候会创建一个字符串缓冲池当使用 s2 = “Monday” 这样的表达是创建字符串的时候，程序首先会在这个String缓冲池中寻找相同值的对象，在第一个程序中，s1先被放到了池中，所以在s2被创建的时候，程序找到了具有相同值的 s1将s2引用s1所引用的对象”Monday”第二段程序中，使用了 new 操作符，他明白的告诉程序：”我要一个新的！不要旧的！”于是一个新的”Monday”Sting对象被创建在内存中。他们的值相同，但是位置不同，一个在池中游泳一个在岸边休息。哎呀，真是资源浪费，明明是一样的非要分开做什么呢？ 3.实体类的比较 当他们用（==）进行比较的时候，比较的是他们在内存中的存放地址，所以，除非是同一个new出来的对象，他们的比较后的结果为true，否则比较后结果为false。 JAVA当中所有的类都是继承于Object这个基类的，在Object中的基类中定义了一个equals的方法，这个方法的初始行为是比较对象的内存地 址。 对于复合数据类型之间进行equals比较，在没有覆写equals方法的情况下，他们之间的比较还是基于他们在内存中的存放位置的地址值的，因为Object的equals方法也是用双等号（==）进行比较的，所以比较后的结果跟双等号（==）的结果相同。 示例： public class Student { String name; public Student(){ } public Student(String name){ this.name=name; } public class Test { public static void main(String[] args) { Student s = new Student(&quot;BlueSky&quot;); Student s1=new Student(&quot;BlueSky&quot;); if(s==s1) System.out.println(&quot;s和是s1相等！&quot;); else System.out.println(&quot;s和是s1不相等！&quot;); if(s.equals(s1)) System.out.println(&quot;s和是s1相等！&quot;); else System.out.println(&quot;s和是s1不相等！&quot;); } } 运行结果：s和是s1不相等！s和是s1不相等！ 结果验证了Object类的equals()方法用来比较是否一个对象是利用内存地址比较，所以在定义一个类的时候，如果涉及到对象的比较（通过我们要比较内容），应该重写equals()方法。重写的一般规则是： 1、先用“==”判断是否相等。 2、判断equals()方法的参数是否为null，如果为null，则返回false；因为当前对象不可能为null，如果为null，则不能调用其equals()方法，否则抛java.lang.NullPointerException异常。 3、当参数不为null，则如果两个对象的运行时类（通过getClass()获取）不相等，返回false，否则继续判断。 4、判断类的成员是否对应相等。往下就随意发挥了。呵呵！ 我们对实体进行比较的时候往往要比较的是里面的值，所以我们为了达到这个目的，要在实体类里面重写equals()方法，进行对象里面的内容比较。如上面，我们在Student类中重写equals()方法。 重写equals()方法后再次进行比较： Student类： public class Student { String name; public Student(){ } public Student(String name){ this.name=name; } public boolean equals(Object obj) { if (this == obj) //传入的对象就是它自己，如s.equals(s)；肯定是相等的； return true; if (obj == null) //如果传入的对象是空，肯定不相等 return false; if (getClass() != obj.getClass()) //如果不是同一个类型的，如Studnet类和Animal类， //也不用比较了，肯定是不相等的 return false; Student other = (Student) obj; if (name == null) { if (other.name != null) return false; } else if (!name.equals(other.name)) //如果name属性相等，则相等 return false; return true; } } 测试类Test： public class Test { public static void main(String[] args) { Student s = new Student(&quot;BlueSky&quot;); Student s1=new Student(&quot;BlueSky&quot;); if(s.equals(s1)) System.out.println(&quot;s和是s1相等！&quot;); else System.out.println(&quot;s和是s1不相等！&quot;); } } 运行结果：“s和是s1相等！” 结论：1.当比较对象为基本数据类型的时候，”==“比较的是二者在栈内存中的值。 2.当比较对象为复杂数据类型的时候，当且仅当该equals方法参数不是 null，两个变量的类型、内容都相同，则比较结果为true。但string类有常量池的缘故较为特殊。 3.当比较对象为实体类的时候，不重写equals方法，比较的是二者在堆内存中的引用地址，无意义，一般在实体类中进行重写equals方法，自定义比较规则. 附：Object的getClass方法与getName方法getClass方法：类型：public final Class&lt;? extends Object&gt; getClass()功能：返回该对象的运行时类的Java.lang.Class对象（API上的解释）有方法类型可以知道，该方法只能由类的实例变量调用例子： [java] view plain copy JButton b1 = new JButton(&quot;button1&quot;); System.out.println(b1.getClass()); 输出： class javax.swing.JButton class属性当你要获得一个类的Class对象时（作函数参数的时候），你不能调用getClass方法，那你只能用类名.class来达到效果例子： [java] view plain copy System.out.println(JButton.class); 输出：class javax.swing.JButton getName方法：类型：public String getName()功能：以String形式返回次Class对象所表示的实体名称例子： [java] view plain copy JButton b1 = new JButton(&quot;button1&quot;); System.out.println(b1.getName()); 输出：javax.swing.JButton 可以发现用class属性和getClass返回的输出是一样的，用getName返回的比前面两种少了class和一个空格。","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/tags/Java基础/"}]},{"title":"Rabbitmq基本原理","slug":"Rabbitmq基本原理","date":"2016-05-03T09:37:31.000Z","updated":"2017-08-25T10:28:47.000Z","comments":true,"path":"2016/05/03/Rabbitmq基本原理/","link":"","permalink":"http://yoursite.com/2016/05/03/Rabbitmq基本原理/","excerpt":"","text":"MQ全称为Message Queue, 是一种分布式应用程序的的通信方法，它是消费-生产者模型的一个典型的代表，producer往消息队列中不断写入消息，而另一端consumer则可以读取或者订阅队列中的消息。RabbitMQ是MQ产品的典型代表，是一款基于AMQP协议可复用的企业消息系统。业务上，可以实现服务提供者和消费者之间的数据解耦，提供高可用性的消息传输机制，在实际生产中应用相当广泛。本文意在介绍Rabbitmq的基本原理，包括rabbitmq基本框架，概念，通信过程等。 系统架构 Rabbitmq系统最核心的组件是Exchange和Queue，下图是系统简单的示意图。Exchange和Queue是在rabbitmq server（又叫做broker）端，producer和consumer在应用端。 ##producer&amp;Consumer producer指的是消息生产者，consumer消息的消费者。 ##Queue 消息队列，提供了FIFO的处理机制，具有缓存消息的能力。rabbitmq中，队列消息可以设置为持久化，临时或者自动删除。 设置为持久化的队列，queue中的消息会在server本地硬盘存储一份，防止系统crash，数据丢失 设置为临时队列，queue中的数据在系统重启之后就会丢失 设置为自动删除的队列，当不存在用户连接到server，队列中的数据会被自动删除##Exchange Exchange类似于数据通信网络中的交换机，提供消息路由策略。rabbitmq中，producer不是通过信道直接将消息发送给queue，而是先发送给Exchange。一个Exchange可以和多个Queue进行绑定，producer在传递消息的时候，会传递一个ROUTING_KEY，Exchange会根据这个ROUTING_KEY按照特定的路由算法，将消息路由给指定的queue。和Queue一样，Exchange也可设置为持久化，临时或者自动删除。 Exchange有4种类型：direct(默认)，fanout, topic, 和headers，不同类型的Exchange转发消息的策略有所区别： Direct直接交换器，工作方式类似于单播，Exchange会将消息发送完全匹配ROUTING_KEY的Queue fanout广播是式交换器，不管消息的ROUTING_KEY设置为什么，Exchange都会将消息转发给所有绑定的Queue。 topic主题交换器，工作方式类似于组播，Exchange会将消息转发和ROUTING_KEY匹配模式相同的所有队列，比如，ROUTING_KEY为user.stock的Message会转发给绑定匹配模式为 .stock,user.stock， . 和#.user.stock.#的队列。（ 表是匹配一个任意词组，#表示匹配0个或多个词组） headers消息体的header匹配（ignore）Binding 所谓绑定就是将一个特定的 Exchange 和一个特定的 Queue 绑定起来。Exchange 和Queue的绑定可以是多对多的关系。 ##virtual host 在rabbitmq server上可以创建多个虚拟的message broker，又叫做virtual hosts (vhosts)。每一个vhost本质上是一个mini-rabbitmq server，分别管理各自的exchange，和bindings。vhost相当于物理的server，可以为不同app提供边界隔离，使得应用安全的运行在不同的vhost实例上，相互之间不会干扰。producer和consumer连接rabbit server需要指定一个vhost。 ##通信过程 假设P1和C1注册了相同的Broker，Exchange和Queue。P1发送的消息最终会被C1消费。基本的通信流程大概如下所示： P1生产消息，发送给服务器端的Exchange Exchange收到消息，根据ROUTINKEY，将消息转发给匹配的Queue1 Queue1收到消息，将消息发送给订阅者C1 C1收到消息，发送ACK给队列确认收到消息 Queue1收到ACK，删除队列中缓存的此条消息Consumer收到消息时需要显式的向rabbit broker发送basic.ack消息或者consumer订阅消息时设置auto_ack参数为true。在通信过程中，队列对ACK的处理有以下几种情况： 如果consumer接收了消息，发送ack,rabbitmq会删除队列中这个消息，发送另一条消息给consumer。 如果cosumer接受了消息, 但在发送ack之前断开连接，rabbitmq会认为这条消息没有被deliver,在consumer在次连接的时候，这条消息会被redeliver。 如果consumer接受了消息，但是程序中有bug,忘记了ack,rabbitmq不会重复发送消息。 rabbitmq2.0.0和之后的版本支持consumer reject某条（类）消息，可以通过设置requeue参数中的reject为true达到目地，那么rabbitmq将会把消息发送给下一个注册的consumer。Conclusion 本文和大家一起学习了rabbitmq的一些基础知识，在之后的博文中，笔者将会和大家一起分享更多的Rabbitmq知识","categories":[],"tags":[{"name":"分布式消息","slug":"分布式消息","permalink":"http://yoursite.com/tags/分布式消息/"}]},{"title":"ReenTrantLock可重入锁（和synchronized的区别）总结","slug":"ReenTrantLock可重入锁（和synchronized的区别）总结","date":"2016-05-03T09:37:31.000Z","updated":"2017-08-25T10:25:05.000Z","comments":true,"path":"2016/05/03/ReenTrantLock可重入锁（和synchronized的区别）总结/","link":"","permalink":"http://yoursite.com/2016/05/03/ReenTrantLock可重入锁（和synchronized的区别）总结/","excerpt":"ReenTrantLock可重入锁（和synchronized的区别）总结","text":"ReenTrantLock可重入锁（和synchronized的区别）总结 可重入性： 从名字上理解，ReenTrantLock的字面意思就是再进入的锁，其实synchronized关键字所使用的锁也是可重入的，两者关于这个的区别不大。两者都是同一个线程没进入一次，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 锁的实现： Synchronized是依赖于JVM实现的，而ReenTrantLock是JDK实现的，有什么区别，说白了就类似于操作系统来控制实现和用户自己敲代码实现的区别。前者的实现是比较难见到的，后者有直接的源码可供阅读。 性能的区别： 在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。 功能区别： 便利性：很明显Synchronized的使用比较方便简洁，并且由编译器去保证锁的加锁和释放，而ReenTrantLock需要手工声明来加锁和释放锁，为了避免忘记手工释放锁造成死锁，所以最好在finally中声明释放锁。 锁的细粒度和灵活度：很明显ReenTrantLock优于Synchronized ReenTrantLock独有的能力： 1.ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 2.ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。 3.ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。 ReenTrantLock实现的原理： 在网上看到相关的源码分析，本来这块应该是本文的核心，但是感觉比较复杂就不一一详解了，简单来说，ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。 什么情况下使用ReenTrantLock： 答案是，如果你需要实现ReenTrantLock的三个独有功能时。","categories":[],"tags":[{"name":"并发相关","slug":"并发相关","permalink":"http://yoursite.com/tags/并发相关/"}]},{"title":"HashMap的底层数据结构解析以及与其他容器对比","slug":"HashMap的底层数据结构解析以及与其他容器对比","date":"2016-04-20T09:37:31.000Z","updated":"2017-08-25T10:49:52.000Z","comments":true,"path":"2016/04/20/HashMap的底层数据结构解析以及与其他容器对比/","link":"","permalink":"http://yoursite.com/2016/04/20/HashMap的底层数据结构解析以及与其他容器对比/","excerpt":"","text":"参考地址：http://blog.csdn.net/ustcbob/article/details/23709589 ###要点总结：HashMap实际上是一个“链表的数组”的数据结构，每个元素存放链表头结点的数组，即数组和链表的结合体。 当我们往HashMap中put元素的时候，先根据key的hashCode重新计算hash值，根据hash值得到这个元素在数组中的位置（即下标），如果数组该位置上已经存放有其他元素了，那么在这个位置上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。如果数组该位置上没有元素，就直接将该元素放到此数组中的该位置上。 HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。HashMap 底层采用一个 Entry[] 数组来保存所有的 key-value 对，当需要存储一个 Entry 对象时，会根据hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 HashMap和Hashtable的区别HashMap和Hashtable都实现了Map接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性，同步(synchronization)，以及速度。 HashMap几乎可以等价于Hashtable，除了HashMap是非synchronized的，并可以接受null(HashMap可以接受为null的键值(key)和值(value)，而Hashtable则不行)。 HashMap是非synchronized，而Hashtable是synchronized，这意味着Hashtable是线程安全的，多个线程可以共享一个Hashtable；而如果没有正确的同步的话，多个线程是不能共享HashMap的。Java 5提供了ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。 另一个区别是HashMap的迭代器(Iterator)是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。 由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。 HashMap不能保证随着时间的推移Map中的元素次序是不变的。 HashMap和Hashtable的底层实现都是数组+链表结构实现的，这点上完全一致 除开HashMap和Hashtable外，还有一个hash集合HashSet，有所区别的是HashSet不是key value结构，仅仅是存储不重复的元素，相当于简化版的HashMap，只是包含HashMap中的key而已 ConcurrentHashMap和HashMap的区别（1）ConcurrentHashMap对整个桶数组进行了分段，而HashMap则没有 （2）ConcurrentHashMap在每一个分段上都用锁进行保护，从而让锁的粒度更精细一些，并发性能更好，而HashMap没有锁机制，不是线程安全的 HashMap概述： HashMap是基于哈希表的Map接口的非同步实现（Hashtable跟HashMap很像，唯一的区别是Hashtalbe中的方法是线程安全的，也就是同步的）。此实现提供所有可选的映射操作，并允许使用null值和null键。此类不保证映射的顺序，特别是它不保证该顺序恒久不变。 HashMap的数据结构： 在Java编程语言中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，HashMap也不例外。HashMap实际上是一个“链表的数组”的数据结构，每个元素存放链表头结点的数组，即数组和链表的结合体。 从上图中可以看出，HashMap底层就是一个数组，数组中的每一项又是一个链表。当新建一个HashMap的时候，就会初始化一个数组。源码如下： [java] view plain copy 1. /** 2. * The table, resized as necessary. Length MUST Always be a power of two. 3. */ 4. transient Entry[] table; 5. 6. static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { 7. final K key; 8. V value; 9. Entry&lt;K,V&gt; next; 10. final int hash; 11. …… 12. } 可以看出，Entry就是数组中的元素，每个Map.Entry就是一个key-value对，它持有一个指向下一个元素的引用，这就构成了链表。 HashMap的存取实现： 1) 存储： [java] view plain copy public V put(K key, V value) { // HashMap允许存放null键和null值。 // 当key为null时，调用putForNullKey方法，将value放置在数组第一个位置。 if (key == null) return putForNullKey(value); // 根据key的hashCode重新计算hash值。 int hash = hash(key.hashCode()); // 搜索指定hash值所对应table中的索引。 int i = indexFor(hash, table.length); // 如果 i 索引处的 Entry 不为 null，通过循环不断遍历 e 元素的下一个元素。 for (Entry e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } // 如果i索引处的Entry为null，表明此处还没有Entry。 // modCount记录HashMap中修改结构的次数 modCount++; // 将key、value添加到i索引处。 addEntry(hash, key, value, i); return null; } 从上面的源代码中可以看出：当我们往HashMap中put元素的时候，先根据key的hashCode重新计算hash值，根据hash值得到这个元素在数组中的位置（即下标），如果数组该位置上已经存放有其他元素了，那么在这个位置上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。如果数组该位置上没有元素，就直接将该元素放到此数组中的该位置上。 addEntry(hash, key, value, i)方法根据计算出的hash值，将key-value对放在数组table的 i 索引处。addEntry 是HashMap 提供的一个包访问权限的方法（就是没有public，protected，private这三个访问权限修饰词修饰，为默认的访问权限，用default表示，但在代码中没有这个default），代码如下： [java] view plain copy 1. void addEntry(int hash, K key, V value, int bucketIndex) { 2. // 获取指定 bucketIndex 索引处的 Entry 3. Entry&lt;K,V&gt; e = table[bucketIndex]; 4. // 将新创建的 Entry 放入 bucketIndex 索引处，并让新的 Entry 指向原来的 Entry 5. table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); 6. // 如果 Map 中的 key-value 对的数量超过了极限 7. if (size++ &gt;= threshold) 8. // 把 table 对象的长度扩充到原来的2倍。 9. resize(2 * table.length); 10. } 当系统决定存储HashMap中的key-value对时，完全没有考虑Entry中的value，仅仅只是根据key来计算并决定每个Entry的存储位置。我们完全可以把 Map 集合中的 value 当成 key 的附属，当系统决定了 key 的存储位置之后，value 随之保存在那里即可。 hash(int h)方法根据key的hashCode重新计算一次散列。此算法加入了高位计算，防止低位不变，高位变化时，造成的hash冲突。 [java] view plain copy 1. static int hash(int h) { 2. h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); 3. return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); 4. } 我们可以看到在HashMap中要找到某个元素，需要根据key的hash值来求得对应数组中的位置。如何计算这个位置就是hash算法。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的 元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表，这样就大大优化了查询的效率。 对于任意给定的对象，只要它的 hashCode() 返回值相同，那么程序调用 hash(int h) 方法所计算得到的 hash 码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，“模”运算的消耗还是比较大的，在HashMap中是这样做的：调用 indexFor(int h, int length) 方法来计算该对象应该保存在 table 数组的哪个索引处。indexFor(int h, int length) 方法的代码如下： [java] view plain copy static int indexFor(int h, int length) { return h &amp; (length-1); } 这个方法非常巧妙，它通过 h &amp; (table.length -1) 来得到该对象的保存位，而HashMap底层数组的长度总是 2 的n 次方，这是HashMap在速度上的优化。在 HashMap 构造器中有如下代码： [java] view plain copy int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1;这段代码保证初始化时HashMap的容量总是2的n次方，即底层数组的长度总是为2的n次方。 当length总是 2 的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 这看上去很简单，其实比较有玄机的，我们举个例子来说明： 假设数组长度分别为15和16，优化后的hash码分别为8和9，那么&amp;运算后的结果如下： h &amp; (table.length-1) hash table.length-1 8 &amp; (15-1)： 0100 &amp; 1110 = 0100 9 &amp; (15-1)： 0101 &amp; 1110 = 0100 ----------------------------------------------------------------------------------------------------------------------- 8 &amp; (16-1)： 0100 &amp; 1111 = 0100 9 &amp; (16-1)： 0101 &amp; 1111 = 0101 ----------------------------------------------------------------------------------------------------------------------- 从上面的例子中可以看出：当8、9两个数和(15-1)2=(1110)进行“与运算&amp;”的时候，产生了相同的结果，都为0100，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到数组中的同一个位置上形成链表，那么查询的时候就需要遍历这个链 表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hash值会与(15-1)2=(1110)进行“与运算&amp;”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ 而当数组长度为16时，即为2的n次方时，2n-1得到的二进制数的每个位上的值都为1（比如(24-1)2=1111），这使得在低位上&amp;时，得到的和原hash的低位相同，加之hash(int h)方法对key的hashCode的进一步优化，加入了高位计算，就使得只有相同的hash值的两个值才会被放到数组中的同一个位置上形成链表。 所以说，当数组长度为2的n次幂的时候，不同的key算得得index相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。 根据上面 put 方法的源代码可以看出，当程序试图将一个key-value对放入HashMap中时，程序首先根据该 key的 hashCode() 返回值决定该 Entry 的存储位置：如果两个 Entry 的 key 的 hashCode() 返回值相同，那它们的存储位置相同。如果这两个 Entry 的 key 通过 equals 比较返回 true，新添加 Entry 的 value 将覆盖集合中原有Entry 的 value，但key不会覆盖。如果这两个 Entry 的 key 通过 equals 比较返回 false，新添加的 Entry 将与集合中原有 Entry 形成 Entry 链，而且新添加的 Entry 位于 Entry 链的头部——具体说明继续看 addEntry() 方法的说明。 2) 读取： [java] view plain copy 1. public V get(Object key) { 2. if (key == null) 3. return getForNullKey(); 4. int hash = hash(key.hashCode()); 5. for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; 6. e != null; 7. e = e.next) { 8. Object k; 9. if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) 10. return e.value; 11. } 12. return null; 13. } 有了上面存储时的hash算法作为基础，理解起来这段代码就很容易了。从上面的源代码中可以看出：从HashMap中get元素时，首先计算key的hashCode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。 3) 归纳起来简单地说，HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。HashMap 底层采用一个 Entry[] 数组来保存所有的 key-value 对，当需要存储一个 Entry 对象时，会根据hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 Map类提供了一个称为entrySet()的方法，这个方法返回一个Map.Entry实例化后的对象集。接着，Map.Entry类提供了一个getKey()方法和一个getValue()方法，因此，上面的代码可以被组织得更符合逻辑。举例如下： Map map = new HashMap(); Iterator iterator = map.entrySet().iterator(); while(iterator.hasNext()){ Map.Entry entry = (Map.Entry)iterator.next(); Object key = entry.getKey(); Object value = entry.getValue(); } HashMap的resize（rehash）： 当HashMap中的元素越来越多的时候，hash冲突的几率也就越来越高，因为数组的长度是固定的。所以为了提高查询的效率，就要对HashMap的数组进行扩容，数组扩容这个操作也会出现在ArrayList中，这是一个常用的操作，而在HashMap数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是resize。 那么HashMap什么时候进行扩容呢？当HashMap中的元素个数超过数组大小loadFactor时，就会进行数组扩容，loadFactor的默认值为0.75，这是一个折中的取值。也就是说，默认情况下，数组大小为16，那么当HashMap中元素个数超过160.75=12（这个值就是代码中的threshold值，也叫做临界值）的时候，就把数组的大小扩展为 2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知HashMap中元素的个数，那么预设元素的个数能够有效的提高HashMap的性能。 HashMap扩容的代码如下所示： [java] view plain copy 1. //HashMap数组扩容 2. void resize(int newCapacity) { 3. Entry[] oldTable = table; 4. int oldCapacity = oldTable.length; 5. //如果当前的数组长度已经达到最大值，则不在进行调整 6. if (oldCapacity == MAXIMUM_CAPACITY) { 7. threshold = Integer.MAX_VALUE; 8. return; 9. } 10. //根据传入参数的长度定义新的数组 11. Entry[] newTable = new Entry[newCapacity]; 12. //按照新的规则，将旧数组中的元素转移到新数组中 13. transfer(newTable); 14. table = newTable; 15. //更新临界值 16. threshold = (int)(newCapacity * loadFactor); 17. } 18. 19. //旧数组中元素往新数组中迁移 20. void transfer(Entry[] newTable) { 21. //旧数组 22. Entry[] src = table; 23. //新数组长度 24. int newCapacity = newTable.length; 25. //遍历旧数组 26. for (int j = 0; j &lt; src.length; j++) { 27. Entry&lt;K,V&gt; e = src[j]; 28. if (e != null) { 29. src[j] = null; 30. do { 31. Entry&lt;K,V&gt; next = e.next; 32. int i = indexFor(e.hash, newCapacity); 33. e.next = newTable[i]; 34. newTable[i] = e; 35. e = next; 36. } while (e != null); 37. } 38. } 39. } 5.HashMap的性能参数： HashMap 包含如下几个构造器： HashMap()：构建一个初始容量为 16，负载因子为 0.75 的 HashMap。 HashMap(int initialCapacity)：构建一个初始容量为 initialCapacity，负载因子为 0.75 的 HashMap。 HashMap(int initialCapacity, float loadFactor)：以指定初始容量、指定的负载因子创建一个 HashMap。 HashMap的基础构造器HashMap(int initialCapacity, float loadFactor)带有两个参数，它们是初始容量initialCapacity和加载因子loadFactor。 initialCapacity：HashMap的最大容量，即为底层数组的长度。 loadFactor：负载因子loadFactor定义为：散列表的实际元素数目(n)/ 散列表的容量(m)。 负载因子衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。对于使用链表法的散列表来说，查找一个元素的平均时间是O(1+a)，因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低；如果负载因子太小，那么散列表的数据将过于稀疏，对空间造成严重浪费。 HashMap的实现中，通过threshold字段来判断HashMap的最大容量： threshold = (int)(capacity * loadFactor); 结合负载因子的定义公式可知，threshold就是在此loadFactor和capacity对应下允许的最大元素数目，超过这个数目就重新resize，以降低实际的负载因子（也就是说虽然数组长度是capacity，但其扩容的临界值确是threshold）。默认的的负载因子0.75是对空间和时间效率的一个平衡选择。当容量超出此最大容量时， resize后的HashMap容量是容量的两倍： if (size++ &gt;= threshold) resize( 2 * table.length); 6.Fail-Fast机制： 我们知道java.util.HashMap不是线程安全的，因此如果在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。（这个在core java这本书中也有提到。） 这一策略在源码中的实现是通过modCount域，modCount顾名思义就是修改次数，对HashMap内容的修改都将增加这个值，那么在迭代器初始化过程中会将这个值赋给迭代器的expectedModCount。 [java] view plain copy 1. HashIterator() { 2. expectedModCount = modCount; 3. if (size &gt; 0) { // advance to first entry 4. Entry[] t = table; 5. while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) 6. ; 7. } 8. } 在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map： 注意到modCount声明为volatile，保证线程之间修改的可见性。（volatile之所以线程安全是因为被volatile修饰的变量不保存缓存，直接在内存中修改，因此能够保证线程之间修改的可见性）。 final Entry nextEntry() { if (modCount != expectedModCount) throw new ConcurrentModificationException();在HashMap的API中指出： 由所有HashMap类的“collection 视图方法”所返回的迭代器都是快速失败的：在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器本身的 remove 方法，其他任何时间任何方式的修改，迭代器都将抛出ConcurrentModificationException。因此，面对并发的修改，迭代器很快就会完全失败，而不保证在将来不确定的时间发生任意不确定行为的风险。 注意，迭代器的快速失败行为不能得到保证，一般来说，存在非同步的并发修改时，不可能作出任何坚决的保证。快速失败迭代器尽最大努力抛出 ConcurrentModificationException。因此，编写依赖于此异常的程序的做法是错误的，正确做法是：迭代器的快速失败行为应该仅用于检测程序错误。","categories":[],"tags":[{"name":"Java容器相关","slug":"Java容器相关","permalink":"http://yoursite.com/tags/Java容器相关/"}]},{"title":"数据库Sharding的基本思想和切分策略","slug":"数据库Sharding的基本思想和切分策略","date":"2016-04-03T12:18:31.000Z","updated":"2017-08-18T09:25:39.000Z","comments":true,"path":"2016/04/03/数据库Sharding的基本思想和切分策略/","link":"","permalink":"http://yoursite.com/2016/04/03/数据库Sharding的基本思想和切分策略/","excerpt":"本文着重介绍sharding的基本思想和理论上的切分策略，关于更加细致的实施策略和参考事例请参考我的另一篇博文：数据库分库分表(sharding)系列(一) 拆分实施策略和示例演示 参考地址：http://blog.csdn.net/bluishglc/article/details/6161475 请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。","text":"本文着重介绍sharding的基本思想和理论上的切分策略，关于更加细致的实施策略和参考事例请参考我的另一篇博文：数据库分库分表(sharding)系列(一) 拆分实施策略和示例演示 参考地址：http://blog.csdn.net/bluishglc/article/details/6161475 请大家多多支持原作者，感谢原作者的认真和辛勤整理。因为没有看到联系方式，不能深入交流，非常遗憾。 一、基本思想Sharding的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。不太严格的讲，对于海量数据的数据库，如果是因为表多而数据多，这时候适合使用垂直切分，即把关系紧密（比如同一模块）的表切分出来放在一个server上。如果表并不多，但每张表的数据非常多，这时候适合水平切分，即把表的数据按某种规则（比如按ID散列）切分到多个数据库(server)上。当然，现实中更多是这两种情况混杂在一起，这时候需要根据实际情况做出选择，也可能会综合使用垂直与水平切分，从而将原有数据库切分成类似矩阵一样可以无限扩充的数据库(server)阵列。下面分别详细地介绍一下垂直切分和水平切分. 垂直切分的最大特点就是规则简单，实施也更为方便，尤其适合各业务之间的耦合度非常低，相互影响很小，业务逻辑非常清晰的系统。在这种系统中，可以很容易做到将不同业务模块所使用的表分拆到不同的数据库中。根据不同的表来进行拆分，对应用程序的影响也更小，拆分规则也会比较简单清晰。（这也就是所谓的”share nothing”）。 水平切分于垂直切分相比，相对来说稍微复杂一些。因为要将同一个表中的不同数据拆分到不同的数据库中，对于应用程序来说，拆分规则本身就较根据表名来拆分更为复杂，后期的数据维护也会更为复杂一些。 让我们从普遍的情况来考虑数据的切分：一方面，一个库的所有表通常不可能由某一张表全部串联起来，这句话暗含的意思是，水平切分几乎都是针对一小搓一小搓（实际上就是垂直切分出来的块）关系紧密的表进行的，而不可能是针对所有表进行的。另一方面，一些负载非常高的系统，即使仅仅只是单个表都无法通过单台数据库主机来承担其负载，这意味着单单是垂直切分也不能完全解决问明。因此多数系统会将垂直切分和水平切分联合使用，先对系统做垂直切分，再针对每一小搓表的情况选择性地做水平切分。从而将整个数据库切分成一个分布式矩阵。 二、切分策略如前面所提到的，切分是按先垂直切分再水平切分的步骤进行的。垂直切分的结果正好为水平切分做好了铺垫。垂直切分的思路就是分析表间的聚合关系，把关系紧密的表放在一起。多数情况下可能是同一个模块，或者是同一“聚集”。这里的“聚集”正是领域驱动设计里所说的聚集。在垂直切分出的表聚集内，找出“根元素”（这里的“根元素”就是领域驱动设计里的“聚合根”），按“根元素”进行水平切分，也就是从“根元素”开始，把所有和它直接与间接关联的数据放入一个shard里。这样出现跨shard关联的可能性就非常的小。应用程序就不必打断既有的表间关联。比如：对于社交网站，几乎所有数据最终都会关联到某个用户上，基于用户进行切分就是最好的选择。再比如论坛系统，用户和论坛两个模块应该在垂直切分时被分在了两个shard里，对于论坛模块来说，Forum显然是聚合根，因此按Forum进行水平切分，把Forum里所有的帖子和回帖都随Forum放在一个shard里是很自然的。 对于共享数据数据，如果是只读的字典表，每个shard里维护一份应该是一个不错的选择，这样不必打断关联关系。如果是一般数据间的跨节点的关联，就必须打断。 需要特别说明的是：当同时进行垂直和水平切分时，切分策略会发生一些微妙的变化。比如：在只考虑垂直切分的时候，被划分到一起的表之间可以保持任意的关联关系，因此你可以按“功能模块”划分表格，但是一旦引入水平切分之后，表间关联关系就会受到很大的制约，通常只能允许一个主表（以该表ID进行散列的表）和其多个次表之间保留关联关系，也就是说：当同时进行垂直和水平切分时，在垂直方向上的切分将不再以“功能模块”进行划分，而是需要更加细粒度的垂直切分，而这个粒度与领域驱动设计中的“聚合”概念不谋而合，甚至可以说是完全一致，每个shard的主表正是一个聚合中的聚合根！这样切分下来你会发现数据库分被切分地过于分散了（shard的数量会比较多，但是shard里的表却不多），为了避免管理过多的数据源，充分利用每一个数据库服务器的资源，可以考虑将业务上相近，并且具有相近数据增长速率（主表数据量在同一数量级上）的两个或多个shard放到同一个数据源里，每个shard依然是独立的，它们有各自的主表，并使用各自主表ID进行散列，不同的只是它们的散列取模（即节点数量）必需是一致的。 1.事务问题：解决事务问题目前有两种可行的方案：分布式事务和通过应用程序与数据库共同控制实现事务下面对两套方案进行一个简单的对比。方案一：使用分布式事务 优点：交由数据库管理，简单有效 缺点：性能代价高，特别是shard越来越多时方案二：由应用程序和数据库共同控制 原理：将一个跨多个数据库的分布式事务分拆成多个仅处 于单个数据库上面的小事务，并通过应用程序来总控 各个小事务。 优点：性能上有优势 缺点：需要应用程序在事务控制上做灵活设计。如果使用 了spring的事务管理，改动起来会面临一定的困难。2.跨节点Join的问题 只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 3.跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。 参考资料： 《MySQL性能调优与架构设计》 注：本文图片摘自《mysql性能调优与架构设计》一 书 第一部分：实施策略 图1.数据库分库分表(sharding)实施策略图解(点击查看大图) 1.准备阶段对数据库进行分库分表(Sharding化)前，需要开发人员充分了解系统业务逻辑和数据库schema.一个好的建议是绘制一张数据库ER图或领域模型图，以这类图为基础划分shard,直观易行，可以确保开发人员始终保持清醒思路。对于是选择数据库ER图还是领域模型图要根据项目自身情况进行选择。如果项目使用数据驱动的开发方式，团队以数据库ER图作为业务交流的基础，则自然会选择数据库ER图，如果项目使用的是领域驱动的开发方式，并通过OR-Mapping构建了一个良好的领域模型，那么领域模型图无疑是最好的选择。就我个人来说，更加倾向使用领域模型图，因为进行切分时更多的是以业务为依据进行分析判断，领域模型无疑更加清晰和直观。 2.分析阶段1. 垂直切分垂直切分的依据原则是：将业务紧密，表间关联密切的表划分在一起，例如同一模块的表。结合已经准备好的数据库ER图或领域模型图，仿照活动图中的泳道概念，一个泳道代表一个shard，把所有表格划分到不同的泳道中。下面的分析示例会展示这种做法。当然，你也可以在打印出的ER图或模型图上直接用铅笔圈，一切取决于你自己的喜好。 2. 水平切分垂直切分后，需要对shard内表格的数据量和增速进一步分析，以确定是否需要进行水平切分。 2.1若划分到一起的表格数据增长缓慢，在产品上线后可遇见的足够长的时期内均可以由单一数据库承载，则不需要进行水平切分，所有表格驻留同一shard,所有表间关联关系会得到最大限度的保留，同时保证了书写SQL的自由度，不易受join、group by、order by等子句限制。 2.2 若划分到一起的表格数据量巨大，增速迅猛，需要进一步进行水平分割。进一步的水平分割就这样进行： 2.2.1.结合业务逻辑和表间关系，将当前shard划分成多个更小的shard,通常情况下，这些更小的shard每一个都只包含一个主表（将以该表ID进行散列的表）和多个与其关联或间接关联的次表。这种一个shard一张主表多张次表的状况是水平切分的必然结果。这样切分下来，shard数量就会迅速增多。如果每一个shard代表一个独立的数据库，那么管理和维护数据库将会非常麻烦，而且这些小shard往往只有两三张表，为此而建立一个新库，利用率并不高，因此，在水平切分完成后可再进行一次“反向的Merge”,即：将业务上相近，并且具有相近数据增长速率（主表数据量在同一数量级上）的两个或多个shard放到同一个数据库上，在逻辑上它们依然是独立的shard，有各自的主表，并依据各自主表的ID进行散列，不同的只是它们的散列取模（即节点数量）必需是一致的。这样，每个数据库结点上的表格数量就相对平均了。 2.2.2. 所有表格均划分到合适的shard之后，所有跨越shard的表间关联都必须打断，在书写sql时，跨shard的join、group by、order by都将被禁止，需要在应用程序层面协调解决这些问题。 特别想提一点：经水平切分后，shard的粒度往往要比只做垂直切割的粒度要小，原单一垂直shard会被细分为一到多个以一个主表为中心关联或间接关联多个次表的shard，此时的shard粒度与领域驱动设计中的“聚合”概念不谋而合，甚至可以说是完全一致，每个shard的主表正是一个聚合中的聚合根！ 3.实施阶段如果项目在开发伊始就决定进行分库分表，则严格按照分析设计方案推进即可。如果是在中期架构演进中实施，除搭建实现sharding逻辑的基础设施外(关于该话题会在下篇文章中进行阐述)，还需要对原有SQL逐一过滤分析，修改那些因为sharding而受到影响的sql. 第二部分：示例演示 本文选择一个人尽皆知的应用：jpetstore来演示如何进行分库分表(sharding)在分析阶段的工作。由于一些个人原因，演示使用的jpetstore来自原ibatis官方的一个Demo版本，SVN地址为：http://mybatis.googlecode.com/svn/tags/java_release_2.3.4-726/jpetstore-5。关于jpetstore的业务逻辑这里不再介绍，这是一个非常简单的电商系统原型，其领域模型如下图： 图2. jpetstore领域模型 由于系统较简单，我们很容易从模型上看出，其主要由三个模块组成：用户，产品和订单。那么垂直切分的方案也就出来了。接下来看水平切分，如果我们从一个实际的宠物店出发考虑，可能出现数据激增的单表应该是Account和Order,因此这两张表需要进行水平切分。对于Product模块来说，如果是一个实际的系统，Product和Item的数量都不会很大，因此只做垂直切分就足够了，也就是（Product，Category，Item，Iventory，Supplier）五张表在一个数据库结点上（没有水平切分，不会存在两个以上的数据库结点）。但是作为一个演示，我们假设产品模块也有大量的数据需要我们做水平切分，那么分析来看，这个模块要拆分出两个shard:一个是（Product（主），Category），另一个是（Item（主），Iventory，Supplier），同时，我们认为：这两个shard在数据增速上应该是相近的，且在业务上也很紧密，那么我们可以把这两个shard放在同一个数据库节点上，Item和Product数据在散列时取一样的模。根据前文介绍的图纸绘制方法，我们得到下面这张sharding示意图： 图3. jpetstore sharding示意图 对于这张图再说明几点： 1.使用泳道表示物理shard（一个数据库结点）2.若垂直切分出的shard进行了进一步的水平切分，但公用一个物理shard的话，则用虚线框住，表示其在逻辑上是一个独立的shard。 3.深色实体表示主表 4.X表示需要打断的表间关联","categories":[],"tags":[{"name":"高并发方案","slug":"高并发方案","permalink":"http://yoursite.com/tags/高并发方案/"}]},{"title":"mac下hexo搭建相关记录","slug":"mac下hexo搭建操作记录","date":"2016-03-23T13:08:57.000Z","updated":"2017-08-18T09:24:19.000Z","comments":true,"path":"2016/03/23/mac下hexo搭建操作记录/","link":"","permalink":"http://yoursite.com/2016/03/23/mac下hexo搭建操作记录/","excerpt":"吃水不忘挖井人，今天把hexo在mac下的搭建和部署说一下。hexo是个挺有意思的东西，但是，因为版本和配置细节等问题，这两天遇坑无数。重装了四五遍。到今晚终于给整差不多了。着重把遇到的问题给说一下。","text":"吃水不忘挖井人，今天把hexo在mac下的搭建和部署说一下。hexo是个挺有意思的东西，但是，因为版本和配置细节等问题，这两天遇坑无数。重装了四五遍。到今晚终于给整差不多了。着重把遇到的问题给说一下。 No1:nvm问题刚开始要注意使用nvm来控制node版本并且注意node版本最好是6.2以上（我因为版本低重装了一遍） No.2:部署成功github访问报404解决：github有缓存，等一会儿或者一晚上，就好了 No.3:注意_config.yml的配置一定要在冒号之后留出空格，否则会报出各种奇葩问题。 _config.yml中都要用半角来输入，要注意比较常见的“／”，因为此符半角全角分不太出来。 No.4markdown编辑器我采用马克飞象先命令行新建md文件，然后利用xcode打开，在马克飞象中编辑后copy到其中，或terminal中利用vim打开，编辑后copy到其中。 No.5我所采用的主题：https://github.com/litten/hexo-theme-yilia No.6使用yilia主题遇到的一些问题解决办法1 我们发布的文章是在主页显示的时候全部显示，那样很长很难看，想要部分展示的效果。 使用”&lt;! – more – &gt;”标签来隐藏其下面的内容~ 实在有坑填不了，换个目录重装，也比在那里纠结快，每重装一次都会比之前快 过程中遇坑无数，我着急解决没有记录下来现在也想不太起来了。善用谷歌和百度，会有意想不到的收获。最初我以为hexo冷门在网络上不会有什么记录没想到一大堆。 图片引用用markdown语法不好用，“”;最终只是用了html标签 域名问题在godaddy上买到了符合我名字的yutinglin.cn,也不知道会不会被封 Markdown编辑器用了十几天马克飞象之后还是转到macdown了，马克飞象需要付费但是因为小众没有破解。 DNS解析利用DNSPOD进行解析，1.在yourname.github.io的根目录下添加CNAME具体就是在Hexo目录里的source文件下添加一个名为CNAME的文件，注意这个文件是没有后缀的，千万不要设置成.txt文本文件，文件的内容就是域名，格式如： niujiajun.com 2.在DNSPOD管理页面点击添加解析，记录类型选A或CNAME，A记录的记录值就是ip地址，github(官方文档)提供了两个IP地址，192.30.252.153和192.30.252.154，这两个IP地址为github的服务器地址，两个都要填上，解析记录设置两个www和@，线路就默认就行了，CNAME记录值填你的github博客网址。如我的是whitescholars.github.io。 评论系统采用网易云跟帖，发现各家评论都很恶心，都需要用户登陆才能评论，完全不考虑用户需求和体验。发现即使是程序员DD的一片最新文章用的畅言评论系统也只有两个评论，这样的话如何起到和大家沟通的作用呢？ 需要注意的地方1.标题设置“###”之后需要空格才能显示出来","categories":[],"tags":[{"name":"操作记录","slug":"操作记录","permalink":"http://yoursite.com/tags/操作记录/"}]},{"title":"HashMap实现原理解析","slug":"HashMap实现原理解析","date":"2016-03-03T09:37:31.000Z","updated":"2017-08-25T08:50:26.000Z","comments":true,"path":"2016/03/03/HashMap实现原理解析/","link":"","permalink":"http://yoursite.com/2016/03/03/HashMap实现原理解析/","excerpt":"","text":"“你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？” 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了：“你了解重新调整HashMap大小存在什么问题吗？”你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 我们可以使用CocurrentHashMap来代替Hashtable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看这篇博客查看Hashtable和ConcurrentHashMap的区别。我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念 HashMap中解决碰撞的方法 equals()和hashCode()的应用，以及它们在HashMap中的重要性 不可变对象的好处 HashMap多线程的条件竞争 重新调整HashMap的大小总结 HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。 参考地址：http://www.importnew.com/7099.html","categories":[],"tags":[{"name":"Java容器相关","slug":"Java容器相关","permalink":"http://yoursite.com/tags/Java容器相关/"}]},{"title":"缘起","slug":"缘起","date":"2016-03-01T02:05:54.000Z","updated":"2017-08-25T10:46:32.000Z","comments":true,"path":"2016/03/01/缘起/","link":"","permalink":"http://yoursite.com/2016/03/01/缘起/","excerpt":"端午之前读了一篇文章，讲了从写博客对一个程序员的进步的好处。遂决定开始动手，完成半年之前未竟的搭建hexo的事业。","text":"端午之前读了一篇文章，讲了从写博客对一个程序员的进步的好处。遂决定开始动手，完成半年之前未竟的搭建hexo的事业。首先从工作的角度来讲，这么做有相当的必要性，不待赘言。其次我和博客也很早就有一些渊源。 从QQ空间时代开始，便喜欢捣鼓。那时候虽然小，写的东西倒也能多少得到同学们和老师们的一些品评。12年的时候在旅顺海滨独自一人的时候，也曾写过几首打油诗和不入流的文章放在新浪轻博客和网易博客上。后来生活所迫，无心维护，这两个产品也没落不为人所知了。12年的时候，我十五岁，那时候阿里巴巴刚刚搞了双十一，BAT的名头还不是那么为众人 所知，还没有内容创业这回事。转眼就是四年。 四年光阴，来如流水，希逝如斯，不必赘述。到今天若说有什么感悟，就是四个字：世事无常。 算起来进入IT行业也有将近两年多了，虚度时多，事业成就一点没有，工作成果不值一提，能力依旧一般。近来得一前辈指点，虽然AI的大潮也想尽力赶上，但眼下还是搞好基础,主要是Java的基础。 过去主要是利用印象笔记偶尔记下日记和笔记以及技术相关的文章和bug总结。以此为契机 ，精进课业，把学习的心得和读书的体会以及技术上的一些操作记录放在这里。尽管资质愚钝，想必也能有所进步。如果能够通过这个小平台给朋友提供一些参考，那是最好不过。 hexo是个不错的东西，在mac上搭建有一些坑，明后天将记录一下与此相关的一些东西。","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]}]}